{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d6ebd992",
   "metadata": {},
   "source": [
    "---\n",
    "description: Utility functions to interact with airflow\n",
    "output-file: airflow_utils.html\n",
    "title: Airflow Utilities\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp airflow.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import subprocess  # nosec B404\n",
    "import shlex\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "import tempfile\n",
    "from time import sleep\n",
    "\n",
    "from airt_service.sanitizer import sanitized_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fa2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d08a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airt_service.db.models import (\n",
    "    DataBlob,\n",
    "    User,\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    "    get_session_with_context,\n",
    ")\n",
    "from sqlmodel import select\n",
    "from airt_service.batch_job import get_environment_vars_for_batch_job\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.helpers import commit_or_rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cb85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=47, uuid=UUID('e1277f72-133f-4770-a4af-4f657cda7fa0'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 12, 2, 8, 8, 31), user_id=133, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        source=uri,\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "    display(datablob)\n",
    "    datablob_id = datablob.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb9799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b32d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-02T08:08:30.682171'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dag_name = f\"test-{datetime.now().isoformat()}\"\n",
    "test_dag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_dag = \"\"\"from datetime import datetime, timedelta\n",
    "from textwrap import dedent\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash import BashOperator\n",
    "with DAG(\n",
    "    '{dag_name}',\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={{\n",
    "        'depends_on_past': False,\n",
    "        'email': ['info@airt.ai'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    }},\n",
    "    description='From S3',\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['s3'],\n",
    "    is_paused_upon_creation=True,\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    t1 = BashOperator(\n",
    "        task_id='local_s3_pull',\n",
    "        depends_on_past=False,\n",
    "        bash_command='s3_pull {{{{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}}}',\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeca6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import datetime, timedelta\n",
      "from textwrap import dedent\n",
      "\n",
      "# The DAG object; we'll need this to instantiate a DAG\n",
      "from airflow import DAG\n",
      "\n",
      "# Operators; we need this to operate!\n",
      "from airflow.operators.bash import BashOperator\n",
      "with DAG(\n",
      "    'somethinghardcodedstringliterally',\n",
      "    # These args will get passed on to each operator\n",
      "    # You can override them on a per-task basis during operator initialization\n",
      "    default_args={\n",
      "        'depends_on_past': False,\n",
      "        'email': ['info@airt.ai'],\n",
      "        'email_on_failure': False,\n",
      "        'email_on_retry': False,\n",
      "        'retries': 1,\n",
      "        'retry_delay': timedelta(minutes=5),\n",
      "        # 'queue': 'bash_queue',\n",
      "        # 'pool': 'backfill',\n",
      "        # 'priority_weight': 10,\n",
      "        # 'end_date': datetime(2016, 1, 1),\n",
      "        # 'wait_for_downstream': False,\n",
      "        # 'sla': timedelta(hours=2),\n",
      "        # 'execution_timeout': timedelta(seconds=300),\n",
      "        # 'on_failure_callback': some_function,\n",
      "        # 'on_success_callback': some_other_function,\n",
      "        # 'on_retry_callback': another_function,\n",
      "        # 'sla_miss_callback': yet_another_function,\n",
      "        # 'trigger_rule': 'all_success'\n",
      "    },\n",
      "    description='From S3',\n",
      "    start_date=datetime(2021, 1, 1),\n",
      "    catchup=False,\n",
      "    tags=['s3'],\n",
      "    is_paused_upon_creation=True,\n",
      ") as dag:\n",
      "\n",
      "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
      "    t1 = BashOperator(\n",
      "        task_id='local_s3_pull',\n",
      "        depends_on_past=False,\n",
      "        bash_command='s3_pull {{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}',\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanitized_print(bash_dag.format(dag_name=\"somethinghardcodedstringliterally\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eeca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def list_dags(\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "):\n",
    "    command = f\"{airflow_command} dags list -o json\"\n",
    "    # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "    p = subprocess.run(  # nosec B603\n",
    "        shlex.split(command), shell=False, capture_output=True, text=True, check=True\n",
    "    )\n",
    "    try:\n",
    "        return json.loads(p.stdout)\n",
    "    except Exception as e:\n",
    "        sanitized_print(f\"{p.stdout=}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_command = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 2 kumaran kumaran 4.0K Dec  2 08:08 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r--r-- 1 kumaran kumaran 1.9K Dec  2 07:59 s3_pull-10.py\n",
      "-rw-r--r-- 1 kumaran kumaran 1.9K Dec  2 08:01 s3_pull-33.py\n",
      "-rw-r--r-- 1 kumaran kumaran 3.8K Dec  2 08:08 tutorial.py\n",
      "/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\n",
      "  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\n",
      "Dag: tutorial, paused: False\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /root/airflow/dags/\n",
    "!cp {os.environ['HOME']}/airflow_venv/lib/python3.8/site-packages/airflow/example_dags/tutorial.py /root/airflow/dags\n",
    "!ll /root/airflow/dags\n",
    "!sleep 10\n",
    "!{airflow_command} dags unpause tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f2e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dag_id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>owner</th>\n",
       "      <th>paused</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3_pull-10</td>\n",
       "      <td>s3_pull-10.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3_pull-33</td>\n",
       "      <td>s3_pull-33.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>tutorial.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dag_id       filepath    owner paused\n",
       "0  s3_pull-10  s3_pull-10.py  airflow  False\n",
       "1  s3_pull-33  s3_pull-33.py  airflow  False\n",
       "2    tutorial    tutorial.py  airflow  False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(list_dags())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def list_dag_runs(\n",
    "    dag_id: str,\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "):\n",
    "    command = f\"{airflow_command} dags list-runs -d {dag_id} -o json\"\n",
    "\n",
    "    # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "    p = subprocess.run(  # nosec B603\n",
    "        shlex.split(command),\n",
    "        shell=False,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    return json.loads(p.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33808368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 2 kumaran kumaran 4.0K Dec  2 08:08 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r--r-- 1 kumaran kumaran 1.9K Dec  2 07:59 s3_pull-10.py\n",
      "-rw-r--r-- 1 kumaran kumaran 1.9K Dec  2 08:01 s3_pull-33.py\n",
      "-rw-r--r-- 1 kumaran kumaran 3.8K Dec  2 08:08 tutorial.py\n",
      "/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\n",
      "  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\n",
      "Dag: tutorial, paused: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dag_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>state</th>\n",
       "      <th>execution_date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>airt-service__2022-12-02T08:00:20.973564</td>\n",
       "      <td>success</td>\n",
       "      <td>2022-12-02T08:00:22+00:00</td>\n",
       "      <td>2022-12-02T08:00:22.449038+00:00</td>\n",
       "      <td>2022-12-02T08:00:29.434476+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>scheduled__2022-12-01T07:58:33.458042+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>2022-12-01T07:58:33.458042+00:00</td>\n",
       "      <td>2022-12-02T07:58:40.464505+00:00</td>\n",
       "      <td>2022-12-02T07:58:47.903252+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dag_id                                       run_id    state  \\\n",
       "0  tutorial     airt-service__2022-12-02T08:00:20.973564  success   \n",
       "1  tutorial  scheduled__2022-12-01T07:58:33.458042+00:00  success   \n",
       "\n",
       "                     execution_date                        start_date  \\\n",
       "0         2022-12-02T08:00:22+00:00  2022-12-02T08:00:22.449038+00:00   \n",
       "1  2022-12-01T07:58:33.458042+00:00  2022-12-02T07:58:40.464505+00:00   \n",
       "\n",
       "                           end_date  \n",
       "0  2022-12-02T08:00:29.434476+00:00  \n",
       "1  2022-12-02T07:58:47.903252+00:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(list_dag_runs(\"tutorial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_dag(\n",
    "    dag_id: str,\n",
    "    dag_definition_template: str,\n",
    "    *,\n",
    "    root_path: Path = Path(\"/root/airflow/dags/\"),\n",
    "    **kwargs,\n",
    "):\n",
    "    root_path.mkdir(exist_ok=True, parents=True)\n",
    "    tmp_file_path = root_path / f'{dag_id.replace(\":\", \"_\")}.py'\n",
    "    with open(tmp_file_path, \"w\") as temp_file:\n",
    "        temp_file.write(dag_definition_template.format(dag_name=dag_id, **kwargs))\n",
    "\n",
    "    while True:\n",
    "        df = pd.DataFrame.from_dict(list_dags())\n",
    "        if (dag_id == df[\"dag_id\"]).sum():\n",
    "            break\n",
    "        sanitized_print(\".\", end=\"\")\n",
    "        sleep(1)\n",
    "    return tmp_file_path\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def create_testing_dag_ctx(\n",
    "    dag_definition_template: str,\n",
    "    *,\n",
    "    root_path: Path = Path(\"/root/airflow/dags/\"),\n",
    "    **kwargs,\n",
    "):\n",
    "    tmp_file_path = None\n",
    "    try:\n",
    "        dag_id = f\"test-{datetime.now().isoformat()}\".replace(\":\", \"_\")\n",
    "\n",
    "        tmp_file_path = create_dag(\n",
    "            dag_id=dag_id,\n",
    "            dag_definition_template=dag_definition_template,\n",
    "            root_path=root_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        yield dag_id\n",
    "    finally:\n",
    "        if tmp_file_path and tmp_file_path.exists():\n",
    "            tmp_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2eea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.',\n",
       " '  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)',\n",
       " 'dag_id                          | filepath                           | owner   | paused',\n",
       " '================================+====================================+=========+=======',\n",
       " 's3_pull-10                      | s3_pull-10.py                      | airflow | False ',\n",
       " 's3_pull-33                      | s3_pull-33.py                      | airflow | False ',\n",
       " 'test-2022-12-02T08_09_08.275205 | test-2022-12-02T08_09_08.275205.py | airflow | None  ',\n",
       " 'tutorial                        | tutorial.py                        | airflow | False ',\n",
       " '                                                                                       ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_id='test-2022-12-02T08_09_08.275205'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    s = !{airflow_command} dags list\n",
    "    display(s)\n",
    "    display(f\"{dag_id=}\")\n",
    "    assert dag_id in \"\\n\".join(s)\n",
    "s = !{airflow_command} dags list\n",
    "assert dag_id not in \"\\n\".join(s), dag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d53b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4077a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def run_subprocess_with_retry(\n",
    "    command: str, *, no_retries: int = 12, sleep_for: int = 5\n",
    "):\n",
    "    for i in range(no_retries):\n",
    "        # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "        p = subprocess.run(  # nosec B603\n",
    "            shlex.split(command),\n",
    "            shell=False,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        if p.returncode == 0:\n",
    "            return p\n",
    "        sleep(sleep_for)\n",
    "    raise TimeoutError(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def unpause_dag(\n",
    "    dag_id: str,\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "    no_retries: int = 12,\n",
    "):\n",
    "    unpause_command = f\"{airflow_command} dags unpause {dag_id}\"\n",
    "    p = run_subprocess_with_retry(unpause_command, no_retries=no_retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed8c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-02T08_09_12.457102'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.',\n",
       " '  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)',\n",
       " 'dag_id                          | filepath                           | owner   | paused',\n",
       " '================================+====================================+=========+=======',\n",
       " 's3_pull-10                      | s3_pull-10.py                      | airflow | False ',\n",
       " 's3_pull-33                      | s3_pull-33.py                      | airflow | False ',\n",
       " 'test-2022-12-02T08_09_12.457102 | test-2022-12-02T08_09_12.457102.py | airflow | False ',\n",
       " 'tutorial                        | tutorial.py                        | airflow | False ',\n",
       " '                                                                                       ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    unpause_dag(dag_id)\n",
    "    s = !{airflow_command} dags list\n",
    "    display(s)\n",
    "    assert dag_id in \"\\n\".join(s), dag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def trigger_dag(\n",
    "    dag_id: str,\n",
    "    conf: Dict[str, Any],\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "    no_retries: int = 12,\n",
    "    unpause_if_needed: bool = True,\n",
    "):\n",
    "    if unpause_if_needed:\n",
    "        unpause_dag(\n",
    "            dag_id=dag_id, airflow_command=airflow_command, no_retries=no_retries\n",
    "        )\n",
    "\n",
    "    run_id = f\"airt-service__{datetime.now().isoformat()}\"\n",
    "    command = f\"{airflow_command} dags trigger {dag_id} --conf {shlex.quote(json.dumps(conf))} --run-id {run_id}\"\n",
    "    p = run_subprocess_with_retry(command, no_retries=no_retries)\n",
    "    sanitized_print(p)\n",
    "\n",
    "    runs = list_dag_runs(dag_id=dag_id)\n",
    "    sanitized_print(runs)\n",
    "\n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-02T08_09_29.759452'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-2022-12-02T08_09_29.759452', '--conf', '{\"datablob_id\": 47}', '--run-id', 'airt-service__2022-12-02T08:09:45.392344'], returncode=0, stdout='[\\x1b[34m2022-12-02 08:09:46,600\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-2022-12-02T08_09_29.759452 @ 2022-12-02T08:09:46+00:00: airt-service__2022-12-02T08:09:45.392344, state:queued, queued_at: 2022-12-02 08:09:46.678697+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-2022-12-02T08_09_29.759452', 'run_id': 'airt-service__2022-12-02T08:09:45.392344', 'state': 'running', 'execution_date': '2022-12-02T08:09:46+00:00', 'start_date': '2022-12-02T08:09:47.472326+00:00', 'end_date': ''}, {'dag_id': 'test-2022-12-02T08_09_29.759452', 'run_id': 'scheduled__2022-12-01T08:09:41.940713+00:00', 'state': 'running', 'execution_date': '2022-12-01T08:09:41.940713+00:00', 'start_date': '2022-12-02T08:09:45.353470+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-02T08:09:45.392344'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob.id})\n",
    "\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a01dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def wait_for_run_to_complete(dag_id: str, run_id: str, timeout: int = 60) -> str:\n",
    "    t0 = datetime.now()\n",
    "    while (datetime.now() - t0) < timedelta(seconds=timeout):\n",
    "        runs = pd.DataFrame(list_dag_runs(dag_id=dag_id))\n",
    "        state = runs.loc[runs[\"run_id\"] == run_id, \"state\"].iloc[0]\n",
    "        if state in [\"success\", \"failed\"]:\n",
    "            return state\n",
    "        sleep(5)\n",
    "    raise TimeoutError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-02T08_09_48.315850'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-2022-12-02T08_09_48.315850', '--conf', '{\"datablob_id\": 47}', '--run-id', 'airt-service__2022-12-02T08:09:57.542561'], returncode=0, stdout='[\\x1b[34m2022-12-02 08:09:58,736\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-2022-12-02T08_09_48.315850 @ 2022-12-02T08:09:58+00:00: airt-service__2022-12-02T08:09:57.542561, state:queued, queued_at: 2022-12-02 08:09:58.812110+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-2022-12-02T08_09_48.315850', 'run_id': 'airt-service__2022-12-02T08:09:57.542561', 'state': 'running', 'execution_date': '2022-12-02T08:09:58+00:00', 'start_date': '2022-12-02T08:09:59.480074+00:00', 'end_date': ''}, {'dag_id': 'test-2022-12-02T08_09_48.315850', 'run_id': 'scheduled__2022-12-01T08:09:57.226166+00:00', 'state': 'running', 'execution_date': '2022-12-01T08:09:57.226166+00:00', 'start_date': '2022-12-02T08:09:58.403549+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-02T08:09:57.542561'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob_id})\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id, run_id, timeout=600)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d707903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'tutorial', '--conf', '{\"datablob_id\": 47}', '--run-id', 'airt-service__2022-12-02T08:10:22.592585'], returncode=0, stdout='[\\x1b[34m2022-12-02 08:10:23,584\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun tutorial @ 2022-12-02T08:10:23+00:00: airt-service__2022-12-02T08:10:22.592585, state:queued, queued_at: 2022-12-02 08:10:23.647660+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'tutorial', 'run_id': 'airt-service__2022-12-02T08:10:22.592585', 'state': 'running', 'execution_date': '2022-12-02T08:10:23+00:00', 'start_date': '2022-12-02T08:10:24.452639+00:00', 'end_date': ''}, {'dag_id': 'tutorial', 'run_id': 'airt-service__2022-12-02T08:00:20.973564', 'state': 'success', 'execution_date': '2022-12-02T08:00:22+00:00', 'start_date': '2022-12-02T08:00:22.449038+00:00', 'end_date': '2022-12-02T08:00:29.434476+00:00'}, {'dag_id': 'tutorial', 'run_id': 'scheduled__2022-12-01T07:58:33.458042+00:00', 'state': 'success', 'execution_date': '2022-12-01T07:58:33.458042+00:00', 'start_date': '2022-12-02T07:58:40.464505+00:00', 'end_date': '2022-12-02T07:58:47.903252+00:00'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-02T08:10:22.592585'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_id = \"tutorial\"\n",
    "run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob_id})\n",
    "display(run_id)\n",
    "state = wait_for_run_to_complete(dag_id, run_id, timeout=600)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_env_vars = get_environment_vars_for_batch_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "batch_dag = \"\"\"from datetime import datetime, timedelta\n",
    "import json\n",
    "from textwrap import dedent\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
    "with DAG(\n",
    "    '{dag_name}',\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={{\n",
    "        'depends_on_past': False,\n",
    "        'email': ['info@airt.ai'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    }},\n",
    "    description='From S3',\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['s3'],\n",
    "    #is_paused_upon_creation=True,\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    env_var_str = '{{{{ dag_run.conf[\"environment\"] }}}}'\n",
    "    import logging\n",
    "    \n",
    "    log: logging.log = logging.getLogger(\"airflow\")\n",
    "    log.setLevel(logging.INFO)\n",
    "    log.info(\"this is me logging some random stuff and see whether it fails or not\")\n",
    "    log.info(env_var_str)\n",
    "    \n",
    "    t1 = BatchOperator(\n",
    "        task_id='batch_s3_pull',\n",
    "        depends_on_past=False,\n",
    "        job_definition=\"staging_csv_processing_job_definition\",\n",
    "        job_queue=\"staging_csv_processing_job_queue\",\n",
    "        job_name=\"test_airflow\",\n",
    "        overrides={{\n",
    "            \"command\":['s3_pull', '{{{{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}}}'],\n",
    "            \"environment\": {env_str}\n",
    "        }}\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4655317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"environment\": \"\"\"+json.dumps([dict(name=name, value=value) for name, value in batch_env_vars.items()]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51351ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "batch_env_var_names = list(batch_env_vars.keys())\n",
    "batch_env_var_names\n",
    "env_str = [\n",
    "    {\"name\": key, \"value\": f\"{{{{ dag_run.conf['{key}'] }}}}\"}\n",
    "    for key in batch_env_var_names\n",
    "]\n",
    "\n",
    "with create_testing_dag_ctx(batch_dag, env_str=env_str) as dag_id:\n",
    "    display(dag_id)\n",
    "    #     sleep(1)\n",
    "    conf = batch_env_vars.copy()\n",
    "    conf[\"datablob_id\"] = 128\n",
    "    run_id = trigger_dag(\n",
    "        dag_id,\n",
    "        conf=conf,\n",
    "    )\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id, run_id, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c7dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
