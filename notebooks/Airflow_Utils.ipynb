{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d6ebd992",
   "metadata": {},
   "source": [
    "---\n",
    "description: Utility functions to interact with airflow\n",
    "output-file: airflow_utils.html\n",
    "title: Airflow Utilities\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp airflow.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "import os\n",
    "import shlex\n",
    "import subprocess  # nosec B404\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from airt_service.sanitizer import sanitized_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65fa2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d08a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlmodel import select\n",
    "\n",
    "from airt_service.batch_job import get_environment_vars_for_batch_job\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.db.models import (\n",
    "    DataBlob,\n",
    "    User,\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    "    get_session_with_context,\n",
    ")\n",
    "from airt_service.helpers import commit_or_rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0cb85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=46, uuid=UUID('bece26cc-4f2a-4e10-a20a-e8a8c4cd5ec4'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 12, 15, 9, 22, 54), user_id=132, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        source=uri,\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "    display(datablob)\n",
    "    datablob_id = datablob.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb9799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b32d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-15T09:22:53.513617'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dag_name = f\"test-{datetime.now().isoformat()}\"\n",
    "test_dag_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_dag = \"\"\"from datetime import datetime, timedelta\n",
    "from textwrap import dedent\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.operators.bash import BashOperator\n",
    "with DAG(\n",
    "    '{dag_name}',\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={{\n",
    "        'depends_on_past': False,\n",
    "        'email': ['info@airt.ai'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    }},\n",
    "    description='From S3',\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['s3'],\n",
    "    is_paused_upon_creation=True,\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    t1 = BashOperator(\n",
    "        task_id='local_s3_pull',\n",
    "        depends_on_past=False,\n",
    "        bash_command='s3_pull {{{{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}}}',\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeca6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import datetime, timedelta\n",
      "from textwrap import dedent\n",
      "\n",
      "# The DAG object; we'll need this to instantiate a DAG\n",
      "from airflow import DAG\n",
      "\n",
      "# Operators; we need this to operate!\n",
      "from airflow.operators.bash import BashOperator\n",
      "with DAG(\n",
      "    'somethinghardcodedstringliterally',\n",
      "    # These args will get passed on to each operator\n",
      "    # You can override them on a per-task basis during operator initialization\n",
      "    default_args={\n",
      "        'depends_on_past': False,\n",
      "        'email': ['info@airt.ai'],\n",
      "        'email_on_failure': False,\n",
      "        'email_on_retry': False,\n",
      "        'retries': 1,\n",
      "        'retry_delay': timedelta(minutes=5),\n",
      "        # 'queue': 'bash_queue',\n",
      "        # 'pool': 'backfill',\n",
      "        # 'priority_weight': 10,\n",
      "        # 'end_date': datetime(2016, 1, 1),\n",
      "        # 'wait_for_downstream': False,\n",
      "        # 'sla': timedelta(hours=2),\n",
      "        # 'execution_timeout': timedelta(seconds=300),\n",
      "        # 'on_failure_callback': some_function,\n",
      "        # 'on_success_callback': some_other_function,\n",
      "        # 'on_retry_callback': another_function,\n",
      "        # 'sla_miss_callback': yet_another_function,\n",
      "        # 'trigger_rule': 'all_success'\n",
      "    },\n",
      "    description='From S3',\n",
      "    start_date=datetime(2021, 1, 1),\n",
      "    catchup=False,\n",
      "    tags=['s3'],\n",
      "    is_paused_upon_creation=True,\n",
      ") as dag:\n",
      "\n",
      "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
      "    t1 = BashOperator(\n",
      "        task_id='local_s3_pull',\n",
      "        depends_on_past=False,\n",
      "        bash_command='s3_pull {{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}',\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanitized_print(bash_dag.format(dag_name=\"somethinghardcodedstringliterally\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eeca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5594df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def list_dags(\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    ") -> Dict[str, Any]:\n",
    "    command = f\"{airflow_command} dags list -o json\"\n",
    "    # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "    p = subprocess.run(  # nosec B603\n",
    "        shlex.split(command), shell=False, capture_output=True, text=True, check=True\n",
    "    )\n",
    "    try:\n",
    "        dags_list: Dict[str, Any] = json.loads(p.stdout)\n",
    "        return dags_list\n",
    "    except Exception as e:\n",
    "        sanitized_print(f\"{p.stdout=}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow_command = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fa0411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: ll: command not found\n",
      "/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\n",
      "  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\n",
      "Dag: tutorial, paused: False\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p {os.environ['HOME']}/airflow/dags\n",
    "!cp {os.environ['HOME']}/airflow_venv/lib/python3.9/site-packages/airflow/example_dags/tutorial.py {os.environ['HOME']}/airflow/dags\n",
    "!ll {os.environ['HOME']}/airflow/dags\n",
    "!sleep 10\n",
    "!{airflow_command} dags unpause tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f2e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dag_id</th>\n",
       "      <th>filepath</th>\n",
       "      <th>owner</th>\n",
       "      <th>paused</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3_pull-27</td>\n",
       "      <td>s3_pull-27.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3_pull-8</td>\n",
       "      <td>s3_pull-8.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>tutorial.py</td>\n",
       "      <td>airflow</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dag_id       filepath    owner paused\n",
       "0  s3_pull-27  s3_pull-27.py  airflow  False\n",
       "1   s3_pull-8   s3_pull-8.py  airflow  False\n",
       "2    tutorial    tutorial.py  airflow  False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(list_dags())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def list_dag_runs(\n",
    "    dag_id: str,\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    ") -> Dict[str, Any]:\n",
    "    command = f\"{airflow_command} dags list-runs -d {dag_id} -o json\"\n",
    "\n",
    "    # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "    p = subprocess.run(  # nosec B603\n",
    "        shlex.split(command),\n",
    "        shell=False,\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True,\n",
    "    )\n",
    "\n",
    "    dag_runs: Dict[str, Any] = json.loads(p.stdout)\n",
    "    return dag_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33808368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dag_id</th>\n",
       "      <th>run_id</th>\n",
       "      <th>state</th>\n",
       "      <th>execution_date</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tutorial</td>\n",
       "      <td>scheduled__2022-12-14T09:22:57.262588+00:00</td>\n",
       "      <td>success</td>\n",
       "      <td>2022-12-14T09:22:57.262588+00:00</td>\n",
       "      <td>2022-12-15T09:23:05.284639+00:00</td>\n",
       "      <td>2022-12-15T09:23:12.566310+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dag_id                                       run_id    state  \\\n",
       "0  tutorial  scheduled__2022-12-14T09:22:57.262588+00:00  success   \n",
       "\n",
       "                     execution_date                        start_date  \\\n",
       "0  2022-12-14T09:22:57.262588+00:00  2022-12-15T09:23:05.284639+00:00   \n",
       "\n",
       "                           end_date  \n",
       "0  2022-12-15T09:23:12.566310+00:00  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sleep 30\n",
    "pd.DataFrame.from_dict(list_dag_runs(\"tutorial\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b50b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_dag(\n",
    "    dag_id: str,\n",
    "    dag_definition_template: str,\n",
    "    *,\n",
    "    root_path: Path = Path(f\"{os.environ['HOME']}/airflow/dags\"),\n",
    "    **kwargs: Any,\n",
    ") -> Path:\n",
    "    root_path.mkdir(exist_ok=True, parents=True)\n",
    "    tmp_file_path = root_path / f'{dag_id.replace(\":\", \"_\")}.py'\n",
    "    with open(tmp_file_path, \"w\") as temp_file:\n",
    "        temp_file.write(dag_definition_template.format(dag_name=dag_id, **kwargs))\n",
    "\n",
    "    while True:\n",
    "        df: pd.DataFrame = pd.DataFrame.from_dict(list_dags())\n",
    "        if (dag_id == df[\"dag_id\"]).sum():  # type: ignore\n",
    "            break\n",
    "        sanitized_print(\".\", end=\"\")\n",
    "        sleep(1)\n",
    "    return tmp_file_path\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def create_testing_dag_ctx(\n",
    "    dag_definition_template: str,\n",
    "    *,\n",
    "    root_path: Path = Path(f\"{os.environ['HOME']}/airflow/dags\"),\n",
    "    **kwargs: Any,\n",
    ") -> Iterator[str]:\n",
    "    tmp_file_path = None\n",
    "    try:\n",
    "        dag_id = f\"test-{datetime.now().isoformat()}\".replace(\":\", \"_\")\n",
    "\n",
    "        tmp_file_path = create_dag(\n",
    "            dag_id=dag_id,\n",
    "            dag_definition_template=dag_definition_template,\n",
    "            root_path=root_path,\n",
    "            **kwargs,\n",
    "        )\n",
    "        yield dag_id\n",
    "    finally:\n",
    "        if tmp_file_path and tmp_file_path.exists():\n",
    "            tmp_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2eea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.',\n",
       " '  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)',\n",
       " 'dag_id                          | filepath                           | owner   | paused',\n",
       " '================================+====================================+=========+=======',\n",
       " 's3_pull-27                      | s3_pull-27.py                      | airflow | False ',\n",
       " 's3_pull-8                       | s3_pull-8.py                       | airflow | False ',\n",
       " 'test-2022-12-15T09_23_38.317117 | test-2022-12-15T09_23_38.317117.py | airflow | None  ',\n",
       " 'tutorial                        | tutorial.py                        | airflow | False ',\n",
       " '                                                                                       ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_id='test-2022-12-15T09_23_38.317117'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    s = !{airflow_command} dags list\n",
    "    display(s)\n",
    "    display(f\"{dag_id=}\")\n",
    "    assert dag_id in \"\\n\".join(s)\n",
    "s = !{airflow_command} dags list\n",
    "assert dag_id not in \"\\n\".join(s), dag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d53b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4077a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def run_subprocess_with_retry(\n",
    "    command: str, *, no_retries: int = 12, sleep_for: int = 5\n",
    ") -> subprocess.CompletedProcess:\n",
    "    for i in range(no_retries):\n",
    "        # nosemgrep: python.lang.security.audit.dangerous-subprocess-use.dangerous-subprocess-use\n",
    "        p = subprocess.run(  # nosec B603\n",
    "            shlex.split(command),\n",
    "            shell=False,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False,\n",
    "        )\n",
    "        if p.returncode == 0:\n",
    "            return p\n",
    "        sleep(sleep_for)\n",
    "    raise TimeoutError(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def unpause_dag(\n",
    "    dag_id: str,\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "    no_retries: int = 12,\n",
    ") -> None:\n",
    "    unpause_command = f\"{airflow_command} dags unpause {dag_id}\"\n",
    "    p = run_subprocess_with_retry(unpause_command, no_retries=no_retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed8c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-15T09_23_42.049857'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.',\n",
       " '  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)',\n",
       " 'dag_id                          | filepath                           | owner   | paused',\n",
       " '================================+====================================+=========+=======',\n",
       " 's3_pull-27                      | s3_pull-27.py                      | airflow | False ',\n",
       " 's3_pull-8                       | s3_pull-8.py                       | airflow | False ',\n",
       " 'test-2022-12-15T09_23_42.049857 | test-2022-12-15T09_23_42.049857.py | airflow | False ',\n",
       " 'tutorial                        | tutorial.py                        | airflow | False ',\n",
       " '                                                                                       ']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    unpause_dag(dag_id)\n",
    "    s = !{airflow_command} dags list\n",
    "    display(s)\n",
    "    assert dag_id in \"\\n\".join(s), dag_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def trigger_dag(\n",
    "    dag_id: str,\n",
    "    conf: Dict[str, Any],\n",
    "    *,\n",
    "    airflow_command: str = f\"{os.environ['HOME']}/airflow_venv/bin/airflow\",\n",
    "    no_retries: int = 12,\n",
    "    unpause_if_needed: bool = True,\n",
    ") -> str:\n",
    "    if unpause_if_needed:\n",
    "        unpause_dag(\n",
    "            dag_id=dag_id, airflow_command=airflow_command, no_retries=no_retries\n",
    "        )\n",
    "\n",
    "    run_id = f\"airt-service__{datetime.now().isoformat()}\"\n",
    "    command = f\"{airflow_command} dags trigger {dag_id} --conf {shlex.quote(json.dumps(conf))} --run-id {run_id}\"\n",
    "    p = run_subprocess_with_retry(command, no_retries=no_retries)\n",
    "    sanitized_print(p)\n",
    "\n",
    "    runs = list_dag_runs(dag_id=dag_id)\n",
    "    sanitized_print(runs)\n",
    "\n",
    "    return run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58a06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-15T09_23_46.332839'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-2022-12-15T09_23_46.332839', '--conf', '{\"datablob_id\": 46}', '--run-id', 'airt-service__2022-12-15T09:24:01.559216'], returncode=0, stdout='[\\x1b[34m2022-12-15 09:24:02,670\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-2022-12-15T09_23_46.332839 @ 2022-12-15T09:24:02+00:00: airt-service__2022-12-15T09:24:01.559216, state:queued, queued_at: 2022-12-15 09:24:02.782682+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-2022-12-15T09_23_46.332839', 'run_id': 'airt-service__2022-12-15T09:24:01.559216', 'state': 'running', 'execution_date': '2022-12-15T09:24:02+00:00', 'start_date': '2022-12-15T09:24:02.925577+00:00', 'end_date': ''}, {'dag_id': 'test-2022-12-15T09_23_46.332839', 'run_id': 'scheduled__2022-12-14T09:23:58.295626+00:00', 'state': 'running', 'execution_date': '2022-12-14T09:23:58.295626+00:00', 'start_date': '2022-12-15T09:24:02.019869+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-15T09:24:01.559216'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob.id})\n",
    "\n",
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a01dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def wait_for_run_to_complete(dag_id: str, run_id: str, timeout: int = 60) -> str:\n",
    "    t0 = datetime.now()\n",
    "    while (datetime.now() - t0) < timedelta(seconds=timeout):\n",
    "        runs = pd.DataFrame(list_dag_runs(dag_id=dag_id))\n",
    "        state: str = runs.loc[runs[\"run_id\"] == run_id, \"state\"].iloc[0]\n",
    "        if state in [\"success\", \"failed\"]:\n",
    "            return state\n",
    "        sleep(5)\n",
    "    raise TimeoutError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25953d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test-2022-12-15T09_24_04.451774'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-2022-12-15T09_24_04.451774', '--conf', '{\"datablob_id\": 46}', '--run-id', 'airt-service__2022-12-15T09:24:19.530380'], returncode=0, stdout='[\\x1b[34m2022-12-15 09:24:20,664\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-2022-12-15T09_24_04.451774 @ 2022-12-15T09:24:20+00:00: airt-service__2022-12-15T09:24:19.530380, state:queued, queued_at: 2022-12-15 09:24:20.782188+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-2022-12-15T09_24_04.451774', 'run_id': 'airt-service__2022-12-15T09:24:19.530380', 'state': 'running', 'execution_date': '2022-12-15T09:24:20+00:00', 'start_date': '2022-12-15T09:24:21.199312+00:00', 'end_date': ''}, {'dag_id': 'test-2022-12-15T09_24_04.451774', 'run_id': 'scheduled__2022-12-14T09:24:13.563722+00:00', 'state': 'running', 'execution_date': '2022-12-14T09:24:13.563722+00:00', 'start_date': '2022-12-15T09:24:20.128483+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-15T09:24:19.530380'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with create_testing_dag_ctx(bash_dag) as dag_id:\n",
    "    display(dag_id)\n",
    "    run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob_id})\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id, run_id, timeout=600)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d707903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'tutorial', '--conf', '{\"datablob_id\": 46}', '--run-id', 'airt-service__2022-12-15T09:24:56.144562'], returncode=0, stdout='[\\x1b[34m2022-12-15 09:24:56,924\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun tutorial @ 2022-12-15T09:24:57+00:00: airt-service__2022-12-15T09:24:56.144562, state:queued, queued_at: 2022-12-15 09:24:57.008356+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'tutorial', 'run_id': 'airt-service__2022-12-15T09:24:56.144562', 'state': 'running', 'execution_date': '2022-12-15T09:24:57+00:00', 'start_date': '2022-12-15T09:24:57.694541+00:00', 'end_date': ''}, {'dag_id': 'tutorial', 'run_id': 'scheduled__2022-12-14T09:22:57.262588+00:00', 'state': 'success', 'execution_date': '2022-12-14T09:22:57.262588+00:00', 'start_date': '2022-12-15T09:23:05.284639+00:00', 'end_date': '2022-12-15T09:23:12.566310+00:00'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-15T09:24:56.144562'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag_id = \"tutorial\"\n",
    "run_id = trigger_dag(dag_id, conf={\"datablob_id\": datablob_id})\n",
    "display(run_id)\n",
    "state = wait_for_run_to_complete(dag_id, run_id, timeout=600)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_env_vars = get_environment_vars_for_batch_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "batch_dag = \"\"\"from datetime import datetime, timedelta\n",
    "import json\n",
    "from textwrap import dedent\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
    "with DAG(\n",
    "    '{dag_name}',\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={{\n",
    "        'depends_on_past': False,\n",
    "        'email': ['info@airt.ai'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        # 'queue': 'bash_queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': timedelta(hours=2),\n",
    "        # 'execution_timeout': timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    }},\n",
    "    description='From S3',\n",
    "    start_date=datetime(2021, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['s3'],\n",
    "    #is_paused_upon_creation=True,\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "    env_var_str = '{{{{ dag_run.conf[\"environment\"] }}}}'\n",
    "    import logging\n",
    "    \n",
    "    log: logging.log = logging.getLogger(\"airflow\")\n",
    "    log.setLevel(logging.INFO)\n",
    "    log.info(\"this is me logging some random stuff and see whether it fails or not\")\n",
    "    log.info(env_var_str)\n",
    "    \n",
    "    t1 = BatchOperator(\n",
    "        task_id='batch_s3_pull',\n",
    "        depends_on_past=False,\n",
    "        job_definition=\"staging_csv_processing_job_definition\",\n",
    "        job_queue=\"staging_csv_processing_job_queue\",\n",
    "        job_name=\"test_airflow\",\n",
    "        overrides={{\n",
    "            \"command\":['s3_pull', '{{{{ dag_run.conf[\"datablob_id\"] if dag_run else \"\" }}}}'],\n",
    "            \"environment\": {env_str}\n",
    "        }}\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4655317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"environment\": \"\"\"+json.dumps([dict(name=name, value=value) for name, value in batch_env_vars.items()]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51351ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "batch_env_var_names = list(batch_env_vars.keys())\n",
    "batch_env_var_names\n",
    "env_str = [\n",
    "    {\"name\": key, \"value\": f\"{{{{ dag_run.conf['{key}'] }}}}\"}\n",
    "    for key in batch_env_var_names\n",
    "]\n",
    "\n",
    "with create_testing_dag_ctx(batch_dag, env_str=env_str) as dag_id:\n",
    "    display(dag_id)\n",
    "    #     sleep(1)\n",
    "    conf = batch_env_vars.copy()\n",
    "    conf[\"datablob_id\"] = 128\n",
    "    run_id = trigger_dag(\n",
    "        dag_id,\n",
    "        conf=conf,\n",
    "    )\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id, run_id, timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c7dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
