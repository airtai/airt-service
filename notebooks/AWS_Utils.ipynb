{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for aws utility functions\n",
    "output-file: aws_utils.html\n",
    "title: AWS Utilities\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb8313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp aws.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c89c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500398f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from fastapi import status, HTTPException\n",
    "from mypy_boto3_s3.service_resource import Bucket\n",
    "\n",
    "\n",
    "from airt.helpers import get_s3_bucket_name_and_folder_from_uri\n",
    "from airt.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6555a3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from time import sleep\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pytest\n",
    "from sqlmodel import select\n",
    "\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.db.models import (\n",
    "    create_user_for_testing,\n",
    "    get_session_with_context,\n",
    "    User,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64682aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ieaggdcklq'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing()\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ee67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83431210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_available_aws_regions() -> List[str]:\n",
    "    \"\"\"Get supported regions\n",
    "\n",
    "    Returns:\n",
    "        List of supported regions\n",
    "    \"\"\"\n",
    "\n",
    "    # boto3.session.Session().get_available_regions('s3') is the api to get available regions of an aws service\n",
    "    # batch supports one less region than s3 so hardcoding the following list of regions\n",
    "    return [\n",
    "        #         \"af-south-1\", # Africa capetown\n",
    "        #         \"ap-east-1\", # Asia Pasific HongKong\n",
    "        \"ap-northeast-1\",\n",
    "        \"ap-northeast-2\",\n",
    "        #         \"ap-northeast-3\", # Problem with creating gpu instances for training during build_wheel stage\n",
    "        \"ap-south-1\",\n",
    "        \"ap-southeast-1\",\n",
    "        \"ap-southeast-2\",\n",
    "        \"ca-central-1\",\n",
    "        \"eu-central-1\",\n",
    "        \"eu-north-1\",\n",
    "        #         \"eu-south-1\", # Europe Milan\n",
    "        \"eu-west-1\",\n",
    "        \"eu-west-2\",\n",
    "        \"eu-west-3\",\n",
    "        #         \"me-south-1\", # Middle East Bahrain\n",
    "        \"sa-east-1\",\n",
    "        \"us-east-1\",\n",
    "        \"us-east-2\",\n",
    "        \"us-west-1\",\n",
    "        \"us-west-2\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028eab0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ap-northeast-1',\n",
       " 'ap-northeast-2',\n",
       " 'ap-south-1',\n",
       " 'ap-southeast-1',\n",
       " 'ap-southeast-2',\n",
       " 'ca-central-1',\n",
       " 'eu-central-1',\n",
       " 'eu-north-1',\n",
       " 'eu-west-1',\n",
       " 'eu-west-2',\n",
       " 'eu-west-3',\n",
       " 'sa-east-1',\n",
       " 'us-east-1',\n",
       " 'us-east-2',\n",
       " 'us-west-1',\n",
       " 'us-west-2']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_aws_regions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ba227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def verify_aws_region(region: str):\n",
    "    \"\"\"\n",
    "    Verify region is in available regions else raise an error\n",
    "\n",
    "    Args:\n",
    "        region: region name\n",
    "    Raises:\n",
    "        HTTPException: If region is not a valid region\n",
    "    \"\"\"\n",
    "    available_regions = get_available_aws_regions()\n",
    "    if region not in available_regions:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=f\"Unknown region - {region}; Available regions are {', '.join(available_regions)}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53603165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Unknown region - region-doesnt-exists; Available regions are ap-northeast-1, ap...l-1, eu-central-1, eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "verify_aws_region(\"eu-west-1\")\n",
    "\n",
    "with pytest.raises(HTTPException) as e:\n",
    "    verify_aws_region(region=\"region-doesnt-exists\")\n",
    "assert \"Unknown region\" in str(e)\n",
    "display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447cbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_s3_storage_bucket(region: str = \"eu-west-1\") -> Tuple[Bucket, str]:\n",
    "    \"\"\"Get the root s3 bucket to store datasources, models, predictions\n",
    "\n",
    "    Args:\n",
    "        region: region name\n",
    "    Returns:\n",
    "        The root storage s3 bucket\n",
    "    Raises:\n",
    "        HTTPException: If region is not a valid region\n",
    "    \"\"\"\n",
    "    verify_aws_region(region)\n",
    "\n",
    "    storage_bucket = f\"s3://{os.environ['STORAGE_BUCKET_PREFIX']}-{region}\"\n",
    "    bucket_name, base_path = get_s3_bucket_name_and_folder_from_uri(storage_bucket)\n",
    "\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "\n",
    "    if not bucket.creation_date:\n",
    "        s3_client = boto3.client(\"s3\", region_name=region)\n",
    "        #         region = s3_client.meta.region_name\n",
    "        try:\n",
    "            s3_client.create_bucket(\n",
    "                Bucket=bucket_name,\n",
    "                CreateBucketConfiguration={\"LocationConstraint\": region},\n",
    "            )\n",
    "        except s3_client.exceptions.BucketAlreadyOwnedByYou as e:\n",
    "            logger.info(\"Bucket already created\")\n",
    "        bucket = s3.Bucket(bucket_name)\n",
    "    return bucket, base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf891b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='kumaran-airt-service-eu-west-1'), '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = get_s3_storage_bucket(region=\"eu-west-1\")\n",
    "display(actual)\n",
    "assert actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3447090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='kumaran-airt-service-eu-west-3'), '')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = get_s3_storage_bucket(region=\"eu-west-3\")\n",
    "display(actual)\n",
    "assert actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a236183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Unknown region - region-doesnt-exists; Available regions are ap-northeast-1, ap...l-1, eu-central-1, eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2') tblen=3>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pytest.raises(HTTPException) as e:\n",
    "    get_s3_storage_bucket(region=\"region-doesnt-exists\")\n",
    "assert \"Unknown region\" in str(e)\n",
    "display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_s3_datablob_path(\n",
    "    user_id: int, datablob_id: int, region: str\n",
    ") -> Tuple[Bucket, str]:\n",
    "    \"\"\"Create an S3 path to store the datablobs\n",
    "\n",
    "    Args:\n",
    "        user_id: User id\n",
    "        datablob_id: Datablob id\n",
    "\n",
    "    Returns:\n",
    "        The root storage bucket object and the s3 path as a tuple\n",
    "    \"\"\"\n",
    "    bucket, base_path = get_s3_storage_bucket(region=region)\n",
    "    s3_path = f\"{user_id}/datablob/{datablob_id}\"\n",
    "    s3_path = f\"{base_path}/{s3_path}\" if base_path else s3_path\n",
    "\n",
    "    return bucket, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803b924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='kumaran-airt-service-eu-west-1'), '999/datablob/999')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = create_s3_datablob_path(user_id=999, datablob_id=999, region=\"eu-west-1\")\n",
    "display(actual)\n",
    "expected = \"999/datablob/999\"\n",
    "_, base_path = get_s3_storage_bucket(region=\"eu-west-1\")\n",
    "expected = f\"{base_path}/{expected}\" if base_path else expected\n",
    "\n",
    "assert actual[0]\n",
    "assert actual[1] == expected\n",
    "\n",
    "# bucket = actual[0]\n",
    "# display(bucket.meta.client.head_object(Bucket=bucket.name, Key=\"999/datasource/999/result.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f077153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_s3_datasource_path(\n",
    "    user_id: int, datasource_id: int, region: str\n",
    ") -> Tuple[Bucket, str]:\n",
    "    \"\"\"Create an S3 path to store the datasources\n",
    "\n",
    "    Args:\n",
    "        user_id: User id\n",
    "        datasource_id: Datasource id to store\n",
    "\n",
    "    Returns:\n",
    "        The root storage bucket object and the s3 path as a tuple\n",
    "    \"\"\"\n",
    "    bucket, base_path = get_s3_storage_bucket(region=region)\n",
    "    s3_path = f\"{user_id}/datasource/{datasource_id}\"\n",
    "    s3_path = f\"{base_path}/{s3_path}\" if base_path else s3_path\n",
    "\n",
    "    return bucket, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2f79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='kumaran-airt-service-eu-west-1'), '999/datasource/999')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = create_s3_datasource_path(user_id=999, datasource_id=999, region=\"eu-west-1\")\n",
    "display(actual)\n",
    "expected = \"999/datasource/999\"\n",
    "_, base_path = get_s3_storage_bucket(region=\"eu-west-1\")\n",
    "expected = f\"{base_path}/{expected}\" if base_path else expected\n",
    "\n",
    "\n",
    "assert actual[0]\n",
    "assert actual[1] == expected\n",
    "\n",
    "# bucket = actual[0]\n",
    "# display(bucket.meta.client.head_object(Bucket=bucket.name, Key=\"999/datasource/999/result.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffa38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_s3_prediction_path(\n",
    "    user_id: int, prediction_id: int, region: str\n",
    ") -> Tuple[Bucket, str]:\n",
    "    \"\"\"Create an S3 path to store the prediction results\n",
    "\n",
    "    Args:\n",
    "        user_id: User id\n",
    "        prediction_id: Prediction id\n",
    "\n",
    "    Returns:\n",
    "        The root storage bucket object and the s3 path as a tuple\n",
    "    \"\"\"\n",
    "    bucket, base_path = get_s3_storage_bucket(region=region)\n",
    "    s3_path = f\"{user_id}/prediction/{prediction_id}\"\n",
    "    s3_path = f\"{base_path}/{s3_path}\" if base_path else s3_path\n",
    "\n",
    "    return bucket, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba968b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(s3.Bucket(name='kumaran-airt-service-eu-west-1'), '999/prediction/999')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = create_s3_prediction_path(user_id=999, prediction_id=999, region=\"eu-west-1\")\n",
    "display(actual)\n",
    "expected = \"999/prediction/999\"\n",
    "_, base_path = get_s3_storage_bucket(region=\"eu-west-1\")\n",
    "expected = f\"{base_path}/{expected}\" if base_path else expected\n",
    "\n",
    "assert actual[0]\n",
    "assert actual[1] == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3188d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_batch_environment_arns(\n",
    "    region: str, batch_environment_arn_path: Optional[Union[str, Path]] = None\n",
    ") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"Read the batch environment arn yaml file and return as a dict\n",
    "\n",
    "    Args:\n",
    "        region: Region to get batch environment arns\n",
    "        batch_environment_arn_path: Path to the arn file. If not set, then the batch_environment\n",
    "            will be loaded from the current working directory\n",
    "\n",
    "    Returns:\n",
    "        The created batch environment arns as a dict\n",
    "    \"\"\"\n",
    "    if batch_environment_arn_path is None:\n",
    "        batch_environment_arn_path = Path(\"./batch_environment.yml\")\n",
    "    with open(batch_environment_arn_path) as f:\n",
    "        batch_environment_arns = yaml.safe_load(f)\n",
    "\n",
    "    return batch_environment_arns[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fc647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'csv_processing': {'compute_environment_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_definition_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_queue_arn': 'arn:aws:batch:placeholder'},\n",
       " 'predictions': {'compute_environment_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_definition_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_queue_arn': 'arn:aws:batch:placeholder'},\n",
       " 'preprocessing': {'compute_environment_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_definition_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_queue_arn': 'arn:aws:batch:placeholder'},\n",
       " 'training': {'compute_environment_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_definition_arn': 'arn:aws:batch:placeholder',\n",
       "  'job_queue_arn': 'arn:aws:batch:placeholder'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region = \"eu-west-1\"\n",
    "test_batch_environment_arns = {\n",
    "    region: {\n",
    "        task: {\n",
    "            arn: \"arn:aws:batch:placeholder\"\n",
    "            for arn in [\n",
    "                \"compute_environment_arn\",\n",
    "                \"job_definition_arn\",\n",
    "                \"job_queue_arn\",\n",
    "            ]\n",
    "        }\n",
    "        for task in [\"csv_processing\", \"predictions\", \"preprocessing\", \"training\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    td = Path(td)\n",
    "    test_batch_environment_arn_path = td / \"batch_environment.yml\"\n",
    "    with open(test_batch_environment_arn_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_arns, f, default_flow_style=False)\n",
    "    actual = get_batch_environment_arns(\n",
    "        region=region, batch_environment_arn_path=test_batch_environment_arn_path\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual == test_batch_environment_arns[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_queue_definition_arns(\n",
    "    task: str,\n",
    "    region: str,\n",
    "    batch_environment_arn_path: Optional[Union[str, Path]] = None,\n",
    ") -> Tuple[str, str]:\n",
    "    \"\"\"Get the job queue arn and the job definition arn for the given task\n",
    "\n",
    "    Args:\n",
    "        task: Task name\n",
    "        region: Region to get queue definition arns\n",
    "        batch_environment_arn_path: Path to the arn file. If not set, then the batch_environment\n",
    "            will be loaded from the current working directory\n",
    "    \"\"\"\n",
    "    batch_environment_arns = get_batch_environment_arns(\n",
    "        region=region, batch_environment_arn_path=batch_environment_arn_path\n",
    "    )\n",
    "    job_queue_arn = batch_environment_arns[task][\"job_queue_arn\"]\n",
    "    job_definition_arn = batch_environment_arns[task][\"job_definition_arn\"]\n",
    "    return job_queue_arn, job_definition_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    td = Path(td)\n",
    "    test_batch_environment_arn_path = td / \"batch_environment.yml\"\n",
    "    with open(test_batch_environment_arn_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_arns, f, default_flow_style=False)\n",
    "\n",
    "    task = \"csv_processing\"\n",
    "    actual_job_queue_arn, actual_job_definition_arn = get_queue_definition_arns(\n",
    "        task=task,\n",
    "        region=region,\n",
    "        batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "    )\n",
    "    assert (\n",
    "        actual_job_queue_arn\n",
    "        == test_batch_environment_arns[region][task][\"job_queue_arn\"]\n",
    "    )\n",
    "    assert (\n",
    "        actual_job_definition_arn\n",
    "        == test_batch_environment_arns[region][task][\"job_definition_arn\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a01fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def upload_to_s3_with_retry(\n",
    "    file_to_upload: str,\n",
    "    presigned_url: str,\n",
    "    presigned_fields: Dict[str, Any],\n",
    "    max_retry: int = 3,\n",
    "    curr_iteration: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Helper function to upload local files to s3 using presigned url; Used only in tests\n",
    "\n",
    "    Args:\n",
    "        file_to_upload: path of file to upload\n",
    "        presigned_url: presigned url to upload to\n",
    "        presigned_fields: presigned fields provided by boto3\n",
    "        max_retry: maximum retry count\n",
    "        curr_iteration: current iteration count for internal use\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_to_upload, \"rb\") as f:\n",
    "            files = {\"file\": (str(file_to_upload), f)}\n",
    "            response = requests.post(presigned_url, data=presigned_fields, files=files)\n",
    "            assert response.status_code == 204, response.text  # nosec B101\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        print(\"Retrying upload\")\n",
    "        if curr_iteration == max_retry:\n",
    "            print(\"Retry failed\")\n",
    "            raise e\n",
    "        upload_to_s3_with_retry(\n",
    "            file_to_upload,\n",
    "            presigned_url,\n",
    "            presigned_fields,\n",
    "            max_retry,\n",
    "            curr_iteration + 1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117010af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/sqlmodel/orm/session.py:60: SAWarning: Class SelectOfScalar will not make use of SQL compilation caching as it does not set the 'inherit_cache' attribute to ``True``.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this object can make use of the cache key generated by the superclass.  Alternatively, this attribute may be set to False which will disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n",
      "  results = super().execute(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('b8769687-9aa3-4186-85fa-28a930f1eba7'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '9/datablob/8/${filename}', 'AWSAccessKeyId': 'AKIAY7RRHQ4BEOUZVSE3', 'policy': 'eyJleHBpcmF0aW9uIjogIjIwMjItMDktMDNUMDY6Mzk6NDBaIiwgImNvbmRpdGlvbnMiOiBbWyJzdGFydHMtd2l0aCIsICIka2V5IiwgIjkvZGF0YWJsb2IvOCJdLCB7ImJ1Y2tldCI6ICJrdW1hcmFuLWFpcnQtc2VydmljZS1ldS13ZXN0LTEifSwgWyJzdGFydHMtd2l0aCIsICIka2V5IiwgIjkvZGF0YWJsb2IvOC8iXV19', 'signature': '2kLPQLr0YsHsBBhzGtd8nD6EfVw='}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccountId DefinitionId        OccurredTime  OccurredTimeTicks  PersonId\n",
       "0     312571   loadTests2 2019-12-31 21:30:02      1577836802678         2\n",
       "1     312571   loadTests3 2020-01-03 23:53:22      1578104602678         2\n",
       "2     312571   loadTests1 2020-01-07 02:16:42      1578372402678         2\n",
       "3     312571   loadTests2 2020-01-10 04:40:02      1578640202678         2\n",
       "4     312571   loadTests3 2020-01-13 07:03:22      1578908002678         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-4.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-3.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-2.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-0.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-1.csv')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccountId,DefinitionId,OccurredTime,OccurredTimeTicks,PersonId\r\n",
      "312571,loadTests2,2019-12-31 21:30:02,1577836802678,2\r\n",
      "312571,loadTests3,2020-01-03 23:53:22,1578104602678,2\r\n",
      "312571,loadTests1,2020-01-07 02:16:42,1578372402678,2\r\n",
      "312571,loadTests2,2020-01-10 04:40:02,1578640202678,2\r\n",
      "312571,loadTests3,2020-01-13 07:03:22,1578908002678,2\r\n",
      "312571,loadTests1,2020-01-16 09:26:42,1579175802678,2\r\n",
      "312571,loadTests2,2020-01-19 11:50:02,1579443602678,2\r\n",
      "312571,loadTests3,2020-01-22 14:13:22,1579711402678,2\r\n",
      "312571,loadTests1,2020-01-25 16:36:42,1579979202678,2\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-0.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x/csv/file-4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_mkwk1c3x\n"
     ]
    }
   ],
   "source": [
    "# Create a csv datasource and upload multiple csv files using presigned url\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_local_request = FromLocalRequest(\n",
    "        path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "    )\n",
    "    from_local_response = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        ddf = dd.read_parquet(test_s3_path.as_path())\n",
    "        display(ddf.head())\n",
    "        ddf.to_csv(test_s3_path.as_path() / \"csv\" / \"file-*.csv\", index=False)\n",
    "        display(list((test_s3_path.as_path() / \"csv\").glob(\"*\")))\n",
    "        !head -n 10 {test_s3_path.as_path()/\"csv\"/\"file-0.csv\"}\n",
    "        sleep(10)\n",
    "\n",
    "        for csv_to_upload in sorted((test_s3_path.as_path() / \"csv\").glob(\"*.csv\")):\n",
    "            display(f\"Uploading {csv_to_upload}\")\n",
    "            upload_to_s3_with_retry(\n",
    "                csv_to_upload,\n",
    "                from_local_response.presigned[\"url\"],\n",
    "                from_local_response.presigned[\"fields\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_s3_bucket_and_path_from_uri(uri: Union[str, Path]) -> Tuple[Bucket, str]:\n",
    "    \"\"\"Get bucket object and s3 path from s3 uri\n",
    "\n",
    "    Args:\n",
    "        uri: full s3 uri\n",
    "\n",
    "    Returns:\n",
    "        The bucket object and the s3 path as a tuple\n",
    "    \"\"\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket_name, s3_path = get_s3_bucket_name_and_folder_from_uri(str(uri))\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    return bucket, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = get_s3_bucket_and_path_from_uri(\n",
    "    uri=\"s3://test-airt-service/account_312571_events\"\n",
    ")\n",
    "assert actual[0]\n",
    "assert actual[0].name == \"test-airt-service\"\n",
    "assert actual[1] == \"account_312571_events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf2971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
