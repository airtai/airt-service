{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for base airflow executor\n",
    "output-file: baseairflowexecutor.html\n",
    "title: Base Airflow Executor\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp airflow.base_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb570e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-10-20 06:45:11.408 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from airt_service.sanitizer import sanitized_print\n",
    "from airt.executor.subcommand import (\n",
    "    ModelExecutor,\n",
    "    CLICommandBase,\n",
    ")\n",
    "from airt.helpers import slugify\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "\n",
    "from airt_service.airflow.utils import create_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from time import sleep\n",
    "\n",
    "from airt.executor.subcommand import ClassCLICommand, SimpleCLICommand\n",
    "from airt.testing import activate_by_import\n",
    "from airt_service.airflow.utils import trigger_dag, unpause_dag, wait_for_run_to_complete, list_dag_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819a243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmpvz84nj5l/data'), PosixPath('/tmp/tmpvz84nj5l/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('local:/tmp/tmpvz84nj5l/data', 'local:/tmp/tmpvz84nj5l/model')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_test_paths(d: str) -> Tuple[str, str]:\n",
    "    d = Path(d)\n",
    "    paths = [d / sd for sd in [\"data\", \"model\"]]\n",
    "    display(f\"{paths=}\")\n",
    "\n",
    "    # create tmp dirs for data and model\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RemotePaths: data_path is \"read-only\", while model_path can be used for both reading and writing between calls\n",
    "    return tuple(f\"local:{p}\" for p in paths)\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "data_path_url, model_path_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class BaseAirflowExecutor(ModelExecutor):\n",
    "    def _create_step_template(self, step: CLICommandBase, **kwargs):\n",
    "        \"\"\"Create template for step\n",
    "\n",
    "        Args:\n",
    "            step: step to create template\n",
    "            kwargs: keyword arguments for step\n",
    "        Returns:\n",
    "            Template for step\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")\n",
    "\n",
    "    def _create_dag_template(\n",
    "        self,\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create DAG template with steps as tasks\n",
    "\n",
    "        Args:\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments to pass to steps' CLI\n",
    "        Returns:\n",
    "            Generated DAG with steps as tasks\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")\n",
    "\n",
    "    def schedule(\n",
    "        self,\n",
    "        *,\n",
    "        schedule_interval: Optional[Union[str, timedelta]] = None,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs,\n",
    "    ) -> Path:\n",
    "        \"\"\"Create scheduled DAG in airflow\n",
    "\n",
    "        Args:\n",
    "            schedule_interval: schedule interval of DAG as string or timedelta object\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            Path in which dag file is stored\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        *,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Path, str]:\n",
    "        \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "        Args:\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            A tuple which contains dag file path and run id\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82ec700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "dag_template = \"\"\"import datetime\n",
    "from textwrap import dedent\n",
    "\n",
    "# The DAG object; we'll need this to instantiate a DAG\n",
    "from airflow import DAG\n",
    "\n",
    "# Operators; we need this to operate!\n",
    "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
    "import azure.batch.models as batchmodels\n",
    "from airflow.providers.microsoft.azure.operators.batch import AzureBatchOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
    "with DAG(\n",
    "    '{dag_name}',\n",
    "    # These args will get passed on to each operator\n",
    "    # You can override them on a per-task basis during operator initialization\n",
    "    default_args={{\n",
    "        'schedule_interval': {schedule_interval},\n",
    "        'depends_on_past': False,\n",
    "        'email': ['info@airt.ai'],\n",
    "        'email_on_failure': False,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 1,\n",
    "        'retry_delay': datetime.timedelta(minutes=5),\n",
    "        # 'queue': 'queue',\n",
    "        # 'pool': 'backfill',\n",
    "        # 'priority_weight': 10,\n",
    "        # 'end_date': datetime.datetime(2016, 1, 1),\n",
    "        # 'wait_for_downstream': False,\n",
    "        # 'sla': datetime.timedelta(hours=2),\n",
    "        # 'execution_timeout': datetime.timedelta(seconds=300),\n",
    "        # 'on_failure_callback': some_function,\n",
    "        # 'on_success_callback': some_other_function,\n",
    "        # 'on_retry_callback': another_function,\n",
    "        # 'sla_miss_callback': yet_another_function,\n",
    "        # 'trigger_rule': 'all_success'\n",
    "    }},\n",
    "    description='{description}',\n",
    "    start_date={start_date},\n",
    "    catchup=False,\n",
    "    tags={tags},\n",
    "    is_paused_upon_creation=False,\n",
    ") as dag:\n",
    "\n",
    "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import datetime\n",
      "from textwrap import dedent\n",
      "\n",
      "# The DAG object; we'll need this to instantiate a DAG\n",
      "from airflow import DAG\n",
      "\n",
      "# Operators; we need this to operate!\n",
      "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
      "import azure.batch.models as batchmodels\n",
      "from airflow.providers.microsoft.azure.operators.batch import AzureBatchOperator\n",
      "from airflow.operators.bash import BashOperator\n",
      "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
      "with DAG(\n",
      "    'random',\n",
      "    # These args will get passed on to each operator\n",
      "    # You can override them on a per-task basis during operator initialization\n",
      "    default_args={\n",
      "        'schedule_interval': '@daily',\n",
      "        'depends_on_past': False,\n",
      "        'email': ['info@airt.ai'],\n",
      "        'email_on_failure': False,\n",
      "        'email_on_retry': False,\n",
      "        'retries': 1,\n",
      "        'retry_delay': datetime.timedelta(minutes=5),\n",
      "        # 'queue': 'queue',\n",
      "        # 'pool': 'backfill',\n",
      "        # 'priority_weight': 10,\n",
      "        # 'end_date': datetime.datetime(2016, 1, 1),\n",
      "        # 'wait_for_downstream': False,\n",
      "        # 'sla': datetime.timedelta(hours=2),\n",
      "        # 'execution_timeout': datetime.timedelta(seconds=300),\n",
      "        # 'on_failure_callback': some_function,\n",
      "        # 'on_success_callback': some_other_function,\n",
      "        # 'on_retry_callback': another_function,\n",
      "        # 'sla_miss_callback': yet_another_function,\n",
      "        # 'trigger_rule': 'all_success'\n",
      "    },\n",
      "    description='test description',\n",
      "    start_date=datetime.datetime(2022, 10, 20, 6, 45, 12, 800818),\n",
      "    catchup=False,\n",
      "    tags=['test_tag'],\n",
      "    is_paused_upon_creation=False,\n",
      ") as dag:\n",
      "\n",
      "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanitized_print(\n",
    "    dag_template.format(\n",
    "        dag_name=\"random\",\n",
    "        schedule_interval=\"'@daily'\",\n",
    "        start_date=datetime.utcnow().today().__repr__(),\n",
    "        description=\"test description\",\n",
    "        tags=([\"test_tag\"]).__repr__(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff73d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag_id(self: BaseAirflowExecutor, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Create dag id by combining steps CLIs and their arguments\n",
    "\n",
    "    Args:\n",
    "        kwargs: keyword arguments needed by steps\n",
    "    Returns:\n",
    "        Created dag id\n",
    "    \"\"\"\n",
    "    return slugify(\"_\".join([step.to_cli(**kwargs) for step in self.steps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "    ),\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa44cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmpo5g49vy6/data'), PosixPath('/tmp/tmpo5g49vy6/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test-executor-my_test_executor-f-data-path-urllocaltmptmpo5g49vy6data-model-path-urllocaltmptmpo5g49vy6model_test-executor-my_test_executor-g-data-path-urllocaltmptmpo5g49vy6data-model-path-urllocaltmptmpo5g49vy6model'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    expected = slugify(\n",
    "        steps[0].to_cli(data_path_url=data_path_url, model_path_url=model_path_url)\n",
    "        + \"_\"\n",
    "        + steps[1].to_cli(data_path_url=data_path_url, model_path_url=model_path_url)\n",
    "    )\n",
    "    abe = BaseAirflowExecutor(\n",
    "        steps=steps,\n",
    "    )\n",
    "    actual = abe._create_dag_id(\n",
    "        data_path_url=data_path_url, model_path_url=model_path_url\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_jinja2_template_kwargs(\n",
    "    self: BaseAirflowExecutor, **kwargs\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert kwargs into jinja2 compatible template kwargs\n",
    "\n",
    "    Args:\n",
    "        kwargs: keyword arguments to convert\n",
    "    Returns:\n",
    "        A dict of jinja2 template formatted kwargs\n",
    "    \"\"\"\n",
    "    formatted_kwargs = {}\n",
    "    for key, value in kwargs.items():\n",
    "        formatted_kwargs[key] = (\n",
    "            \"{{{{ dag_run.conf['\"\n",
    "            + key\n",
    "            + \"'] if '\"\n",
    "            + key\n",
    "            + \"' in dag_run.conf else \"\n",
    "            + value.__repr__()\n",
    "            + \" }}}}\"\n",
    "        )\n",
    "    return formatted_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f65ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmp4jo015lo/data'), PosixPath('/tmp/tmp4jo015lo/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'data_path_url': \"{{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else 'local:/tmp/tmp4jo015lo/data' }}}}\",\n",
       " 'model_path_url': \"{{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else 'local:/tmp/tmp4jo015lo/model' }}}}\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    expected = {\n",
    "        \"data_path_url\": \"{{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else '\"\n",
    "        + data_path_url\n",
    "        + \"' }}}}\",\n",
    "        \"model_path_url\": \"{{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else '\"\n",
    "        + model_path_url\n",
    "        + \"' }}}}\",\n",
    "    }\n",
    "\n",
    "    actual = abe._create_jinja2_template_kwargs(\n",
    "        data_path_url=data_path_url, model_path_url=model_path_url\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec82535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT ADD EXPORT - This patch cell is being used for testing purpose\n",
    "# Actual _create_step_template should be implemented by child class\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_step_template(self: BaseAirflowExecutor, step: CLICommandBase, **kwargs):\n",
    "    \"\"\"\n",
    "    Create template for step\n",
    "\n",
    "    Args:\n",
    "        step: step to create template\n",
    "        kwargs: keyword arguments for step\n",
    "    Returns:\n",
    "        Template for step\n",
    "    \"\"\"\n",
    "    triple_quote = \"'''\"\n",
    "    formatted_kwargs = self._create_jinja2_template_kwargs(**kwargs)\n",
    "\n",
    "    cli_command = step.to_cli(**formatted_kwargs)\n",
    "    task_id = slugify(step.to_cli(**kwargs))\n",
    "\n",
    "    task = f\"\"\"BashOperator(task_id=\"{task_id}\", bash_command={triple_quote}{cli_command}{triple_quote})\"\"\"\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef2f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmp1ucduhf_/data'), PosixPath('/tmp/tmp1ucduhf_/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BashOperator(task_id=\"test-executor-my_test_executor-f-data-path-urllocaltmptmp1ucduhf_data-model-path-urllocaltmptmp1ucduhf_model\", bash_command=\\'\\'\\'test-executor my_test_executor f --data-path-url={{{{ dag_run.conf[\\'data_path_url\\'] if \\'data_path_url\\' in dag_run.conf else \\'local:/tmp/tmp1ucduhf_/data\\' }}}} --model-path-url={{{{ dag_run.conf[\\'model_path_url\\'] if \\'model_path_url\\' in dag_run.conf else \\'local:/tmp/tmp1ucduhf_/model\\' }}}}\\'\\'\\')'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    actual = abe._create_step_template(\n",
    "        steps[0], data_path_url=data_path_url, model_path_url=model_path_url\n",
    "    )\n",
    "    display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT ADD EXPORT - This patch cell is being used for testing purpose\n",
    "# Actual _create_step_template should be implemented by child class\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag_template(\n",
    "    self: BaseAirflowExecutor,\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create DAG template with steps as tasks\n",
    "\n",
    "    Args:\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments to pass to steps' CLI\n",
    "    Returns:\n",
    "        Generated DAG with steps as tasks\n",
    "    \"\"\"\n",
    "    curr_dag_template = dag_template\n",
    "\n",
    "    downstream_tasks = \"\"\n",
    "    newline = \"\\n\"\n",
    "    tab = \" \" * 4\n",
    "\n",
    "    existing_tasks = 0\n",
    "    for i, step in enumerate(self.steps):\n",
    "        if on_step_start is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_start, step_count=i+1, **kwargs)}\"\"\"\n",
    "            existing_tasks += 1\n",
    "\n",
    "        curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(step, **kwargs)}\"\"\"\n",
    "        existing_tasks += 1\n",
    "\n",
    "        if on_step_end is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_end, step_count=i+1, **kwargs)}\"\"\"\n",
    "            existing_tasks += 1\n",
    "\n",
    "    downstream_tasks = f\"{newline}{tab}\" + \" >> \".join(\n",
    "        [f\"t{i}\" for i in range(1, existing_tasks + 1)]\n",
    "    )\n",
    "    curr_dag_template += downstream_tasks\n",
    "\n",
    "    return curr_dag_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmpx_vn59vn/data'), PosixPath('/tmp/tmpx_vn59vn/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import datetime\n",
      "from textwrap import dedent\n",
      "\n",
      "# The DAG object; we'll need this to instantiate a DAG\n",
      "from airflow import DAG\n",
      "\n",
      "# Operators; we need this to operate!\n",
      "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
      "import azure.batch.models as batchmodels\n",
      "from airflow.providers.microsoft.azure.operators.batch import AzureBatchOperator\n",
      "from airflow.operators.bash import BashOperator\n",
      "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
      "with DAG(\n",
      "    '{dag_name}',\n",
      "    # These args will get passed on to each operator\n",
      "    # You can override them on a per-task basis during operator initialization\n",
      "    default_args={{\n",
      "        'schedule_interval': {schedule_interval},\n",
      "        'depends_on_past': False,\n",
      "        'email': ['info@airt.ai'],\n",
      "        'email_on_failure': False,\n",
      "        'email_on_retry': False,\n",
      "        'retries': 1,\n",
      "        'retry_delay': datetime.timedelta(minutes=5),\n",
      "        # 'queue': 'queue',\n",
      "        # 'pool': 'backfill',\n",
      "        # 'priority_weight': 10,\n",
      "        # 'end_date': datetime.datetime(2016, 1, 1),\n",
      "        # 'wait_for_downstream': False,\n",
      "        # 'sla': datetime.timedelta(hours=2),\n",
      "        # 'execution_timeout': datetime.timedelta(seconds=300),\n",
      "        # 'on_failure_callback': some_function,\n",
      "        # 'on_success_callback': some_other_function,\n",
      "        # 'on_retry_callback': another_function,\n",
      "        # 'sla_miss_callback': yet_another_function,\n",
      "        # 'trigger_rule': 'all_success'\n",
      "    }},\n",
      "    description='{description}',\n",
      "    start_date={start_date},\n",
      "    catchup=False,\n",
      "    tags={tags},\n",
      "    is_paused_upon_creation=False,\n",
      ") as dag:\n",
      "\n",
      "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
      "\n",
      "    t1 = BashOperator(task_id=\"sleep-1\", bash_command='''sleep {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 1 }}}}''')\n",
      "    t2 = BashOperator(task_id=\"test-executor-my_test_executor-f-data-path-urllocaltmptmpx_vn59vndata-model-path-urllocaltmptmpx_vn59vnmodel\", bash_command='''test-executor my_test_executor f --data-path-url={{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else 'local:/tmp/tmpx_vn59vn/data' }}}} --model-path-url={{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else 'local:/tmp/tmpx_vn59vn/model' }}}}''')\n",
      "    t3 = BashOperator(task_id=\"echo-step-1-completed\", bash_command='''echo step {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 1 }}}} completed''')\n",
      "    t4 = BashOperator(task_id=\"sleep-2\", bash_command='''sleep {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 2 }}}}''')\n",
      "    t5 = BashOperator(task_id=\"test-executor-my_test_executor-g-data-path-urllocaltmptmpx_vn59vndata-model-path-urllocaltmptmpx_vn59vnmodel\", bash_command='''test-executor my_test_executor g --data-path-url={{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else 'local:/tmp/tmpx_vn59vn/data' }}}} --model-path-url={{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else 'local:/tmp/tmpx_vn59vn/model' }}}}''')\n",
      "    t6 = BashOperator(task_id=\"echo-step-2-completed\", bash_command='''echo step {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 2 }}}} completed''')\n",
      "    t1 >> t2 >> t3 >> t4 >> t5 >> t6\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    kwargs = {\"data_path_url\": data_path_url, \"model_path_url\": model_path_url}\n",
    "\n",
    "    abe = BaseAirflowExecutor(steps=steps)\n",
    "\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "    sanitized_print(\n",
    "        abe._create_dag_template(\n",
    "            on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ad503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag(\n",
    "    self: BaseAirflowExecutor,\n",
    "    *,\n",
    "    schedule_interval: Optional[str] = None,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[str, Path]:\n",
    "    \"\"\"Create DAG in airflow\n",
    "\n",
    "    Args:\n",
    "        schedule_interval: schedule interval of DAG as string\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        A tuple of dag id and dag file path\n",
    "    \"\"\"\n",
    "    if isinstance(tags, str):\n",
    "        tags = [tags]\n",
    "\n",
    "    curr_dag_template = self._create_dag_template(\n",
    "        on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "    )\n",
    "    dag_id = self._create_dag_id(**kwargs)\n",
    "    dag_file_path = create_dag(\n",
    "        dag_id=dag_id,\n",
    "        dag_definition_template=curr_dag_template,\n",
    "        schedule_interval=schedule_interval,\n",
    "        start_date=datetime.utcnow().today().__repr__(),\n",
    "        description=description,\n",
    "        tags=tags.__repr__(),\n",
    "    )\n",
    "\n",
    "    return dag_id, dag_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f031b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmp3m4x5s7t/data'), PosixPath('/tmp/tmp3m4x5s7t/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=PosixPath('/root/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel', '--conf', '{}', '--run-id', 'airt-service__2022-10-20T06:45:31.469563'], returncode=0, stdout='[\\x1b[34m2022-10-20 06:45:32,422\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel @ 2022-10-20T06:45:32+00:00: airt-service__2022-10-20T06:45:31.469563, state:queued, queued_at: 2022-10-20 06:45:32.481137+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp3m4x5s7tdata-model-path-urllocaltmptmp3m4x5s7tmodel', 'run_id': 'airt-service__2022-10-20T06:45:31.469563', 'state': 'running', 'execution_date': '2022-10-20T06:45:32+00:00', 'start_date': '2022-10-20T06:45:33.033998+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-10-20T06:45:31.469563'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = BaseAirflowExecutor(steps=steps)\n",
    "    dag_id, dag_file_path = abe._create_dag(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=None,\n",
    "        description=\"test description\",\n",
    "        tags=[\"test_tag\"],\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe89860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def schedule(\n",
    "    self: BaseAirflowExecutor,\n",
    "    *,\n",
    "    schedule_interval: Optional[Union[str, timedelta]] = None,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> Path:\n",
    "    \"\"\"Create scheduled DAG in airflow\n",
    "\n",
    "    Args:\n",
    "        schedule_interval: schedule interval of DAG as string or timedelta object\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        Path in which dag file is stored\n",
    "    \"\"\"\n",
    "    schedule_interval = (\n",
    "        f\"'{schedule_interval}'\"\n",
    "        if isinstance(schedule_interval, str)\n",
    "        else schedule_interval.__repr__()\n",
    "    )\n",
    "    dag_id, dag_file_path = self._create_dag(\n",
    "        schedule_interval=schedule_interval,\n",
    "        description=description,\n",
    "        tags=tags,\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return dag_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5135ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paths=[PosixPath('/tmp/tmpw8xwrxp1/data'), PosixPath('/tmp/tmpw8xwrxp1/model')]\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=PosixPath('/root/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model_test-executor-my_test_executor-g-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model_test-executor-my_test_executor-g-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model', '--conf', '{}', '--run-id', 'airt-service__2022-10-20T06:46:06.626565'], returncode=0, stdout='[\\x1b[34m2022-10-20 06:46:07,529\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model_test-executor-my_test_executor-g-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model @ 2022-10-20T06:46:07+00:00: airt-service__2022-10-20T06:46:06.626565, state:queued, queued_at: 2022-10-20 06:46:07.589198+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model_test-executor-my_test_executor-g-data-path-urllocaltmptmpw8xwrxp1data-model-path-urllocaltmptmpw8xwrxp1model', 'run_id': 'airt-service__2022-10-20T06:46:06.626565', 'state': 'running', 'execution_date': '2022-10-20T06:46:07+00:00', 'start_date': '2022-10-20T06:46:08.520235+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-10-20T06:46:06.626565'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = BaseAirflowExecutor(steps=steps)\n",
    "    dag_file_path = abe.schedule(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=timedelta(days=7),\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e0991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
