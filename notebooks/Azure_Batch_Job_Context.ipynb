{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Context classes to run jobs in Azure Batch\n",
    "output-file: azure_batch_job_context.html\n",
    "title: Azure Batch Job Context\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp batch_job_components.azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 07:54:31.649615: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[WARNING] airt.testing.activate_by_import: Failed to set gpu memory limit for tf; This could happen because of no gpu availability\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from os import environ\n",
    "from typing import *\n",
    "\n",
    "import azure.batch.models as batchmodels\n",
    "from airt.logger import get_logger\n",
    "\n",
    "import airt_service\n",
    "import airt_service.sanitizer\n",
    "from airt_service.azure.batch_utils import azure_batch_create_job\n",
    "from airt_service.azure.utils import get_batch_account_pool_job_names\n",
    "from airt_service.batch_job_components.base import BatchJobContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _pytest.monkeypatch import MonkeyPatch\n",
    "from fastcore.utils import patch\n",
    "\n",
    "from airt_service.helpers import set_env_variable_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_setattr = MonkeyPatch.setattr\n",
    "\n",
    "\n",
    "@patch\n",
    "def setattr(self: MonkeyPatch, *args, **kwargs):\n",
    "    global logger\n",
    "    old_setattr(self, *args, **kwargs)\n",
    "    logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AzureBatchJobContext(BatchJobContext):\n",
    "    \"\"\"A class for creating AzureBatchJobContext\"\"\"\n",
    "\n",
    "    def __init__(self, task: str, **kwargs):\n",
    "        \"\"\"Azure Batch Job Context\n",
    "\n",
    "        Do not use __init__, please use factory method `create` to initiate object\n",
    "        \"\"\"\n",
    "        BatchJobContext.__init__(self, task=task)\n",
    "        self.region = kwargs[\"region\"]\n",
    "\n",
    "    def create_job(self, command: str, environment_vars: Dict[str, str]):\n",
    "        \"\"\"Create a new job\n",
    "\n",
    "        Args:\n",
    "            command: Command to execute in job\n",
    "            environment_vars: Environment vars to set in the container\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"{self.__class__.__name__}.create_job({self=}, {command=}, {environment_vars=})\"\n",
    "        )\n",
    "        # ToDo: We have batch accounts available only in northeurope for now\n",
    "        region = \"northeurope\"\n",
    "        (\n",
    "            batch_account_name,\n",
    "            batch_pool_name,\n",
    "            batch_job_name,\n",
    "        ) = airt_service.azure.utils.get_batch_account_pool_job_names(self.task, region)\n",
    "\n",
    "        tag = \"dev\"\n",
    "        if environ[\"DOMAIN\"] == \"api.airt.ai\":\n",
    "            tag = \"latest\"\n",
    "\n",
    "        container_settings = batchmodels.TaskContainerSettings(\n",
    "            image_name=f\"ghcr.io/airtai/airt-service:{tag}\"\n",
    "        )\n",
    "\n",
    "        airt_service.azure.batch_utils.azure_batch_create_job(\n",
    "            command=command,\n",
    "            batch_job_name=batch_job_name,\n",
    "            batch_pool_name=batch_pool_name,\n",
    "            batch_account_name=batch_account_name,\n",
    "            region=region,\n",
    "            container_settings=container_settings,\n",
    "            environment_vars=environment_vars,\n",
    "        )\n",
    "\n",
    "\n",
    "AzureBatchJobContext.add_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job_components.base: Entering AzureBatchJobContext(task=csv_processing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AzureBatchJobContext(task=csv_processing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AzureBatchJobContext.create_job(self=AzureBatchJobContext(task=csv_processing), command='process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data', environment_vars={'AZURE_SUBSCRIPTION_ID': '************************************'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"kwargs={'command': 'process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data', 'batch_job_name': 'batch_job_name', 'batch_pool_name': 'batch_pool_name', 'batch_account_name': 'batch_account_name', 'region': 'northeurope', 'container_settings': <azure.batch.models._models_py3.TaskContainerSettings object>, 'environment_vars': {'AZURE_SUBSCRIPTION_ID': '************************************'}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job_components.base: Exiting AzureBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    }
   ],
   "source": [
    "with MonkeyPatch.context() as monkeypatch:\n",
    "    batch_account_name = \"batch_account_name\"\n",
    "    batch_pool_name = \"batch_pool_name\"\n",
    "    batch_job_name = \"batch_job_name\"\n",
    "    region = \"northeurope\"\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.azure.utils.get_batch_account_pool_job_names\",\n",
    "        lambda task, region: (\n",
    "            batch_account_name,\n",
    "            batch_pool_name,\n",
    "            batch_job_name,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def test_patch_create_job(*args, **kwargs):\n",
    "        display(f\"{kwargs=}\")\n",
    "        assert kwargs[\"batch_account_name\"] == batch_account_name\n",
    "        assert kwargs[\"batch_pool_name\"] == batch_pool_name\n",
    "        assert kwargs[\"batch_job_name\"] == batch_job_name\n",
    "        assert kwargs[\"region\"] == region\n",
    "        assert (\n",
    "            kwargs[\"command\"]\n",
    "            == f\"process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data\"\n",
    "        )\n",
    "        assert \"AZURE_SUBSCRIPTION_ID\" in kwargs[\"environment_vars\"]\n",
    "\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.azure.batch_utils.azure_batch_create_job\", test_patch_create_job\n",
    "    )\n",
    "\n",
    "    with BatchJobContext.create(\n",
    "        \"csv_processing\", cloud_provider=\"azure\", region=region\n",
    "    ) as batch_ctx:\n",
    "        display(batch_ctx)\n",
    "        assert batch_ctx.__class__.__name__ == \"AzureBatchJobContext\"\n",
    "        batch_ctx.create_job(\n",
    "            command=\"process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data\",\n",
    "            environment_vars={\n",
    "                \"AZURE_SUBSCRIPTION_ID\": \"random_value\",\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
