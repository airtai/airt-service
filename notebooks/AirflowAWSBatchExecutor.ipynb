{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6022dd65",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for airflow aws batch executor\n",
    "output-file: airflowawsbatchexecutor.html\n",
    "title: Airflow AWS Batch Executor\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp airflow.aws_batch_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb570e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import shlex\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import yaml\n",
    "from airt.executor.subcommand import ClassCLICommand, CLICommandBase\n",
    "from airt.helpers import slugify\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "from fastcore.script import Param, call_parse\n",
    "\n",
    "from airt_service.airflow.base_executor import BaseAirflowExecutor, dag_template\n",
    "from airt_service.airflow.utils import trigger_dag, wait_for_run_to_complete\n",
    "from airt_service.aws.batch_utils import (\n",
    "    _create_default_batch_environment_config,\n",
    "    create_testing_batch_environment_ctx,\n",
    ")\n",
    "from airt_service.aws.utils import get_batch_environment_arns, get_queue_definition_arns\n",
    "from airt_service.batch_job import get_environment_vars_for_batch_job\n",
    "from airt_service.helpers import generate_random_string\n",
    "from airt_service.sanitizer import sanitized_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "\n",
    "import pytest\n",
    "from airt.executor.subcommand import SimpleCLICommand\n",
    "from airt.testing import activate_by_import\n",
    "\n",
    "from airt_service.airflow.utils import list_dag_runs\n",
    "from airt_service.db.models import create_user_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "def setup_test_paths(td: str) -> Tuple[str, str]:\n",
    "    d = Path(td)\n",
    "    paths = [d / sd for sd in [\"data\", \"model\"]]\n",
    "    sanitized_print(f\"{paths=}\")\n",
    "\n",
    "    # create tmp dirs for data and model\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RemotePaths: data_path is \"read-only\", while model_path can be used for both reading and writing between calls\n",
    "    return tuple(f\"local:{p}\" for p in paths)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63982167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "data_path_url, model_path_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de287057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_EXEC_ENVIRONMENT = \"preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AirflowAWSBatchExecutor(BaseAirflowExecutor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps: List[CLICommandBase],\n",
    "        region: str,\n",
    "        exec_environments: Optional[List[Optional[str]]] = None,\n",
    "        batch_environment_arn_path: Optional[Union[str, Path]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructs a new AirflowAWSBatchExecutor instance\n",
    "\n",
    "        Args:\n",
    "            steps: List of instances of either ClassCLICommand or SimpleCLICommand\n",
    "            region: Region to execute\n",
    "            exec_environments: List of execution environments to execute steps\n",
    "        \"\"\"\n",
    "        self.region = region\n",
    "        self.batch_environment_arn_path = batch_environment_arn_path\n",
    "\n",
    "        if exec_environments is None:\n",
    "            exec_environments = [DEFAULT_EXEC_ENVIRONMENT] * len(steps)\n",
    "\n",
    "        if len(exec_environments) != len(steps):\n",
    "            raise ValueError(\n",
    "                f\"len(exec_environments)={len(exec_environments)} != len(steps){len(steps)}\"\n",
    "            )\n",
    "\n",
    "        existing_exec_environments = list(\n",
    "            get_batch_environment_arns(\n",
    "                self.region, self.batch_environment_arn_path\n",
    "            ).keys()\n",
    "        )\n",
    "\n",
    "        self.exec_environments = []\n",
    "        for exec_env in exec_environments:\n",
    "            if exec_env is None:\n",
    "                self.exec_environments.append(DEFAULT_EXEC_ENVIRONMENT)\n",
    "                continue\n",
    "            if exec_env not in existing_exec_environments:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid value {exec_env} given for exec environment; Allowed values are {existing_exec_environments}\"\n",
    "                )\n",
    "            self.exec_environments.append(exec_env)\n",
    "\n",
    "        self.exec_environments = [\n",
    "            exec_env if exec_env is not None else DEFAULT_EXEC_ENVIRONMENT\n",
    "            for exec_env in exec_environments\n",
    "        ]\n",
    "\n",
    "        super(AirflowAWSBatchExecutor, self).__init__(steps)\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        *,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Tuple[Path, str]:\n",
    "        \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "        Args:\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            A tuple which contains dag file path and run id\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_batch_environment_arns(folder: Path):\n",
    "    test_batch_environment_arns = {\n",
    "        \"eu-west-1\": {\n",
    "            task: {\n",
    "                arn: \"arn:aws:batch:placeholder\"\n",
    "                for arn in [\n",
    "                    \"compute_environment_arn\",\n",
    "                    \"job_definition_arn\",\n",
    "                    \"job_queue_arn\",\n",
    "                ]\n",
    "            }\n",
    "            for task in [\"csv_processing\", \"predictions\", \"preprocessing\", \"training\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    folder = Path(folder)\n",
    "    test_batch_environment_arn_path = folder / \"batch_environment.yml\"\n",
    "    with open(test_batch_environment_arn_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_arns, f, default_flow_style=False)\n",
    "\n",
    "    return test_batch_environment_arn_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "    ),\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_arn_path = save_test_batch_environment_arns(d)\n",
    "    abe = AirflowAWSBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "    )\n",
    "    display(abe.exec_environments)\n",
    "    assert abe.exec_environments == [\"preprocessing\"] * len(steps)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAWSBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"preprocessing\"],\n",
    "            batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "        )\n",
    "    display(e)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAWSBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"gibberish\", \"gibberish\"],\n",
    "            batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "        )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec82535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def _create_step_template(\n",
    "    self: AirflowAWSBatchExecutor,\n",
    "    step: CLICommandBase,\n",
    "    exec_environment: str,\n",
    "    **kwargs: Any,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create template for step\n",
    "\n",
    "    Args:\n",
    "        step: step to create template\n",
    "        kwargs: keyword arguments for step\n",
    "    Returns:\n",
    "        Template for step\n",
    "    \"\"\"\n",
    "    cli_command = step.to_cli(**kwargs)\n",
    "    task_id = slugify(cli_command)\n",
    "\n",
    "    batch_environment_vars = [\n",
    "        dict(name=name, value=value)\n",
    "        for name, value in get_environment_vars_for_batch_job().items()\n",
    "    ]\n",
    "    overrides = (\n",
    "        dict(\n",
    "            command=shlex.split(cli_command),\n",
    "            environment=batch_environment_vars,\n",
    "        )\n",
    "        .__repr__()\n",
    "        .replace(\"{\", \"{{\")\n",
    "        .replace(\"}\", \"}}\")\n",
    "    )\n",
    "\n",
    "    job_queue_arn, job_definition_arn = get_queue_definition_arns(\n",
    "        task=exec_environment,\n",
    "        region=self.region,\n",
    "        batch_environment_arn_path=self.batch_environment_arn_path,\n",
    "    )\n",
    "\n",
    "    task = f\"\"\"BatchOperator(task_id='{task_id}', job_definition=\"{job_definition_arn}\", job_queue=\"{job_queue_arn}\", job_name=\"{task_id}\", overrides={overrides})\"\"\"\n",
    "\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_arn_path = save_test_batch_environment_arns(d)\n",
    "    abe = AirflowAWSBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "    )\n",
    "    actual = abe._create_step_template(\n",
    "        steps[0],\n",
    "        exec_environment=\"training\",\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "    )\n",
    "    display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def _create_dag_template(\n",
    "    self: AirflowAWSBatchExecutor,\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs: Any,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create DAG template with steps as tasks\n",
    "\n",
    "    Args:\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments to pass to steps' CLI\n",
    "    Returns:\n",
    "        Generated DAG with steps as tasks\n",
    "    \"\"\"\n",
    "    curr_dag_template = dag_template\n",
    "\n",
    "    downstream_tasks = \"\"\n",
    "    newline = \"\\n\"\n",
    "    tab = \" \" * 4\n",
    "\n",
    "    existing_tasks = 0\n",
    "    for i, step in enumerate(self.steps):\n",
    "        if on_step_start is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_start, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "        curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(step, self.exec_environments[i], **kwargs)}\"\"\"  # type: ignore\n",
    "        existing_tasks += 1\n",
    "\n",
    "        if on_step_end is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_end, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "    downstream_tasks = f\"{newline}{tab}\" + \" >> \".join(\n",
    "        [f\"t{i}\" for i in range(1, existing_tasks + 1)]\n",
    "    )\n",
    "    curr_dag_template += downstream_tasks\n",
    "\n",
    "    return curr_dag_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065626",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    test_batch_environment_arn_path = save_test_batch_environment_arns(d)\n",
    "\n",
    "    kwargs = {\"data_path_url\": data_path_url, \"model_path_url\": model_path_url}\n",
    "\n",
    "    abe = AirflowAWSBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_arn_path=test_batch_environment_arn_path,\n",
    "    )\n",
    "\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "    sanitized_print(\n",
    "        abe._create_dag_template(\n",
    "            on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f031b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# Test case for AirflowAWSBatchExecutor._create_dag\n",
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", None]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    env_config_path = td / \"env_config.yaml\"\n",
    "    created_env_info_path = td / \"output_file.yaml\"\n",
    "    _create_default_batch_environment_config(\n",
    "        prefix=f\"airflow_batch_create_dag_testing_{generate_random_string()}\",\n",
    "        output_path=env_config_path,\n",
    "        regions=[region],\n",
    "    )\n",
    "\n",
    "    with open(env_config_path) as f:\n",
    "        env_config = yaml.safe_load(f)\n",
    "    display(f\"{env_config=}\")\n",
    "    with create_testing_batch_environment_ctx(\n",
    "        input_yaml_path=env_config_path, output_yaml_path=created_env_info_path\n",
    "    ):\n",
    "        abe = AirflowAWSBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=exec_environments,\n",
    "            batch_environment_arn_path=created_env_info_path,\n",
    "        )\n",
    "        dag_id, dag_file_path = abe._create_dag(\n",
    "            data_path_url=data_path_url,\n",
    "            model_path_url=model_path_url,\n",
    "            #         schedule_interval=\"@weekly\",\n",
    "            schedule_interval=None,\n",
    "            description=\"test description\",\n",
    "            tags=\"test_tag\",\n",
    "            on_step_start=on_step_start,\n",
    "            on_step_end=on_step_end,\n",
    "        )\n",
    "\n",
    "        display(f\"{dag_file_path=}\")\n",
    "        dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "        sleep(15)\n",
    "\n",
    "        dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "        display(f\"{dag_runs=}\")\n",
    "\n",
    "        run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "        #     run_id = dag_runs[0][\"run_id\"]\n",
    "        display(run_id)\n",
    "        state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "        display(state)\n",
    "        dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5135ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# Test case for AirflowAWSBatchExecutor.schedule\n",
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    exec_environments = [\"csv_processing\", \"preprocessing\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    env_config_path = td / \"env_config.yaml\"\n",
    "    created_env_info_path = td / \"output_file.yaml\"\n",
    "    _create_default_batch_environment_config(\n",
    "        prefix=f\"airflow_batch_schedule_testing_{generate_random_string()}\",\n",
    "        output_path=env_config_path,\n",
    "        regions=[region],\n",
    "    )\n",
    "\n",
    "    with open(env_config_path) as f:\n",
    "        env_config = yaml.safe_load(f)\n",
    "    display(f\"{env_config=}\")\n",
    "    with create_testing_batch_environment_ctx(\n",
    "        input_yaml_path=env_config_path, output_yaml_path=created_env_info_path\n",
    "    ):\n",
    "        abe = AirflowAWSBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=exec_environments,\n",
    "            batch_environment_arn_path=created_env_info_path,\n",
    "        )\n",
    "        dag_file_path = abe.schedule(\n",
    "            data_path_url=data_path_url,\n",
    "            model_path_url=model_path_url,\n",
    "            #         schedule_interval=\"@weekly\",\n",
    "            schedule_interval=timedelta(days=7),\n",
    "            description=\"test description\",\n",
    "            tags=\"test_tag\",\n",
    "            on_step_start=on_step_start,\n",
    "            on_step_end=on_step_end,\n",
    "        )\n",
    "\n",
    "        display(f\"{dag_file_path=}\")\n",
    "        dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "        sleep(15)\n",
    "\n",
    "        dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "        display(f\"{dag_runs=}\")\n",
    "\n",
    "        run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "        #     run_id = dag_runs[0][\"run_id\"]\n",
    "        display(run_id)\n",
    "        state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "        display(state)\n",
    "        dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch  # type: ignore\n",
    "def execute(\n",
    "    self: AirflowAWSBatchExecutor,\n",
    "    *,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs: Any,\n",
    ") -> Tuple[Path, str]:\n",
    "    \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "    Args:\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        A tuple which contains dag file path and run id\n",
    "    \"\"\"\n",
    "    schedule_interval = None\n",
    "    dag_id, dag_file_path = self._create_dag(\n",
    "        schedule_interval=schedule_interval,\n",
    "        description=description,\n",
    "        tags=tags,\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "    return dag_file_path, run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "region = \"eu-west-1\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", \"predictions\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    env_config_path = td / \"env_config.yaml\"\n",
    "    created_env_info_path = td / \"output_file.yaml\"\n",
    "    _create_default_batch_environment_config(\n",
    "        prefix=f\"airflow_batch_execute_testing_{generate_random_string()}\",\n",
    "        output_path=env_config_path,\n",
    "        regions=[region],\n",
    "    )\n",
    "\n",
    "    with open(env_config_path) as f:\n",
    "        env_config = yaml.safe_load(f)\n",
    "    display(f\"{env_config=}\")\n",
    "    with create_testing_batch_environment_ctx(\n",
    "        input_yaml_path=env_config_path, output_yaml_path=created_env_info_path\n",
    "    ):\n",
    "        abe = AirflowAWSBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=exec_environments,\n",
    "            batch_environment_arn_path=created_env_info_path,\n",
    "        )\n",
    "\n",
    "        dag_file_path, run_id = abe.execute(\n",
    "            description=\"test description\",\n",
    "            tags=\"test_tag\",\n",
    "            on_step_start=on_step_start,\n",
    "            on_step_end=on_step_end,\n",
    "            data_path_url=data_path_url,\n",
    "            model_path_url=model_path_url,\n",
    "        )\n",
    "        display(dag_file_path)\n",
    "        display(run_id)\n",
    "\n",
    "        dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "        state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "        display(state)\n",
    "        dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _test_aws_batch_executor(region: str = \"eu-west-1\") -> None:\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "        steps = [\n",
    "            ClassCLICommand(\n",
    "                executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "            )\n",
    "        ]\n",
    "        exec_environments = [\"training\"]\n",
    "\n",
    "        td = Path(d)\n",
    "        env_config_path = td / \"env_config.yaml\"\n",
    "        created_env_info_path = td / \"output_file.yaml\"\n",
    "\n",
    "        prefix = f\"airflow_batch_execute_testing_{generate_random_string()}\"\n",
    "        regions = [region]\n",
    "        _create_default_batch_environment_config(\n",
    "            prefix=prefix,\n",
    "            output_path=env_config_path,\n",
    "            regions=regions,\n",
    "        )\n",
    "\n",
    "        with open(env_config_path) as f:\n",
    "            env_config = yaml.safe_load(f)\n",
    "        logger.info(f\"{env_config=}\")\n",
    "        with create_testing_batch_environment_ctx(\n",
    "            input_yaml_path=env_config_path, output_yaml_path=created_env_info_path  # type: ignore\n",
    "        ):\n",
    "            abe = AirflowAWSBatchExecutor(\n",
    "                steps=steps,\n",
    "                region=region,\n",
    "                exec_environments=exec_environments,  # type: ignore\n",
    "                batch_environment_arn_path=created_env_info_path,\n",
    "            )\n",
    "\n",
    "            dag_file_path, run_id = abe.execute(\n",
    "                description=\"test description\",\n",
    "                tags=\"test_tag\",\n",
    "                data_path_url=data_path_url,\n",
    "                model_path_url=model_path_url,\n",
    "            )\n",
    "            logger.info(f\"{dag_file_path=}\")\n",
    "            logger.info(f\"{run_id=}\")\n",
    "\n",
    "            dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "            state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=1200)\n",
    "            logger.info(f\"{state=}\")\n",
    "            dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse  # type: ignore\n",
    "def test_aws_batch_executor(region: Param(\"region\", str) = \"eu-west-1\"):  # type: ignore\n",
    "    \"\"\"\n",
    "    Create throw away environment for aws batch and execute airflow batch executor\n",
    "    \"\"\"\n",
    "    _test_aws_batch_executor(region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "test_aws_batch_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bda18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
