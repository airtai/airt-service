{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to process csv files\n",
    "output-file: datasource_csv.html\n",
    "title: DataSource CSV\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.5.1\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.10.0\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "from typing import *\n",
    "\n",
    "from airt.data.importers import import_csv\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath\n",
    "from fastcore.script import Param, call_parse\n",
    "from fastcore.utils import *\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt_service.aws.utils import create_s3_datasource_path\n",
    "from airt_service.azure.utils import create_azure_blob_storage_datasource_path\n",
    "from airt_service.data.datasource import DataSource\n",
    "from airt_service.data.utils import (\n",
    "    calculate_azure_data_object_folder_size_and_path,\n",
    "    calculate_data_object_folder_size_and_path,\n",
    "    calculate_data_object_pulled_on,\n",
    ")\n",
    "from airt_service.db.models import DataBlob, get_session_with_context\n",
    "from airt_service.helpers import truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "from time import sleep\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from airt_service.aws.utils import upload_to_s3_with_retry\n",
    "from airt_service.constants import DS_HEAD_FILE_NAME, METADATA_FOLDER_PATH\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.db.models import User, create_user_for_testing, get_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exjcallcpz'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing()\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse  # type: ignore\n",
    "def process_csv(\n",
    "    datablob_id: Param(\"datablob_id\", int),  # type: ignore\n",
    "    datasource_id: Param(\"datasource_id\", int),  # type: ignore\n",
    "    *,\n",
    "    deduplicate_data: Param(\"deduplicate_data\", bool) = False,  # type: ignore\n",
    "    index_column: Param(\"index_column\", str),  # type: ignore\n",
    "    sort_by: Param(\"sort_by\", str),  # type: ignore\n",
    "    blocksize: Param(\"blocksize\", str) = \"256MB\",  # type: ignore\n",
    "    kwargs_json: Param(\"kwargs_json\", str) = \"{}\",  # type: ignore\n",
    ") -> None:\n",
    "    \"\"\"Download the user uploaded CSV from S3, run import_csv against it and finally upload the processed parquet files to S3\n",
    "\n",
    "    Args:\n",
    "        datablob_id: Datablob id\n",
    "        datasource_id: Datasource id\n",
    "        deduplicate_data: If set to True (default value False), then duplicate rows are removed while uploading\n",
    "        index_column: Name of the column to use as index and partition the data\n",
    "        sort_by: Name of the column to sort data within the same index value\n",
    "        blocksize: Size of partition\n",
    "        kwargs_json: Parameters as json string which are passed to the **dask.dataframe.read_csv()** function,\n",
    "            typically params for underlining **pd.read_csv()** from Pandas.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"process_csv({datablob_id=}, {datasource_id=}): processing user uploaded csv file for {datablob_id=} and uploading parquet back to S3 for {datasource_id=}\"\n",
    "    )\n",
    "    with get_session_with_context() as session:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.id == datablob_id)\n",
    "        ).one()\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.id == datasource_id)\n",
    "        ).one()\n",
    "\n",
    "        # Following is needed if datablob was created from user uploaded csv files\n",
    "        calculate_data_object_folder_size_and_path(datablob)\n",
    "\n",
    "        datasource.error = None\n",
    "        datasource.completed_steps = 0\n",
    "        datasource.folder_size = None\n",
    "        datasource.no_of_rows = None\n",
    "        datasource.path = None\n",
    "        datasource.hash = None\n",
    "\n",
    "        try:\n",
    "            source_path = datablob.path\n",
    "            if datasource.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_datasource_path(\n",
    "                    user_id=datasource.user.id,\n",
    "                    datasource_id=datasource.id,\n",
    "                    region=datasource.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datasource.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_datasource_path(\n",
    "                    user_id=datasource.user.id,\n",
    "                    datasource_id=datasource.id,\n",
    "                    region=datasource.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "            logger.info(\n",
    "                f\"process_csv({datablob_id=}, {datasource_id=}): step 1/4: downloading user uploaded file from bucket {source_path}\"\n",
    "            )\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=source_path,\n",
    "                pull_on_enter=True,\n",
    "                push_on_exit=False,\n",
    "                exist_ok=True,\n",
    "                parents=False,\n",
    "            ) as source_s3_path:\n",
    "                calculate_data_object_pulled_on(datasource)\n",
    "                if len(list(source_s3_path.as_path().iterdir())) == 0:\n",
    "                    raise ValueError(\"Files not found\")\n",
    "                logger.info(\n",
    "                    f\"process_csv({datablob_id=}, {datasource_id=}): step 2/4: running import_csv()\"\n",
    "                )\n",
    "\n",
    "                with RemotePath.from_url(\n",
    "                    remote_url=destination_remote_url,\n",
    "                    pull_on_enter=False,\n",
    "                    push_on_exit=True,\n",
    "                    exist_ok=True,\n",
    "                    parents=True,\n",
    "                ) as destination_path:\n",
    "                    processed_path = destination_path.as_path()\n",
    "                    kwargs = json.loads(kwargs_json)\n",
    "                    try:\n",
    "                        sort_by = json.loads(sort_by)\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    import_csv(\n",
    "                        input_path=source_s3_path.as_path(),\n",
    "                        output_path=processed_path,\n",
    "                        index_column=index_column,\n",
    "                        sort_by=sort_by,\n",
    "                        blocksize=blocksize,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "                    datasource.calculate_properties(processed_path)\n",
    "\n",
    "                    if not len(list(processed_path.glob(\"*.parquet\"))) > 0:\n",
    "                        raise ValueError(\n",
    "                            f\"processing failed; parquet files not available\"\n",
    "                        )\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"process_csv({datablob_id=}, {datasource_id=}): step 3/4: uploading parquet files back to path {destination_path}\"\n",
    "                    )\n",
    "            logger.info(\n",
    "                f\"process_csv({datablob_id=}, {datasource_id=}): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\"\n",
    "            )\n",
    "\n",
    "            calculate_data_object_folder_size_and_path(datasource)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"process_csv({datablob_id=}, {datasource_id=}): error: {str(e)}\"\n",
    "            )\n",
    "            datasource.error = truncate(str(e))\n",
    "        logger.info(f\"process_csv({datablob_id=}, {datasource_id=}): completed\")\n",
    "        session.add(datablob)\n",
    "        session.add(datasource)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('2a0a74eb-4fa2-4590-ba7b-a9b8b0f95934'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'AWSAccessKeyId': '********************', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AccountId DefinitionId        OccurredTime  \\\n",
       "__null_dask_index__                                               \n",
       "0                       312571   loadTests2 2019-12-31 21:30:02   \n",
       "1                       312571   loadTests3 2020-01-03 23:53:22   \n",
       "2                       312571   loadTests1 2020-01-07 02:16:42   \n",
       "3                       312571   loadTests2 2020-01-10 04:40:02   \n",
       "4                       312571   loadTests3 2020-01-13 07:03:22   \n",
       "\n",
       "                     OccurredTimeTicks  PersonId  \n",
       "__null_dask_index__                               \n",
       "0                        1577836802678         2  \n",
       "1                        1578104602678         2  \n",
       "2                        1578372402678         2  \n",
       "3                        1578640202678         2  \n",
       "4                        1578908002678         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/file.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccountId,DefinitionId,OccurredTime,OccurredTimeTicks,PersonId\n",
      "312571,loadTests2,2019-12-31 21:30:02,1577836802678,2\n",
      "312571,loadTests3,2020-01-03 23:53:22,1578104602678,2\n",
      "312571,loadTests1,2020-01-07 02:16:42,1578372402678,2\n",
      "312571,loadTests2,2020-01-10 04:40:02,1578640202678,2\n",
      "312571,loadTests3,2020-01-13 07:03:22,1578908002678,2\n",
      "312571,loadTests1,2020-01-16 09:26:42,1579175802678,2\n",
      "312571,loadTests2,2020-01-19 11:50:02,1579443602678,2\n",
      "312571,loadTests3,2020-01-22 14:13:22,1579711402678,2\n",
      "312571,loadTests1,2020-01-25 16:36:42,1579979202678,2\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_2xbwdvga\n"
     ]
    }
   ],
   "source": [
    "# Create a csv datasource and upload single csv file using presigned url\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_local_request = FromLocalRequest(\n",
    "        path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "    )\n",
    "    from_local_response = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        df = pd.read_parquet(test_s3_path.as_path())\n",
    "        display(df.head())\n",
    "        df.to_csv(test_s3_path.as_path() / \"file.csv\", index=False)\n",
    "        display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "        !head -n 10 {test_s3_path.as_path()/\"file.csv\"}\n",
    "\n",
    "        upload_to_s3_with_retry(\n",
    "            test_s3_path.as_path() / \"file.csv\",\n",
    "            from_local_response.presigned[\"url\"],\n",
    "            from_local_response.presigned[\"fields\"],\n",
    "        )\n",
    "\n",
    "    datablob_id = (\n",
    "        session.exec(select(DataBlob).where(DataBlob.uuid == from_local_response.uuid))\n",
    "        .one()\n",
    "        .id\n",
    "    )\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    datasource_id = datasource.id\n",
    "    user_id = user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): processing user uploaded csv file for datablob_id=51 and uploading parquet back to S3 for datasource_id=29\n",
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/132/datablob/51\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/132/datablob/51\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/132/datablob/51 locally in /tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/132/datablob/51 to /tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4\n",
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/132/datasource/29\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/132/datasource/29 locally in /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4/file.csv,..., /tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4/file.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:34175' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:34833' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:40797' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and ['OccurredTime'].\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and ['OccurredTime'].\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/132/datasource/29, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k to s3://kumaran-airt-service-eu-west-1/132/datasource/29\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_fzoh5g_k\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1132datablob51_cached_ntxr64d4\n",
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] __main__: process_csv(datablob_id=51, datasource_id=29): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=29, uuid=UUID('c98bd928-e0bf-4072-ac77-a6e9d2c8db8a'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/132/datasource/29', created=datetime.datetime(2022, 10, 27, 8, 10, 18), user_id=132, pulled_on=datetime.datetime(2022, 10, 27, 8, 10, 25), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/132/datasource/29\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_kx3y8q1r\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/132/datasource/29 locally in /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_kx3y8q1r\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/132/datasource/29 to /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_kx3y8q1r\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1132datasource29_cached_kx3y8q1r\n"
     ]
    }
   ],
   "source": [
    "# Test process_csv\n",
    "\n",
    "process_csv(\n",
    "    datablob_id=datablob_id,\n",
    "    datasource_id=datasource_id,\n",
    "    deduplicate_data=True,\n",
    "    index_column=\"PersonId\",\n",
    "    sort_by=json.dumps([\"OccurredTime\"]),\n",
    "    blocksize=\"256MB\",\n",
    "    kwargs_json=json.dumps(\n",
    "        dict(\n",
    "            usecols=[0, 1, 2, 3, 4],\n",
    "            parse_dates=[\"OccurredTime\"],\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_id)\n",
    "    ).one()\n",
    "    display(datasource)\n",
    "    assert datasource.folder_size == 6619982, datasource.folder_size\n",
    "    assert datasource.no_of_rows == 498961\n",
    "    assert (\n",
    "        datasource.path\n",
    "        == f\"s3://{environ['STORAGE_BUCKET_PREFIX']}-eu-west-1/{user_id}/datasource/{datasource.id}\"\n",
    "    ), datasource.path\n",
    "    assert datasource.hash == \"1dd8ee7a0f96a48110dec6e25891d18d\", datasource.hash\n",
    "\n",
    "    # tests for datasource head and dtypes\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"{datasource.path}\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as test_s3_info_path:\n",
    "        processed_test_s3_info_path = test_s3_info_path.as_path()\n",
    "\n",
    "        head_df = pd.read_parquet(\n",
    "            processed_test_s3_info_path / METADATA_FOLDER_PATH / DS_HEAD_FILE_NAME\n",
    "        )\n",
    "        assert head_df.index.name == \"PersonId\"\n",
    "        assert head_df.shape == (10, 4)\n",
    "\n",
    "        dtypes_dict = head_df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        assert dtypes_dict == {\n",
    "            \"AccountId\": \"int64\",\n",
    "            \"DefinitionId\": \"object\",\n",
    "            \"OccurredTime\": \"datetime64[ns]\",\n",
    "            \"OccurredTimeTicks\": \"int64\",\n",
    "        }, dtypes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('705db79a-c658-40d4-8160-4989eda1e326'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'AWSAccessKeyId': '********************', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccountId DefinitionId        OccurredTime  OccurredTimeTicks  PersonId\n",
       "0     312571   loadTests2 2019-12-31 21:30:02      1577836802678         2\n",
       "1     312571   loadTests3 2020-01-03 23:53:22      1578104602678         2\n",
       "2     312571   loadTests1 2020-01-07 02:16:42      1578372402678         2\n",
       "3     312571   loadTests2 2020-01-10 04:40:02      1578640202678         2\n",
       "4     312571   loadTests3 2020-01-13 07:03:22      1578908002678         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-4.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-3.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-2.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-0.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-1.csv')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccountId,DefinitionId,OccurredTime,OccurredTimeTicks,PersonId\r\n",
      "312571,loadTests2,2019-12-31 21:30:02,1577836802678,2\r\n",
      "312571,loadTests3,2020-01-03 23:53:22,1578104602678,2\r\n",
      "312571,loadTests1,2020-01-07 02:16:42,1578372402678,2\r\n",
      "312571,loadTests2,2020-01-10 04:40:02,1578640202678,2\r\n",
      "312571,loadTests3,2020-01-13 07:03:22,1578908002678,2\r\n",
      "312571,loadTests1,2020-01-16 09:26:42,1579175802678,2\r\n",
      "312571,loadTests2,2020-01-19 11:50:02,1579443602678,2\r\n",
      "312571,loadTests3,2020-01-22 14:13:22,1579711402678,2\r\n",
      "312571,loadTests1,2020-01-25 16:36:42,1579979202678,2\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-0.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c/csv/file-4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_1trnug7c\n"
     ]
    }
   ],
   "source": [
    "# Create a csv datasource and upload multiple csv files using presigned url\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_local_request = FromLocalRequest(\n",
    "        path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "    )\n",
    "    from_local_response = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        ddf = dd.read_parquet(test_s3_path.as_path())\n",
    "        display(ddf.head())\n",
    "        ddf.to_csv(test_s3_path.as_path() / \"csv\" / \"file-*.csv\", index=False)\n",
    "        display(list((test_s3_path.as_path() / \"csv\").glob(\"*\")))\n",
    "        !head -n 10 {test_s3_path.as_path()/\"csv\"/\"file-0.csv\"}\n",
    "        sleep(10)\n",
    "\n",
    "        for csv_to_upload in sorted((test_s3_path.as_path() / \"csv\").glob(\"*.csv\")):\n",
    "            display(f\"Uploading {csv_to_upload}\")\n",
    "            upload_to_s3_with_retry(\n",
    "                csv_to_upload,\n",
    "                from_local_response.presigned[\"url\"],\n",
    "                from_local_response.presigned[\"fields\"],\n",
    "            )\n",
    "\n",
    "    datablob_id = (\n",
    "        session.exec(select(DataBlob).where(DataBlob.uuid == from_local_response.uuid))\n",
    "        .one()\n",
    "        .id\n",
    "    )\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"us-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    datasource_id = datasource.id\n",
    "    user_id = user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): processing user uploaded csv file for datablob_id=53 and uploading parquet back to S3 for datasource_id=32\n",
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/132/datablob/53\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/132/datablob/53\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/132/datablob/53 locally in /tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/132/datablob/53 to /tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7\n",
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-us-west-1/132/datasource/32\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-us-west-1/132/datasource/32 locally in /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7/file-0.csv,..., /tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7/file-4.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:40321' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:35187' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:39059' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-us-west-1/132/datasource/32, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a to s3://kumaran-airt-service-us-west-1/132/datasource/32\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_a0snmh2a\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1132datablob53_cached_at_1drr7\n",
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] __main__: process_csv(datablob_id=53, datasource_id=32): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=32, uuid=UUID('0e742604-2180-4b49-9ff7-fb23f178cf44'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='us-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-us-west-1/132/datasource/32', created=datetime.datetime(2022, 10, 27, 8, 12, 43), user_id=132, pulled_on=datetime.datetime(2022, 10, 27, 8, 12, 52), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-us-west-1/132/datasource/32\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_5zkc9719\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-us-west-1/132/datasource/32 locally in /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_5zkc9719\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-us-west-1/132/datasource/32 to /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_5zkc9719\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-us-west-1132datasource32_cached_5zkc9719\n"
     ]
    }
   ],
   "source": [
    "# Test process_csv\n",
    "process_csv(\n",
    "    datablob_id=datablob_id,\n",
    "    datasource_id=datasource_id,\n",
    "    deduplicate_data=True,\n",
    "    index_column=\"PersonId\",\n",
    "    sort_by=\"OccurredTime\",\n",
    "    blocksize=\"256MB\",\n",
    "    kwargs_json=json.dumps(\n",
    "        dict(\n",
    "            usecols=[0, 1, 2, 3, 4],\n",
    "            parse_dates=[\"OccurredTime\"],\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_id)\n",
    "    ).one()\n",
    "    display(datasource)\n",
    "    assert datasource.folder_size == 6619982, datasource.folder_size\n",
    "    assert datasource.no_of_rows == 498961\n",
    "    assert (\n",
    "        datasource.path\n",
    "        == f\"s3://{environ['STORAGE_BUCKET_PREFIX']}-us-west-1/{user_id}/datasource/{datasource.id}\"\n",
    "    ), datasource.path\n",
    "    assert datasource.hash == \"1dd8ee7a0f96a48110dec6e25891d18d\", datasource.hash\n",
    "\n",
    "    # tests for datasource head and dtypes\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"{datasource.path}\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as test_s3_info_path:\n",
    "        processed_test_s3_info_path = test_s3_info_path.as_path()\n",
    "\n",
    "        head_df = pd.read_parquet(\n",
    "            processed_test_s3_info_path / METADATA_FOLDER_PATH / DS_HEAD_FILE_NAME\n",
    "        )\n",
    "        assert head_df.index.name == \"PersonId\"\n",
    "        assert head_df.shape == (10, 4)\n",
    "\n",
    "        dtypes_dict = head_df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "        assert dtypes_dict == {\n",
    "            \"AccountId\": \"int64\",\n",
    "            \"DefinitionId\": \"object\",\n",
    "            \"OccurredTime\": \"datetime64[ns]\",\n",
    "            \"OccurredTimeTicks\": \"int64\",\n",
    "        }, dtypes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
