{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for airflow bash executor\n",
    "output-file: airflowbashexecutor.html\n",
    "title: Airflow Bash Executor\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp airflow.bash_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb570e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-09-13 11:19:34.246 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from airt.executor.subcommand import CLICommandBase\n",
    "from airt.helpers import slugify\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "\n",
    "from airt_service.airflow.base_executor import BaseAirflowExecutor, dag_template\n",
    "from airt_service.airflow.utils import trigger_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "\n",
    "from sqlmodel import select\n",
    "\n",
    "from airt.executor.subcommand import SimpleCLICommand, ClassCLICommand\n",
    "from airt.testing import activate_by_import\n",
    "from airt_service.airflow.utils import wait_for_run_to_complete, list_dag_runs\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.db.models import (\n",
    "    User,\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    "    get_session_with_context,\n",
    "    DataBlob,\n",
    "    DataSource,\n",
    ")\n",
    "from airt_service.helpers import commit_or_rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e5aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jnmqrtsece'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpkuje5ha9/data'), Path('/tmp/tmpkuje5ha9/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local:/tmp/tmpkuje5ha9/data', 'local:/tmp/tmpkuje5ha9/model')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_test_paths(d: str) -> Tuple[str, str]:\n",
    "    d = Path(d)\n",
    "    paths = [d / sd for sd in [\"data\", \"model\"]]\n",
    "    print(f\"{paths=}\")\n",
    "\n",
    "    # create tmp dirs for data and model\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RemotePaths: data_path is \"read-only\", while model_path can be used for both reading and writing between calls\n",
    "    return tuple(f\"local:{p}\" for p in paths)\n",
    "\n",
    "\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "data_path_url, model_path_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AirflowBashExecutor(BaseAirflowExecutor):\n",
    "    def execute(\n",
    "        self,\n",
    "        *,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs\n",
    "    ) -> Tuple[Path, str]:\n",
    "        \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "        Args:\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            A tuple which contains dag file path and run id\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec82535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_step_template(self: AirflowBashExecutor, step: CLICommandBase, **kwargs):\n",
    "    \"\"\"\n",
    "    Create template for step\n",
    "\n",
    "    Args:\n",
    "        step: step to create template\n",
    "        kwargs: keyword arguments for step\n",
    "    Returns:\n",
    "        Template for step\n",
    "    \"\"\"\n",
    "    triple_quote = \"'''\"\n",
    "    formatted_kwargs = self._create_jinja2_template_kwargs(**kwargs)\n",
    "\n",
    "    cli_command = step.to_cli(**formatted_kwargs)\n",
    "    task_id = slugify(step.to_cli(**kwargs))\n",
    "\n",
    "    task = f\"\"\"BashOperator(task_id=\"{task_id}\", bash_command={triple_quote}{cli_command}{triple_quote})\"\"\"\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9bb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "    ),\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmp6vfu5r5b/data'), Path('/tmp/tmp6vfu5r5b/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BashOperator(task_id=\"test-executor-my_test_executor-f-data-path-urllocaltmptmp6vfu5r5bdata-model-path-urllocaltmptmp6vfu5r5bmodel\", bash_command=\\'\\'\\'test-executor my_test_executor f --data-path-url={{{{ dag_run.conf[\\'data_path_url\\'] if \\'data_path_url\\' in dag_run.conf else \\'local:/tmp/tmp6vfu5r5b/data\\' }}}} --model-path-url={{{{ dag_run.conf[\\'model_path_url\\'] if \\'model_path_url\\' in dag_run.conf else \\'local:/tmp/tmp6vfu5r5b/model\\' }}}}\\'\\'\\')'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    abe = AirflowBashExecutor(\n",
    "        steps=steps,\n",
    "    )\n",
    "    actual = abe._create_step_template(\n",
    "        steps[0], data_path_url=data_path_url, model_path_url=model_path_url\n",
    "    )\n",
    "    display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag_template(\n",
    "    self: BaseAirflowExecutor,\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create DAG template with steps as tasks\n",
    "\n",
    "    Args:\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments to pass to steps' CLI\n",
    "    Returns:\n",
    "        Generated DAG with steps as tasks\n",
    "    \"\"\"\n",
    "    curr_dag_template = dag_template\n",
    "\n",
    "    downstream_tasks = \"\"\n",
    "    newline = \"\\n\"\n",
    "    tab = \" \" * 4\n",
    "\n",
    "    existing_tasks = 0\n",
    "    for i, step in enumerate(self.steps):\n",
    "        if on_step_start is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_start, step_count=i+1, **kwargs)}\"\"\"\n",
    "            existing_tasks += 1\n",
    "\n",
    "        curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(step, **kwargs)}\"\"\"\n",
    "        existing_tasks += 1\n",
    "\n",
    "        if on_step_end is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_end, step_count=i+1, **kwargs)}\"\"\"\n",
    "            existing_tasks += 1\n",
    "\n",
    "    downstream_tasks = f\"{newline}{tab}\" + \" >> \".join(\n",
    "        [f\"t{i}\" for i in range(1, existing_tasks + 1)]\n",
    "    )\n",
    "    curr_dag_template += downstream_tasks\n",
    "\n",
    "    return curr_dag_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpvv7l9r_3/data'), Path('/tmp/tmpvv7l9r_3/model')]\n",
      "import datetime\n",
      "from textwrap import dedent\n",
      "\n",
      "# The DAG object; we'll need this to instantiate a DAG\n",
      "from airflow import DAG\n",
      "\n",
      "# Operators; we need this to operate!\n",
      "from airflow.providers.amazon.aws.operators.batch import BatchOperator\n",
      "from airflow.operators.bash import BashOperator\n",
      "from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n",
      "with DAG(\n",
      "    '{dag_name}',\n",
      "    # These args will get passed on to each operator\n",
      "    # You can override them on a per-task basis during operator initialization\n",
      "    default_args={{\n",
      "        'schedule_interval': {schedule_interval},\n",
      "        'depends_on_past': False,\n",
      "        'email': ['info@airt.ai'],\n",
      "        'email_on_failure': False,\n",
      "        'email_on_retry': False,\n",
      "        'retries': 1,\n",
      "        'retry_delay': datetime.timedelta(minutes=5),\n",
      "        # 'queue': 'queue',\n",
      "        # 'pool': 'backfill',\n",
      "        # 'priority_weight': 10,\n",
      "        # 'end_date': datetime.datetime(2016, 1, 1),\n",
      "        # 'wait_for_downstream': False,\n",
      "        # 'sla': datetime.timedelta(hours=2),\n",
      "        # 'execution_timeout': datetime.timedelta(seconds=300),\n",
      "        # 'on_failure_callback': some_function,\n",
      "        # 'on_success_callback': some_other_function,\n",
      "        # 'on_retry_callback': another_function,\n",
      "        # 'sla_miss_callback': yet_another_function,\n",
      "        # 'trigger_rule': 'all_success'\n",
      "    }},\n",
      "    description='{description}',\n",
      "    start_date={start_date},\n",
      "    catchup=False,\n",
      "    tags={tags},\n",
      "    is_paused_upon_creation=False,\n",
      ") as dag:\n",
      "\n",
      "    # t1, t2 and t3 are examples of tasks created by instantiating operators\n",
      "\n",
      "    t1 = BashOperator(task_id=\"sleep-1\", bash_command='''sleep {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 1 }}}}''')\n",
      "    t2 = BashOperator(task_id=\"test-executor-my_test_executor-f-data-path-urllocaltmptmpvv7l9r_3data-model-path-urllocaltmptmpvv7l9r_3model\", bash_command='''test-executor my_test_executor f --data-path-url={{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else 'local:/tmp/tmpvv7l9r_3/data' }}}} --model-path-url={{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else 'local:/tmp/tmpvv7l9r_3/model' }}}}''')\n",
      "    t3 = BashOperator(task_id=\"echo-step-1-completed\", bash_command='''echo step {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 1 }}}} completed''')\n",
      "    t4 = BashOperator(task_id=\"sleep-2\", bash_command='''sleep {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 2 }}}}''')\n",
      "    t5 = BashOperator(task_id=\"test-executor-my_test_executor-g-data-path-urllocaltmptmpvv7l9r_3data-model-path-urllocaltmptmpvv7l9r_3model\", bash_command='''test-executor my_test_executor g --data-path-url={{{{ dag_run.conf['data_path_url'] if 'data_path_url' in dag_run.conf else 'local:/tmp/tmpvv7l9r_3/data' }}}} --model-path-url={{{{ dag_run.conf['model_path_url'] if 'model_path_url' in dag_run.conf else 'local:/tmp/tmpvv7l9r_3/model' }}}}''')\n",
      "    t6 = BashOperator(task_id=\"echo-step-2-completed\", bash_command='''echo step {{{{ dag_run.conf['step_count'] if 'step_count' in dag_run.conf else 2 }}}} completed''')\n",
      "    t1 >> t2 >> t3 >> t4 >> t5 >> t6\n"
     ]
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    kwargs = {\"data_path_url\": data_path_url, \"model_path_url\": model_path_url}\n",
    "\n",
    "    abe = AirflowBashExecutor(steps=steps)\n",
    "\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "    print(\n",
    "        abe._create_dag_template(\n",
    "            on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f031b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmp4n319w1w/data'), Path('/tmp/tmp4n319w1w/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=Path('/root/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel', '--conf', '{}', '--run-id', 'airt-service__2022-09-13T11:19:55.443395'], returncode=0, stdout='[\\x1b[34m2022-09-13 11:19:56,330\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m40} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel @ 2022-09-13T11:19:56+00:00: airt-service__2022-09-13T11:19:55.443395, externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmp4n319w1wdata-model-path-urllocaltmptmp4n319w1wmodel', 'run_id': 'airt-service__2022-09-13T11:19:55.443395', 'state': 'running', 'execution_date': '2022-09-13T11:19:56+00:00', 'start_date': '2022-09-13T11:19:56.577585+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-09-13T11:19:55.443395'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test case for AirflowBashExecutor._create_dag\n",
    "\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = AirflowBashExecutor(steps=steps)\n",
    "    dag_id, dag_file_path = abe._create_dag(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=None,\n",
    "        description=\"test description\",\n",
    "        tags=[\"test_tag\"],\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmp30zowdz5/data'), Path('/tmp/tmp30zowdz5/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=Path('/root/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model_test-executor-my_test_executor-g-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model_test-executor-my_test_executor-g-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model', '--conf', '{}', '--run-id', 'airt-service__2022-09-13T11:20:31.164363'], returncode=0, stdout='[\\x1b[34m2022-09-13 11:20:32,135\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m40} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model_test-executor-my_test_executor-g-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model @ 2022-09-13T11:20:32+00:00: airt-service__2022-09-13T11:20:31.164363, externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model_test-executor-my_test_executor-g-data-path-urllocaltmptmp30zowdz5data-model-path-urllocaltmptmp30zowdz5model', 'run_id': 'airt-service__2022-09-13T11:20:31.164363', 'state': 'running', 'execution_date': '2022-09-13T11:20:32+00:00', 'start_date': '2022-09-13T11:20:32.171002+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-09-13T11:20:31.164363'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test case for AirflowBashExecutor.schedule\n",
    "\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = AirflowBashExecutor(steps=steps)\n",
    "    dag_file_path = abe.schedule(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=timedelta(days=7),\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def execute(\n",
    "    self: AirflowBashExecutor,\n",
    "    *,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs\n",
    ") -> Tuple[Path, str]:\n",
    "    \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "    Args:\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        A tuple which contains dag file path and run id\n",
    "    \"\"\"\n",
    "    schedule_interval = None\n",
    "    dag_id, dag_file_path = self._create_dag(\n",
    "        schedule_interval=schedule_interval,\n",
    "        description=description,\n",
    "        tags=tags,\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    conf = {key: value for key, value in kwargs.items()}\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf=conf)\n",
    "    return dag_file_path, run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef996a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpyzt4fehf/data'), Path('/tmp/tmpyzt4fehf/model')]\n",
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel', '--conf', '{\"data_path_url\": \"local:/tmp/tmpyzt4fehf/data\", \"model_path_url\": \"local:/tmp/tmpyzt4fehf/model\"}', '--run-id', 'airt-service__2022-09-13T11:20:51.257666'], returncode=0, stdout='[\\x1b[34m2022-09-13 11:20:52,274\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m40} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel @ 2022-09-13T11:20:52+00:00: airt-service__2022-09-13T11:20:51.257666, externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel', 'run_id': 'airt-service__2022-09-13T11:20:51.257666', 'state': 'running', 'execution_date': '2022-09-13T11:20:52+00:00', 'start_date': '2022-09-13T11:20:52.888510+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path('/root/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel_test-executor-my_test_executor-g-data-path-urllocaltmptmpyzt4fehfdata-model-path-urllocaltmptmpyzt4fehfmodel.py')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-09-13T11:20:51.257666'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    steps = [\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        ),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = AirflowBashExecutor(\n",
    "        steps=steps,\n",
    "    )\n",
    "\n",
    "    dag_file_path, run_id = abe.execute(\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "    )\n",
    "    display(dag_file_path)\n",
    "    display(run_id)\n",
    "\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=179, uuid=UUID('c611001b-c8b4-49bf-9c52-a647a3de8bca'), type='s3', uri='s3://AKIAY7RRHQ4BEOUZVSE3:8VUSagSJGSMO9cQVpqWM6NJ9THoD8wtTC7EMRF+9@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 9, 13, 11, 21, 36), user_id=137, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3_pull {datablob_id}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 's3_pull-179', '--conf', '{\"datablob_id\": 179}', '--run-id', 'airt-service__2022-09-13T11:21:51.736479'], returncode=0, stdout='[\\x1b[34m2022-09-13 11:21:52,964\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m40} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun s3_pull-179 @ 2022-09-13T11:21:52+00:00: airt-service__2022-09-13T11:21:51.736479, externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:525 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 's3_pull-179', 'run_id': 'airt-service__2022-09-13T11:21:51.736479', 'state': 'running', 'execution_date': '2022-09-13T11:21:52+00:00', 'start_date': '2022-09-13T11:21:53.034039+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path('/root/airflow/dags/s3_pull-179.py')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-09-13T11:21:51.736479'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "    display(datablob)\n",
    "    datablob_command = \"s3_pull {datablob_id}\"\n",
    "    display(datablob_command)\n",
    "\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=datablob_command),\n",
    "    ]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    abe = AirflowBashExecutor(\n",
    "        steps=steps,\n",
    "    )\n",
    "\n",
    "    dag_file_path, run_id = abe.execute(\n",
    "        description=\"test description\",\n",
    "        tags=[\"test_tag\"],\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        datablob_id=datablob.id,\n",
    "    )\n",
    "    display(dag_file_path)\n",
    "    display(run_id)\n",
    "\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bda18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User sent request. Following lines part of airt-service before returning response to user\n",
    "\n",
    "# abd_factory  = AirflowBashExecutor(executor_cli=,) # Does nothing except setting instance variables\n",
    "# abd_factory.schedule(data_path_url, model_path_url, period=\"7 days\") # Generates and save dag, unpauses it, will run periodically\n",
    "\n",
    "\n",
    "# abd_factory.execute(data_path_url, model_path_url) # Generates and saves dag and runs it immediately?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e0991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
