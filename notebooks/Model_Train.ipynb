{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fdd41b12",
   "metadata": {},
   "source": [
    "---\n",
    "description: Router to have routes to train, evaluate model and start prediction\n",
    "output-file: model_train.html\n",
    "title: Model Train Router\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227df1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import shutil\n",
    "from datetime import timedelta\n",
    "from typing import *\n",
    "import uuid\n",
    "\n",
    "from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks\n",
    "from fastcore.script import call_parse, Param\n",
    "from pydantic import BaseModel\n",
    "from sqlalchemy.exc import NoResultFound\n",
    "from sqlmodel import Session, select\n",
    "\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.auth import get_current_active_user\n",
    "from airt_service.aws.utils import create_s3_prediction_path\n",
    "from airt_service.azure.utils import create_azure_blob_storage_prediction_path\n",
    "from airt_service.batch_job import create_batch_job\n",
    "from airt_service.constants import METADATA_FOLDER_PATH\n",
    "from airt_service.data.datasource import get_datasource_responses\n",
    "from airt_service.db.models import get_session, get_session_with_context\n",
    "from airt_service.db.models import User, DataSource, DataSourceSelect, Model\n",
    "from airt_service.db.models import ModelRead, Prediction, PredictionRead\n",
    "from airt_service.errors import HTTPError, ERRORS\n",
    "from airt_service.helpers import truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.4.4\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.9.0\n",
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os import environ\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.background_task import execute_cli\n",
    "from airt_service.data.csv import process_csv\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.db.models import (\n",
    "    create_user_for_testing,\n",
    "    get_session_with_context,\n",
    "    SubscriptionType,\n",
    "    DataBlob,\n",
    ")\n",
    "from airt_service.aws.utils import upload_to_s3_with_retry\n",
    "from airt_service.constants import METADATA_FOLDER_PATH\n",
    "from airt_service.users import (\n",
    "    generate_mfa_url,\n",
    "    activate_mfa,\n",
    "    ActivateMFARequest,\n",
    "    disable_mfa,\n",
    ")\n",
    "from airt_service.helpers import commit_or_rollback, set_env_variable_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301cc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21420ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gztlexjind'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07920442",
   "metadata": {},
   "outputs": [],
   "source": [
    "INVALID_UUID_FOR_TESTING = \"00000000-0000-0000-0000-000000000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce99fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('8a196b81-174c-4b63-8a2d-f9d330754731'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '29/datablob/113/${filename}', 'AWSAccessKeyId': 'AKIAY7RRHQ4BEOUZVSE3', 'policy': 'eyJleHBpcmF0aW9uIjogIjIwMjItMDktMTRUMTA6MTE6NTlaIiwgImNvbmRpdGlvbnMiOiBbWyJzdGFydHMtd2l0aCIsICIka2V5IiwgIjI5L2RhdGFibG9iLzExMyJdLCB7ImJ1Y2tldCI6ICJrdW1hcmFuLWFpcnQtc2VydmljZS1ldS13ZXN0LTEifSwgWyJzdGFydHMtd2l0aCIsICIka2V5IiwgIjI5L2RhdGFibG9iLzExMy8iXV19', 'signature': '9ZpqX0GgPSNZBfQ08yM47SPw9vw='}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AccountId DefinitionId        OccurredTime  \\\n",
       "__null_dask_index__                                               \n",
       "0                       312571   loadTests2 2019-12-31 21:30:02   \n",
       "1                       312571   loadTests3 2020-01-03 23:53:22   \n",
       "2                       312571   loadTests1 2020-01-07 02:16:42   \n",
       "3                       312571   loadTests2 2020-01-10 04:40:02   \n",
       "4                       312571   loadTests3 2020-01-13 07:03:22   \n",
       "\n",
       "                     OccurredTimeTicks  PersonId  \n",
       "__null_dask_index__                               \n",
       "0                        1577836802678         2  \n",
       "1                        1578104602678         2  \n",
       "2                        1578372402678         2  \n",
       "3                        1578640202678         2  \n",
       "4                        1578908002678         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/file.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccountId,DefinitionId,OccurredTime,OccurredTimeTicks,PersonId\n",
      "312571,loadTests2,2019-12-31 21:30:02,1577836802678,2\n",
      "312571,loadTests3,2020-01-03 23:53:22,1578104602678,2\n",
      "312571,loadTests1,2020-01-07 02:16:42,1578372402678,2\n",
      "312571,loadTests2,2020-01-10 04:40:02,1578640202678,2\n",
      "312571,loadTests3,2020-01-13 07:03:22,1578908002678,2\n",
      "312571,loadTests1,2020-01-16 09:26:42,1579175802678,2\n",
      "312571,loadTests2,2020-01-19 11:50:02,1579443602678,2\n",
      "312571,loadTests3,2020-01-22 14:13:22,1579711402678,2\n",
      "312571,loadTests1,2020-01-25 16:36:42,1579979202678,2\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_mkdhs1gz\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): processing user uploaded csv file for datablob_id=113 and uploading parquet back to S3 for datasource_id=39\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/29/datablob/113\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/29/datablob/113\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/29/datablob/113 locally in /tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/29/datablob/113 to /tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/29/datasource/39\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/29/datasource/39 locally in /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i/file.csv,..., /tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i/file.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:46793' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:43687' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:43995' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/29/datasource/39, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq to s3://kumaran-airt-service-eu-west-1/29/datasource/39\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_x2l9wskq\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-129datablob113_cached_p3mbo82i\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=113, datasource_id=39): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=39, uuid=UUID('b666419d-f67f-45b5-bc35-b7fbd1151d1b'), hash='64ab63985d6651f495ddccd4d96d16cb', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/29/datasource/39', created=datetime.datetime(2022, 9, 13, 10, 12, 17), user_id=29, pulled_on=datetime.datetime(2022, 9, 13, 10, 12, 24), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and pull datasource to use in following tests\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    from_local_request = FromLocalRequest(\n",
    "        path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "    )\n",
    "    from_local_response = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        df = pd.read_parquet(test_s3_path.as_path())\n",
    "        display(df.head())\n",
    "        df.to_csv(test_s3_path.as_path() / \"file.csv\", index=False)\n",
    "        display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "        !head -n 10 {test_s3_path.as_path()/\"file.csv\"}\n",
    "\n",
    "        upload_to_s3_with_retry(\n",
    "            test_s3_path.as_path() / \"file.csv\",\n",
    "            from_local_response.presigned[\"url\"],\n",
    "            from_local_response.presigned[\"fields\"],\n",
    "        )\n",
    "\n",
    "    datablob_id = session.exec(select(DataBlob).where(DataBlob.uuid == from_local_response.uuid)).one().id\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    process_csv(\n",
    "        datablob_id=datablob_id,\n",
    "        datasource_id=datasource.id,\n",
    "        deduplicate_data=True,\n",
    "        index_column=\"PersonId\",\n",
    "        sort_by=\"OccurredTime\",\n",
    "        blocksize=\"256MB\",\n",
    "        kwargs_json=json.dumps(\n",
    "            dict(\n",
    "                usecols=[0, 1, 2, 3, 4],\n",
    "                parse_dates=[\"OccurredTime\"],\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource.id)\n",
    "    ).one()\n",
    "    display(datasource)\n",
    "    datasource_id = datasource.id\n",
    "    datasource_cloud_provider = datasource.cloud_provider\n",
    "    datasource_region = datasource.region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default router for all train routes\n",
    "model_train_router = APIRouter(\n",
    "    prefix=\"/model\",\n",
    "    tags=[\"train\"],\n",
    "    #     dependencies=[Depends(get_current_active_user)],\n",
    "    responses={\n",
    "        404: {\"description\": \"Not found\"},\n",
    "        500: {\n",
    "            \"model\": HTTPError,\n",
    "            \"description\": ERRORS[\"INTERNAL_SERVER_ERROR\"],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5471a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class TrainRequest(BaseModel):\n",
    "    \"\"\"Request object for the /model/train route\n",
    "\n",
    "    Args:\n",
    "        data_uuid: Datasource uuid to train model\n",
    "        client_column: Column in which client ids are present\n",
    "        target_column: Column where target events for training are present\n",
    "        target: Regex string to use as target event for training\n",
    "        predict_after: Time period after to predict(in seconds)\n",
    "    \"\"\"\n",
    "\n",
    "    data_uuid: uuid.UUID\n",
    "    client_column: str\n",
    "    target_column: str\n",
    "    target: str\n",
    "    predict_after: timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.post(\n",
    "    \"/train\",\n",
    "    response_model=ModelRead,\n",
    "    responses={\n",
    "        **get_datasource_responses,  # type: ignore\n",
    "        412: {\"model\": HTTPError, \"description\": ERRORS[\"QUOTA_EXCEEDED\"]},\n",
    "    },\n",
    ")\n",
    "def train_model(\n",
    "    train_request: TrainRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Model:\n",
    "    \"\"\"Start model training from the given datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    if not user.subscription_type.value in [\n",
    "        \"small\",\n",
    "        \"medium\",\n",
    "        \"large\",\n",
    "        \"infobip\",\n",
    "        \"captn\",\n",
    "    ]:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_412_PRECONDITION_FAILED,\n",
    "            detail=ERRORS[\"QUOTA_EXCEEDED\"],\n",
    "        )\n",
    "    datasource = DataSource.get(uuid=train_request.data_uuid, user=user, session=session)  # type: ignore\n",
    "    # send msg to batch job queue to start training and return model_id\n",
    "    model = Model(\n",
    "        client_column=train_request.client_column,\n",
    "        target_column=train_request.target_column,\n",
    "        target=train_request.target,\n",
    "        predict_after=train_request.predict_after,\n",
    "        cloud_provider=datasource.cloud_provider,\n",
    "        region=datasource.region,\n",
    "        total_steps=5,\n",
    "        user=user,\n",
    "        datasource_id=datasource.id,\n",
    "    )\n",
    "    session.add(model)\n",
    "    session.commit()\n",
    "    session.refresh(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d78d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=412, detail='Quota exceeded') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_without_quota = create_user_for_testing(subscription_type=\"test\")\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == user_without_quota)).one()\n",
    "\n",
    "    train_request = TrainRequest(\n",
    "        data_uuid=datasource.uuid,\n",
    "        client_column=\"AccountId\",\n",
    "        target_column=\"DefinitionId\",\n",
    "        target=\"load*\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "    )\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        train_model(train_request=train_request, user=user, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8699859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(target='load*', created=datetime.datetime(2022, 9, 13, 10, 12, 44), predict_after=datetime.timedelta(days=20), id=6, timestamp_column=None, uuid=UUID('0ff152dc-7002-4747-963a-f64486e300d5'), total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, datasource_id=39, client_column='AccountId', error=None, user_id=29, target_column='DefinitionId', region='eu-west-1', disabled=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    train_request = TrainRequest(\n",
    "        data_uuid=datasource.uuid,\n",
    "        client_column=\"AccountId\",\n",
    "        target_column=\"DefinitionId\",\n",
    "        target=\"load*\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "    )\n",
    "\n",
    "    actual = train_model(train_request=train_request, user=user, session=session)\n",
    "\n",
    "# For following tests\n",
    "with get_session_with_context() as session:\n",
    "    model_trained = session.exec(select(Model).where(Model.id == actual.id)).one()\n",
    "    display(model_trained)\n",
    "    model_trained_id = model_trained.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f055037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "get_model_responses = {\n",
    "    400: {\"model\": HTTPError, \"description\": ERRORS[\"INCORRECT_MODEL_ID\"]}\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_uuid: str, user: User, session: Session) -> Model:\n",
    "    \"\"\"Get model object for the model_id\n",
    "\n",
    "    Args:\n",
    "        model_uuid: Model uuid\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "     Returns:\n",
    "        The model object for given model uuid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = session.exec(\n",
    "            select(Model).where(Model.uuid == model_uuid, Model.user == user)\n",
    "        ).one()\n",
    "    except NoResultFound:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"INCORRECT_MODEL_ID\"],\n",
    "        )\n",
    "\n",
    "    if model.disabled:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST, detail=ERRORS[\"MODEL_IS_DELETED\"]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(target='load*', created=datetime.datetime(2022, 9, 13, 10, 12, 44), predict_after=datetime.timedelta(days=20), id=6, timestamp_column=None, uuid=UUID('0ff152dc-7002-4747-963a-f64486e300d5'), total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, datasource_id=39, client_column='AccountId', error=None, user_id=29, target_column='DefinitionId', region='eu-west-1', disabled=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Incorrect model id') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Incorrect model id') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model_trained = session.merge(model_trained)\n",
    "\n",
    "    expected = model_trained\n",
    "    actual = get_model(model_uuid=expected.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual == expected\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_model(model_uuid=INVALID_UUID_FOR_TESTING, user=user, session=session)\n",
    "    display(e)\n",
    "\n",
    "    user_kumaran = session.exec(select(User).where(User.username == \"kumaran\")).one()\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_model(model_uuid=expected.uuid, user=user_kumaran, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e665c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Model is deleted') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    datasource = user.datasources[0]\n",
    "    model_disabled = Model(\n",
    "        client_column=\"client_column\",\n",
    "        target_column=\"target_column\",\n",
    "        target=\"target\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "        cloud_provider=datasource.cloud_provider,\n",
    "        region=datasource.region,\n",
    "        total_steps=5,\n",
    "        user=user,\n",
    "        datasource_id=datasource.id,\n",
    "        disabled=True,\n",
    "    )\n",
    "    session.add(model_disabled)\n",
    "    session.commit()\n",
    "    session.refresh(model_disabled)\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_model(model_uuid=model_disabled.uuid, user=user, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed95d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.get(\n",
    "    \"/{model_uuid}\",\n",
    "    response_model=ModelRead,\n",
    "    responses={\n",
    "        **get_model_responses,  # type: ignore\n",
    "        422: {\"model\": HTTPError, \"description\": \"Model error\"},\n",
    "    },\n",
    ")\n",
    "def get_details_of_model(\n",
    "    model_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Model:\n",
    "    \"\"\"Get details of the model\"\"\"\n",
    "    user = session.merge(user)\n",
    "    # get details from the internal db for model_id\n",
    "    model = get_model(model_uuid=model_uuid, user=user, session=session)\n",
    "\n",
    "    if model.error is not None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=model.error\n",
    "        )\n",
    "\n",
    "    # ToDo: Remove following temporary fix once actual train is implemented\n",
    "    model.completed_steps = model.total_steps\n",
    "    session.add(model)\n",
    "    session.commit()\n",
    "    session.refresh(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a1631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(target='load*', created=datetime.datetime(2022, 9, 13, 10, 12, 44), predict_after=datetime.timedelta(days=20), id=6, timestamp_column=None, uuid=UUID('0ff152dc-7002-4747-963a-f64486e300d5'), total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=5, datasource_id=39, client_column='AccountId', error=None, user_id=29, target_column='DefinitionId', region='eu-west-1', disabled=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model_trained = session.merge(model_trained)\n",
    "\n",
    "    expected = model_trained\n",
    "    actual = get_details_of_model(model_uuid=expected.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04c07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=422, detail='test error') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model_errored = Model(\n",
    "        client_column=\"wrong_client_column\",\n",
    "        target_column=\"wrong_target_column\",\n",
    "        target=\"wrong_target\",\n",
    "        predict_after=timedelta(100),\n",
    "        cloud_provider=datasource_cloud_provider,\n",
    "        region=datasource_region,\n",
    "        total_steps=5,\n",
    "        user=user,\n",
    "        datasource_id=datasource_id,\n",
    "        error=\"test error\",\n",
    "    )\n",
    "    session.add(model_errored)\n",
    "    session.commit()\n",
    "    session.refresh(model_errored)\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_details_of_model(model_uuid=model_errored.uuid, user=user, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b961b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.delete(\n",
    "    \"/{model_uuid}\", response_model=ModelRead, responses=get_model_responses  # type: ignore\n",
    ")\n",
    "def delete_model(\n",
    "    model_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Model:\n",
    "    \"\"\"Delete model\"\"\"\n",
    "    user = session.merge(user)\n",
    "    # get details from the internal db for model_id\n",
    "    model = get_model(model_uuid=model_uuid, user=user, session=session)\n",
    "\n",
    "    if model.path is not None:\n",
    "        shutil.rmtree(model.path)\n",
    "    model.disabled = True\n",
    "\n",
    "    session.add(model)\n",
    "    session.commit()\n",
    "    session.refresh(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401014f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(target='target', created=datetime.datetime(2022, 9, 13, 10, 12, 44), predict_after=datetime.timedelta(days=100), id=9, timestamp_column=None, uuid=UUID('cc026c46-96f1-497b-b5f4-5ab70f864ac4'), total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, datasource_id=39, client_column='client_column', error=None, user_id=29, target_column='target_column', region='eu-west-1', disabled=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model = Model(\n",
    "        client_column=\"client_column\",\n",
    "        target_column=\"target_column\",\n",
    "        target=\"target\",\n",
    "        predict_after=timedelta(100),\n",
    "        cloud_provider=datasource_cloud_provider,\n",
    "        region=datasource_region,\n",
    "        total_steps=5,\n",
    "        user=user,\n",
    "        datasource_id=datasource_id,\n",
    "    )\n",
    "    session.add(model)\n",
    "    session.commit()\n",
    "    session.refresh(model)\n",
    "\n",
    "    actual = delete_model(model_uuid=model.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual.disabled == True\n",
    "    # assert not Path(actual.path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140549b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.get(\"/{model_uuid}/evaluate\", responses=get_model_responses)  # type: ignore\n",
    "def evaluate_model(\n",
    "    model_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Get accuracy, recall, precision of the trained model\"\"\"\n",
    "    user = session.merge(user)\n",
    "    # get evaluation for the trained model\n",
    "    model = get_model(model_uuid=model_uuid, user=user, session=session)\n",
    "    return {\"accuracy\": 0.985, \"recall\": 0.962, \"precision\": 0.934}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1a2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.985, 'recall': 0.962, 'precision': 0.934}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model_trained = session.merge(model_trained)\n",
    "\n",
    "    actual = evaluate_model(model_uuid=model_trained.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert isinstance(actual, dict)\n",
    "    assert \"accuracy\" in actual\n",
    "    assert \"recall\" in actual\n",
    "    assert \"precision\" in actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.get(\"/\", response_model=List[ModelRead])\n",
    "def get_all_model(\n",
    "    disabled: bool = False,\n",
    "    completed: bool = False,\n",
    "    offset: int = 0,\n",
    "    limit: int = Query(default=100, lte=100),\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> List[Model]:\n",
    "    \"\"\"Get all models created by user\"\"\"\n",
    "    user = session.merge(user)\n",
    "    statement = select(Model).where(Model.user == user)\n",
    "    statement = statement.where(Model.disabled == disabled)\n",
    "    if completed:\n",
    "        statement = statement.where(Model.completed_steps == Model.total_steps)\n",
    "    # get all models from db\n",
    "    models = session.exec(statement.offset(offset).limit(limit)).all()\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0ee5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(target='load*', created=datetime.datetime(2022, 9, 13, 10, 12, 44), predict_after=datetime.timedelta(days=20), id=6, timestamp_column=None, uuid=UUID('0ff152dc-7002-4747-963a-f64486e300d5'), total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=5, datasource_id=39, client_column='AccountId', error=None, user_id=29, target_column='DefinitionId', region='eu-west-1', disabled=False)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    actual = get_all_model(\n",
    "        disabled=False, completed=False, offset=0, limit=1, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "\n",
    "    assert len(actual) == 1\n",
    "    assert isinstance(actual[0], Model)\n",
    "    # assert actual[0] == user.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105b165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(actual)=2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = get_all_model(\n",
    "    disabled=False, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for model in actual:\n",
    "    assert not model.disabled\n",
    "\n",
    "actual = get_all_model(\n",
    "    disabled=True, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for model in actual:\n",
    "    assert model.disabled\n",
    "\n",
    "actual = get_all_model(\n",
    "    disabled=False, completed=True, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for model in actual:\n",
    "    assert model.completed_steps == model.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce944e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@model_train_router.post(\n",
    "    \"/{model_uuid}/predict\",\n",
    "    response_model=PredictionRead,\n",
    "    responses={\n",
    "        **get_model_responses,  # type: ignore\n",
    "        412: {\"model\": HTTPError, \"description\": ERRORS[\"QUOTA_EXCEEDED\"]},\n",
    "    },\n",
    ")\n",
    "def predict_model(\n",
    "    *,\n",
    "    model_uuid: str,\n",
    "    datasource_select: Optional[DataSourceSelect] = None,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> Prediction:\n",
    "    \"\"\"Start prediction using trained model and for the given datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    if not user.subscription_type.value in [\n",
    "        \"small\",\n",
    "        \"medium\",\n",
    "        \"large\",\n",
    "        \"infobip\",\n",
    "        \"captn\",\n",
    "    ]:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_412_PRECONDITION_FAILED,\n",
    "            detail=ERRORS[\"QUOTA_EXCEEDED\"],\n",
    "        )\n",
    "    model = get_model(model_uuid=model_uuid, user=user, session=session)\n",
    "    data_uuid = (\n",
    "        model.datasource.uuid\n",
    "        if datasource_select is None\n",
    "        else datasource_select.data_uuid\n",
    "    )\n",
    "    datasource = DataSource.get(uuid=data_uuid, user=user, session=session)  # type: ignore\n",
    "    # start prediction for the trained model and return prediction_id\n",
    "    prediction = Prediction(\n",
    "        total_steps=3,\n",
    "        user=user,\n",
    "        model=model,\n",
    "        datasource_id=datasource.id,\n",
    "        cloud_provider=datasource.cloud_provider,\n",
    "        region=datasource.region,\n",
    "    )\n",
    "    session.add(prediction)\n",
    "    session.commit()\n",
    "\n",
    "    command = f\"predict {prediction.id}\"\n",
    "\n",
    "    create_batch_job(\n",
    "        command=command,\n",
    "        task=\"csv_processing\",\n",
    "        cloud_provider=prediction.cloud_provider,\n",
    "        region=prediction.region,\n",
    "        background_tasks=background_tasks,\n",
    "    )\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a11a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 7', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 7', environment_vars={'AWS_ACCESS_KEY_ID': 'AKIAY7RRHQ4BEOUZVSE3', 'AWS_SECRET_ACCESS_KEY': '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', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '17a59428-c3d7-4cd7-94fe-b99d97d5f0ef', 'AZURE_TENANT_ID': '2d76de3f-27df-469a-8f99-addacb9239b8', 'AZURE_CLIENT_ID': '15281946-e57c-43ca-886e-d4611cfe9fe4', 'AZURE_CLIENT_SECRET': 'POf8Q~1-sM-u2JThF2xbZflR2L5ifm-FGKoKRc-H', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'AIRT_SERVICE_SUPER_USER_PASSWORD': 'što posiješ, to i požanješ', 'AIRT_TOKEN_SECRET_KEY': 'adsflkjoemnvaaserniuhenbcvoiebnfase', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': 'SuperSecretPassword', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(id=7, region='eu-west-1', completed_steps=0, cloud_provider=<CloudProvider.aws: 'aws'>, model_id=6, path=None, disabled=False, total_steps=3, error=None, datasource_id=39, created=datetime.datetime(2022, 9, 13, 10, 12, 45), uuid=UUID('f843f183-3d9d-4aa1-9584-607642983328'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7f7f90974550>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"bg_task.kwargs={'command': 'predict 7'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=412, detail='Quota exceeded') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    model_trained = session.exec(select(Model).where(Model.id == model_trained.id)).one()\n",
    "    # model_trained = session.merge(model_trained)\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        actual = predict_model(\n",
    "            model_uuid=model_trained.uuid, user=user, session=session, background_tasks=b\n",
    "        )\n",
    "    display(actual)\n",
    "    assert isinstance(actual, Prediction)\n",
    "    bg_task = b.tasks[-1]\n",
    "    display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "    assert bg_task.func == execute_cli\n",
    "    assert bg_task.kwargs[\"command\"] == f\"predict {actual.id}\"\n",
    "\n",
    "    user.subscription_type = SubscriptionType.test\n",
    "    session.add(user)\n",
    "    session.commit()\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            predict_model(\n",
    "                model_uuid=model_trained.uuid, user=user, session=session, background_tasks=b\n",
    "            )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db59eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def predict(prediction_id: Param(\"id of prediction in db\", int)):  # type: ignore\n",
    "    \"\"\"Copy datasource parquet to prediction path to create dummy prediction output\n",
    "\n",
    "    Args:\n",
    "        prediction_id: Id of prediction in db\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```predict 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        prediction = session.exec(\n",
    "            select(Prediction).where(Prediction.id == prediction_id)\n",
    "        ).one()\n",
    "        prediction.path = None\n",
    "\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.id == prediction.model.datasource_id)\n",
    "        ).one()\n",
    "\n",
    "        try:\n",
    "            if datasource.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_prediction_path(\n",
    "                    user_id=prediction.model.user.id,\n",
    "                    prediction_id=prediction.id,\n",
    "                    region=prediction.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datasource.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_prediction_path(\n",
    "                    user_id=prediction.model.user.id,\n",
    "                    prediction_id=prediction.id,\n",
    "                    region=prediction.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=destination_remote_url,\n",
    "                pull_on_enter=False,\n",
    "                push_on_exit=True,\n",
    "                exist_ok=True,\n",
    "                parents=True,\n",
    "            ) as destination_path:\n",
    "                sync_path = destination_path.as_path()\n",
    "                source_remote_url = datasource.path\n",
    "                with RemotePath.from_url(\n",
    "                    remote_url=source_remote_url,\n",
    "                    pull_on_enter=True,\n",
    "                    push_on_exit=False,\n",
    "                    exist_ok=True,\n",
    "                    parents=False,\n",
    "                ) as source_remote_path:\n",
    "                    source_files = source_remote_path.as_path().iterdir()\n",
    "                    source_files = [\n",
    "                        f for f in source_files if METADATA_FOLDER_PATH not in str(f)\n",
    "                    ]\n",
    "                    for f in source_files:\n",
    "                        shutil.move(str(f), sync_path)\n",
    "\n",
    "            prediction.path = destination_remote_url  # type: ignore\n",
    "            prediction.completed_steps = prediction.total_steps\n",
    "        except Exception as e:\n",
    "            prediction.error = truncate(str(e))\n",
    "        session.add(prediction)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 8', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 8', environment_vars={'AWS_ACCESS_KEY_ID': 'AKIAY7RRHQ4BEOUZVSE3', 'AWS_SECRET_ACCESS_KEY': '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', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '17a59428-c3d7-4cd7-94fe-b99d97d5f0ef', 'AZURE_TENANT_ID': '2d76de3f-27df-469a-8f99-addacb9239b8', 'AZURE_CLIENT_ID': '15281946-e57c-43ca-886e-d4611cfe9fe4', 'AZURE_CLIENT_SECRET': 'POf8Q~1-sM-u2JThF2xbZflR2L5ifm-FGKoKRc-H', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'AIRT_SERVICE_SUPER_USER_PASSWORD': 'što posiješ, to i požanješ', 'AIRT_TOKEN_SECRET_KEY': 'adsflkjoemnvaaserniuhenbcvoiebnfase', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': 'SuperSecretPassword', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(id=8, region='eu-west-1', completed_steps=0, cloud_provider=<CloudProvider.aws: 'aws'>, model_id=6, path=None, disabled=False, total_steps=3, error=None, datasource_id=39, created=datetime.datetime(2022, 9, 13, 10, 12, 45), uuid=UUID('65fbb0ac-6b23-4172-9caf-9efb5d618e42'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/29/prediction/8\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_s3uxalzn\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/29/prediction/8 locally in /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_s3uxalzn\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/29/datasource/39\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_gmpxhb0a\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/29/datasource/39 locally in /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_gmpxhb0a\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/29/datasource/39 to /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_gmpxhb0a\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-129datasource39_cached_gmpxhb0a\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_s3uxalzn to s3://kumaran-airt-service-eu-west-1/29/prediction/8\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_s3uxalzn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=3, region='eu-west-1', disabled=False, id=8, path='s3://kumaran-airt-service-eu-west-1/29/prediction/8', model_id=6, total_steps=3, error=None, created=datetime.datetime(2022, 9, 13, 10, 12, 45), uuid=UUID('65fbb0ac-6b23-4172-9caf-9efb5d618e42'), datasource_id=39)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/29/prediction/8\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_gzr_goej\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/29/prediction/8 locally in /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_gzr_goej\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/29/prediction/8 to /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_gzr_goej\n",
      "OK\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-129prediction8_cached_gzr_goej\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    user.subscription_type = SubscriptionType.small\n",
    "    session.add(user)\n",
    "    session.commit()\n",
    "    model_trained = session.merge(model_trained)\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        prediction = predict_model(\n",
    "            model_uuid=model_trained.uuid,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(prediction)\n",
    "\n",
    "    predict(prediction_id=prediction.id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    prediction = session.exec(\n",
    "        select(Prediction).where(Prediction.id == prediction.id)\n",
    "    ).one()\n",
    "    display(prediction)\n",
    "    assert prediction.error is None\n",
    "    assert prediction.path\n",
    "    assert prediction.completed_steps == prediction.total_steps\n",
    "\n",
    "    # Validating the contents of the destination bucket\n",
    "    destination_bucket, destination_s3_path = create_s3_prediction_path(\n",
    "        user_id=prediction.model.user.id,\n",
    "        prediction_id=prediction.id,\n",
    "        region=prediction.region,\n",
    "    )\n",
    "\n",
    "    destination_bucket, destination_s3_path\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://{destination_bucket.name}/{destination_s3_path}\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as cache_path:\n",
    "        files = [str(p) for p in cache_path.as_path().rglob(\"*.*\")]\n",
    "        assert METADATA_FOLDER_PATH not in files\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63dc371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
