{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Context classes to run jobs in AWS Batch\n",
    "output-file: aws_batch_job_context.html\n",
    "title: AWS Batch Job Context\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp batch_job_components.aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "\n",
    "from airt.logger import get_logger\n",
    "\n",
    "import airt_service\n",
    "import airt_service.sanitizer\n",
    "from airt_service.aws.batch_utils import aws_batch_create_job\n",
    "from airt_service.aws.utils import get_queue_definition_arns\n",
    "from airt_service.batch_job_components.base import BatchJobContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _pytest.monkeypatch import MonkeyPatch\n",
    "from fastcore.utils import patch\n",
    "\n",
    "from airt_service.helpers import set_env_variable_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_setattr = MonkeyPatch.setattr\n",
    "\n",
    "\n",
    "@patch\n",
    "def setattr(self: MonkeyPatch, *args, **kwargs):\n",
    "    global logger\n",
    "    old_setattr(self, *args, **kwargs)\n",
    "    logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AwsBatchJobContext(BatchJobContext):\n",
    "    \"\"\"A class for creating AwsBatchJobContext\"\"\"\n",
    "\n",
    "    def __init__(self, task: str, **kwargs: Any):\n",
    "        \"\"\"AWS Batch Job Context\n",
    "\n",
    "        Do not use __init__, please use factory method `create` to initiate object\n",
    "        \"\"\"\n",
    "        BatchJobContext.__init__(self, task=task)\n",
    "        self.region = kwargs[\"region\"]\n",
    "\n",
    "    def create_job(self, command: str, environment_vars: Dict[str, str]) -> None:\n",
    "        \"\"\"Create a new job\n",
    "\n",
    "        Args:\n",
    "            command: Command to execute in job\n",
    "            environment_vars: Environment vars to set in the container\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"{self.__class__.__name__}.create_job({self=}, {command=}, {environment_vars=})\"\n",
    "        )\n",
    "        (\n",
    "            job_queue_arn,\n",
    "            job_definition_arn,\n",
    "        ) = airt_service.aws.utils.get_queue_definition_arns(self.task, self.region)\n",
    "\n",
    "        airt_service.aws.batch_utils.aws_batch_create_job(\n",
    "            job_queue_arn=job_queue_arn,\n",
    "            job_definition_arn=job_definition_arn,\n",
    "            region=self.region,\n",
    "            command=command,\n",
    "            environment_vars=environment_vars,\n",
    "        )\n",
    "\n",
    "\n",
    "AwsBatchJobContext.add_factory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job_components.base: Entering AwsBatchJobContext(task=csv_processing)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AwsBatchJobContext(task=csv_processing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: AwsBatchJobContext.create_job(self=AwsBatchJobContext(task=csv_processing), command='process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************'})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"kwargs={'job_queue_arn': 'aws:job_queue_arn', 'job_definition_arn': 'aws:job_definition_arn', 'region': 'eu-west-1', 'command': 'process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data', 'environment_vars': {'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************'}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job_components.base: Exiting AwsBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    }
   ],
   "source": [
    "with MonkeyPatch.context() as monkeypatch:\n",
    "    job_queue_arn = \"aws:job_queue_arn\"\n",
    "    job_definition_arn = \"aws:job_definition_arn\"\n",
    "    region = \"eu-west-1\"\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.aws.utils.get_queue_definition_arns\",\n",
    "        lambda task, region: (job_queue_arn, job_definition_arn),\n",
    "    )\n",
    "\n",
    "    def test_patch_create_job(*args, **kwargs):\n",
    "        display(f\"{kwargs=}\")\n",
    "        assert kwargs[\"job_queue_arn\"] == job_queue_arn\n",
    "        assert kwargs[\"job_definition_arn\"] == job_definition_arn\n",
    "        assert kwargs[\"region\"] == region\n",
    "        assert (\n",
    "            kwargs[\"command\"]\n",
    "            == f\"process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data\"\n",
    "        )\n",
    "        assert \"AWS_ACCESS_KEY_ID\" in kwargs[\"environment_vars\"]\n",
    "        assert \"AWS_SECRET_ACCESS_KEY\" in kwargs[\"environment_vars\"]\n",
    "\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.aws.batch_utils.aws_batch_create_job\", test_patch_create_job\n",
    "    )\n",
    "\n",
    "    with BatchJobContext.create(\"csv_processing\", region=region) as batch_ctx:\n",
    "        display(batch_ctx)\n",
    "        assert batch_ctx.__class__.__name__ == \"AwsBatchJobContext\"\n",
    "        batch_ctx.create_job(\n",
    "            command=\"process_csv_for 3 PersonId OccurredTime --blocksize 256MB --deduplicate_data\",\n",
    "            environment_vars={\n",
    "                \"AWS_ACCESS_KEY_ID\": \"random_value\",\n",
    "                \"AWS_SECRET_ACCESS_KEY\": \"random_value\",\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
