{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to interact with db datablob\n",
    "output-file: datablob_db.html\n",
    "title: DataBlob DB\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from fastcore.script import call_parse, Param\n",
    "from fastcore.utils import *\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt_service.aws.utils import create_s3_datablob_path\n",
    "from airt_service.azure.utils import create_azure_blob_storage_datablob_path\n",
    "from airt_service.data.utils import (\n",
    "    calculate_data_object_folder_size_and_path,\n",
    "    calculate_data_object_pulled_on,\n",
    "    get_db_connection_params_from_db_uri,\n",
    ")\n",
    "from airt_service.db.models import (\n",
    "    create_connection_string,\n",
    "    get_session_with_context,\n",
    "    DataBlob,\n",
    "    PredictionPush,\n",
    ")\n",
    "from airt_service.helpers import truncate\n",
    "from airt.engine.engine import using_cluster\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from fastapi import BackgroundTasks\n",
    "\n",
    "from airt_service.aws.utils import create_s3_prediction_path\n",
    "from airt_service.data.s3 import copy_between_s3\n",
    "from airt_service.data.utils import create_db_uri_for_db_datablob\n",
    "from airt_service.db.models import (\n",
    "    DataSource,\n",
    "    get_db_params_from_env_vars,\n",
    "    get_engine,\n",
    "    get_session,\n",
    "    create_user_for_testing,\n",
    "    User,\n",
    ")\n",
    "from airt_service.model.train import TrainRequest, train_model, predict_model\n",
    "from airt_service.helpers import (\n",
    "    commit_or_rollback,\n",
    "    set_env_variable_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ypesaudaiy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"conn_str='mysql://****************************************@kumaran-mysql:3306/airt_service'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'creating db'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_params = get_db_params_from_env_vars()\n",
    "conn_str = create_connection_string(**db_params)\n",
    "display(f\"{conn_str=}\")\n",
    "display(\"creating db\")\n",
    "engine = sa.create_engine(conn_str)\n",
    "conn = engine.connect()\n",
    "conn.execute(\"commit\")\n",
    "try:\n",
    "    conn.execute(\"create database test\")\n",
    "except sa.exc.ProgrammingError as e:\n",
    "    display(e)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_529etyn8\n"
     ]
    }
   ],
   "source": [
    "with RemotePath.from_url(\n",
    "    remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "    pull_on_enter=True,\n",
    "    push_on_exit=False,\n",
    "    exist_ok=True,\n",
    "    parents=False,\n",
    "    access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ") as test_s3_path:\n",
    "    display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "\n",
    "    db_params = get_db_params_from_env_vars()\n",
    "    db_params[\"database\"] = \"test\"\n",
    "    engine = get_engine(**db_params)\n",
    "\n",
    "    df = pd.read_parquet(test_s3_path.as_path())\n",
    "    try:\n",
    "        df.to_sql(\"test_db_pull\", con=engine, if_exists=\"fail\")\n",
    "    except ValueError as e:\n",
    "        display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def download_from_db(\n",
    "    *,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    database: str,\n",
    "    database_server: str,\n",
    "    table: str,\n",
    "    chunksize: Optional[int] = 1_000_000,\n",
    "    output_path: Path,\n",
    "):\n",
    "    \"\"\"Download data from database and stores it as parquet files in output path\n",
    "\n",
    "    Args:\n",
    "        host: Host of db\n",
    "        port: Port of db\n",
    "        username: Username of db\n",
    "        password: Password of db\n",
    "        database: Database to use in db\n",
    "        database_server: Server/engine of db\n",
    "        table: Table to use in db\n",
    "        chunksize: Chunksize to download as\n",
    "        output_path: Path to store parquet files\n",
    "    \"\"\"\n",
    "    conn_str = create_connection_string(\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        database_server=database_server,\n",
    "    )\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        d = Path(td)\n",
    "        for i, df in enumerate(\n",
    "            pd.read_sql_table(table_name=table, con=conn_str, chunksize=chunksize)\n",
    "        ):\n",
    "            fname = d / f\"{database_server}_{database}_{table}_data_{i:09d}.parquet\"\n",
    "            logger.info(\n",
    "                f\"Writing data retrieved from the database to temporary file {fname}\"\n",
    "            )\n",
    "            df.to_parquet(fname)\n",
    "        logger.info(\n",
    "            f\"Rewriting temporary parquet files from {d} to output directory {output_path}\"\n",
    "        )\n",
    "        ddf = dd.read_parquet(\n",
    "            d,\n",
    "            blocksize=None,\n",
    "        )\n",
    "        ddf.to_parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'username': 'root',\n",
       " 'password': '****************************************',\n",
       " 'host': 'kumaran-mysql',\n",
       " 'port': 3306,\n",
       " 'database': 'test',\n",
       " 'database_server': 'mysql'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmpbnfhn906/mysql_test_test_db_pull_data_000000000.parquet\n",
      "[INFO] __main__: Rewriting temporary parquet files from /tmp/tmpbnfhn906 to output directory /tmp/test_s3_download_7qtdt68u\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/test_s3_download_7qtdt68u/part.0.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccountId DefinitionId        OccurredTime  OccurredTimeTicks  PersonId\n",
       "0     312571   loadTests2 2019-12-31 21:30:02      1577836802678         2\n",
       "1     312571   loadTests3 2020-01-03 23:53:22      1578104602678         2\n",
       "2     312571   loadTests1 2020-01-07 02:16:42      1578372402678         2\n",
       "3     312571   loadTests2 2020-01-10 04:40:02      1578640202678         2\n",
       "4     312571   loadTests3 2020-01-13 07:03:22      1578908002678         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(db_params)\n",
    "with tempfile.TemporaryDirectory(prefix=\"test_s3_download_\") as d:\n",
    "    d = Path(d)\n",
    "    db_params = get_db_params_from_env_vars()\n",
    "    db_params[\"database\"] = \"test\"\n",
    "    download_from_db(\n",
    "        **db_params,\n",
    "        table=\"test_db_pull\",\n",
    "        output_path=d,\n",
    "    )\n",
    "    len(d.ls())\n",
    "    display(list(d.glob(\"*\")))\n",
    "    ddf = dd.read_parquet(d)\n",
    "    display(ddf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def db_pull(datablob_id: Param(\"id of datablob in db\", int)):  # type: ignore\n",
    "    \"\"\"Pull the datablob and update its progress in internal db\n",
    "\n",
    "    Args:\n",
    "        datablob_id: Id of datablob in db\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```db_pull 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.id == datablob_id)\n",
    "        ).one()\n",
    "\n",
    "        datablob.error = None\n",
    "        datablob.completed_steps = 0\n",
    "        datablob.folder_size = None\n",
    "        datablob.path = None\n",
    "\n",
    "        (\n",
    "            username,\n",
    "            password,\n",
    "            host,\n",
    "            port,\n",
    "            table,\n",
    "            database,\n",
    "            database_server,\n",
    "        ) = get_db_connection_params_from_db_uri(datablob.uri)\n",
    "\n",
    "        try:\n",
    "            if datablob.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datablob.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=destination_remote_url,\n",
    "                pull_on_enter=False,\n",
    "                push_on_exit=True,\n",
    "                exist_ok=True,\n",
    "                parents=True,\n",
    "            ) as destionation_s3_path:\n",
    "                sync_path = destionation_s3_path.as_path()\n",
    "                download_from_db(\n",
    "                    host=host,\n",
    "                    port=port,\n",
    "                    username=username,\n",
    "                    password=password,\n",
    "                    database=database,\n",
    "                    database_server=database_server,\n",
    "                    table=table,\n",
    "                    output_path=sync_path,\n",
    "                )\n",
    "                calculate_data_object_pulled_on(datablob)\n",
    "\n",
    "                if len(list(sync_path.glob(\"*\"))) == 0:\n",
    "                    raise ValueError(f\"no files to download, table is empty\")\n",
    "\n",
    "            # Calculate folder size in S3\n",
    "            calculate_data_object_folder_size_and_path(datablob)\n",
    "        except Exception as e:\n",
    "            datablob.error = truncate(str(e))\n",
    "        session.add(datablob)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/85/datablob/15\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-185datablob15_cached_l3vazhmh\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/85/datablob/15 locally in /tmp/s3kumaran-airt-service-eu-west-185datablob15_cached_l3vazhmh\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp8kc0t6vx/mysql_test_test_db_pull_data_000000000.parquet\n",
      "[INFO] __main__: Rewriting temporary parquet files from /tmp/tmp8kc0t6vx to output directory /tmp/s3kumaran-airt-service-eu-west-185datablob15_cached_l3vazhmh\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-185datablob15_cached_l3vazhmh to s3://kumaran-airt-service-eu-west-1/85/datablob/15\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-185datablob15_cached_l3vazhmh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=15, uuid=UUID('95eb6302-b049-4fd7-b47f-b0a27d2f53dd'), type='db', uri='mysql://****************************************@kumaran-mysql:3306/test/test_db_pull', source='mysql://kumaran-mysql:3306/test/test_db_pull', total_steps=1, completed_steps=1, folder_size=8896699, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/85/datablob/15', created=datetime.datetime(2022, 10, 20, 6, 42, 30), user_id=85, pulled_on=datetime.datetime(2022, 10, 20, 6, 42, 33), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    db_params = get_db_params_from_env_vars()\n",
    "    db_params[\"database\"] = \"test\"\n",
    "    db_params[\"table\"] = \"test_db_pull\"\n",
    "\n",
    "    source = f\"{db_params['database_server']}://{db_params['host']}:{db_params['port']}/{db_params['database']}/{db_params['table']}\"\n",
    "\n",
    "    datablob = DataBlob(\n",
    "        type=\"db\",\n",
    "        uri=create_db_uri_for_db_datablob(**db_params),\n",
    "        source=source,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path\n",
    "\n",
    "    db_pull(datablob_id=datablob.id)\n",
    "    \n",
    "    user_id = user.id\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob.id)).one()\n",
    "    display(datablob)\n",
    "    assert datablob.folder_size == 8896699, datablob.folder_size\n",
    "    assert (\n",
    "        datablob.path\n",
    "        == f\"s3://{os.environ['STORAGE_BUCKET_PREFIX']}-eu-west-1/{user_id}/datablob/{datablob.id}\"\n",
    "    ), datablob.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def db_push(prediction_push_id: int):  # type: ignore\n",
    "    \"\"\"Push prediction data to a rdbms\n",
    "\n",
    "    Params:\n",
    "        prediction_push_id: Id of prediction_push\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```db_push 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        prediction_push = session.exec(\n",
    "            select(PredictionPush).where(PredictionPush.id == prediction_push_id)\n",
    "        ).one()\n",
    "\n",
    "        prediction_push.error = None\n",
    "        prediction_push.completed_steps = 0\n",
    "\n",
    "        (\n",
    "            username,\n",
    "            password,\n",
    "            host,\n",
    "            port,\n",
    "            table,\n",
    "            database,\n",
    "            database_server,\n",
    "        ) = get_db_connection_params_from_db_uri(db_uri=prediction_push.uri)\n",
    "\n",
    "        try:\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=prediction_push.prediction.path,\n",
    "                pull_on_enter=True,\n",
    "                push_on_exit=False,\n",
    "                exist_ok=True,\n",
    "                parents=False,\n",
    "            ) as s3_path:\n",
    "                with using_cluster(\"cpu\") as engine:\n",
    "                    ddf = engine.dd.read_parquet(s3_path.as_path())\n",
    "                    conn_str = create_connection_string(\n",
    "                        username=username,\n",
    "                        password=password,\n",
    "                        host=host,\n",
    "                        port=port,\n",
    "                        database=database,\n",
    "                        database_server=database_server,\n",
    "                    )\n",
    "                    ddf.to_sql(\n",
    "                        name=table,\n",
    "                        uri=conn_str,\n",
    "                        if_exists=\"append\",\n",
    "                        index=True,\n",
    "                        method=\"multi\",\n",
    "                    )\n",
    "            prediction_push.completed_steps = 1\n",
    "        except Exception as e:\n",
    "            prediction_push.error = truncate(str(e))\n",
    "\n",
    "        session.add(prediction_push)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 12', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 12', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(disabled=False, total_steps=3, uuid=UUID('71dde9ea-4478-4233-80c8-fba141ea4962'), error=None, datasource_id=8, id=12, completed_steps=0, created=datetime.datetime(2022, 10, 20, 6, 43, 40), model_id=11, cloud_provider=<CloudProvider.aws: 'aws'>, path=None, region='eu-west-1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/85/prediction/12\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_cyxvgmqn\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/85/prediction/12 locally in /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_cyxvgmqn\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_7wq3qdst\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_7wq3qdst\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_7wq3qdst\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_7wq3qdst\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_cyxvgmqn to s3://kumaran-airt-service-eu-west-1/85/prediction/12\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_cyxvgmqn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=7, uuid=UUID('2272899b-00b9-464f-8d47-d8efd05fc573'), uri='mysql://****************************************@kumaran-mysql:3306/airt_service/test_db_push', total_steps=1, completed_steps=0, error=None, created=datetime.datetime(2022, 10, 20, 6, 43, 55), prediction_id=12, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        datasource = DataSource(\n",
    "            datablob_id=datablob.id,\n",
    "            cloud_provider=datablob.cloud_provider,\n",
    "            region=datablob.region,\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "\n",
    "    train_request = TrainRequest(\n",
    "        data_uuid=datasource.uuid,\n",
    "        client_column=\"AccountId\",\n",
    "        target_column=\"DefinitionId\",\n",
    "        target=\"load*\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "    )\n",
    "\n",
    "    model = train_model(train_request=train_request, user=user, session=session)\n",
    "    b = BackgroundTasks()\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        prediction = predict_model(\n",
    "            model_uuid=model.uuid, user=user, session=session, background_tasks=b\n",
    "        )\n",
    "    display(prediction)\n",
    "\n",
    "    bucket, s3_path = create_s3_prediction_path(\n",
    "        user_id=user.id, prediction_id=prediction.id, region=prediction.region\n",
    "    )\n",
    "    copy_between_s3(\n",
    "        source_remote_url=r\"s3://test-airt-service/account_312571_events\",\n",
    "        destination_remote_url=f\"s3://{bucket.name}/{s3_path}\",\n",
    "    )\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        prediction.path = f\"s3://{bucket.name}/{s3_path}\"\n",
    "        session.add(prediction)\n",
    "\n",
    "    prediction_push = PredictionPush(\n",
    "        total_steps=1,\n",
    "        prediction_id=prediction.id,\n",
    "        uri=create_db_uri_for_db_datablob(\n",
    "            table=\"test_db_push\", **get_db_params_from_env_vars()\n",
    "        ),\n",
    "    )\n",
    "    session.add(prediction_push)\n",
    "    session.commit()\n",
    "\n",
    "    display(prediction_push)\n",
    "    assert prediction_push.completed_steps == 0\n",
    "    prediction_push_id = prediction_push.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/85/prediction/12\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_r3z8jdx_\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/85/prediction/12 locally in /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_r3z8jdx_\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/85/prediction/12 to /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_r3z8jdx_\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:38633' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-185prediction12_cached_r3z8jdx_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=7, uuid=UUID('2272899b-00b9-464f-8d47-d8efd05fc573'), uri='mysql://****************************************@kumaran-mysql:3306/airt_service/test_db_push', total_steps=1, completed_steps=1, error=None, created=datetime.datetime(2022, 10, 20, 6, 43, 55), prediction_id=12, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmptaivir50/mysql_airt_service_test_db_push_data_000000000.parquet\n",
      "[INFO] __main__: Rewriting temporary parquet files from /tmp/tmptaivir50 to output directory /tmp/tmpqkavtj0f\n",
      "part.0.parquet\n"
     ]
    }
   ],
   "source": [
    "db_push(prediction_push_id=prediction_push_id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    prediction_push = session.exec(\n",
    "        select(PredictionPush).where(PredictionPush.id == prediction_push_id)\n",
    "    ).one()\n",
    "    display(prediction_push)\n",
    "    assert prediction_push.completed_steps == prediction_push.total_steps\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as td:\n",
    "        download_from_db(\n",
    "            table=\"test_db_push\", output_path=Path(td), **get_db_params_from_env_vars()\n",
    "        )\n",
    "        assert any(Path(td).iterdir())\n",
    "        !ls {td}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
