{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Router to combine all datasource functionalities\n",
    "output-file: datasource_router.html\n",
    "title: DataSource Router\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ff042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "from airt.remote_path import RemotePath\n",
    "from checksumdir import dirhash\n",
    "from fastapi import APIRouter, Depends, HTTPException, Query, status\n",
    "from sqlalchemy.exc import NoResultFound\n",
    "from sqlmodel import Session, select\n",
    "\n",
    "import airt_service.sanitizer\n",
    "\n",
    "# import airt_service\n",
    "from airt_service.auth import get_current_active_user\n",
    "from airt_service.constants import DS_HEAD_FILE_NAME, METADATA_FOLDER_PATH\n",
    "from airt_service.data.utils import delete_data_object_files_in_cloud\n",
    "from airt_service.db.models import (\n",
    "    DataBlob,\n",
    "    DataSource,\n",
    "    DataSourceRead,\n",
    "    Tag,\n",
    "    TagCreate,\n",
    "    User,\n",
    "    get_session,\n",
    ")\n",
    "from airt_service.errors import ERRORS, HTTPError\n",
    "from airt_service.helpers import commit_or_rollback, df_to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.4.4\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.9.0\n",
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from os import environ\n",
    "from time import sleep\n",
    "\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "from airt_service.aws.utils import create_s3_datasource_path, upload_to_s3_with_retry\n",
    "from airt_service.data.csv import process_csv\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.db.models import create_user_for_testing, get_session_with_context\n",
    "from airt_service.helpers import dict_to_df\n",
    "from airt_service.users import (\n",
    "    ActivateMFARequest,\n",
    "    activate_mfa,\n",
    "    disable_mfa,\n",
    "    generate_mfa_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a7194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mauodnfciq'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing()\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bdc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: log a random string\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"log a random string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Default router for all data sources\n",
    "datasource_router = APIRouter(\n",
    "    prefix=\"/datasource\",\n",
    "    tags=[\"datasource\"],\n",
    "    #     dependencies=[Depends(get_current_active_user)],\n",
    "    responses={\n",
    "        404: {\"description\": \"Not found\"},\n",
    "        500: {\n",
    "            \"model\": HTTPError,\n",
    "            \"description\": ERRORS[\"INTERNAL_SERVER_ERROR\"],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def calculate_properties(self: DataSource, cache_path: Path):\n",
    "    \"\"\"Calculate properties of datasource like no of rows, dtypes, head, hash from parquet files\n",
    "\n",
    "    Args:\n",
    "        cache_path: Cache folder path containing the synced parquet files\n",
    "    \"\"\"\n",
    "    self.hash = dirhash(cache_path, hashfunc=\"md5\")\n",
    "\n",
    "    ddf = dd.read_parquet(cache_path)\n",
    "    self.no_of_rows = ddf.shape[0].compute()\n",
    "\n",
    "    metadata_folder_path = cache_path / METADATA_FOLDER_PATH\n",
    "    metadata_folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "    head = ddf.head(10)\n",
    "    head.to_parquet(metadata_folder_path / DS_HEAD_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_7c4qpy_w\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_7c4qpy_w\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_7c4qpy_w\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_7c4qpy_w\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datasource = DataSource(\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "    session.refresh(datasource)\n",
    "\n",
    "    assert not datasource.no_of_rows\n",
    "    assert not datasource.path\n",
    "    assert not datasource.hash\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        processed_destination_path = test_s3_path.as_path()\n",
    "        ddf = dd.read_parquet(processed_destination_path)\n",
    "        expected_head = ddf.head(n=10)\n",
    "\n",
    "        datasource.calculate_properties(cache_path=processed_destination_path)\n",
    "\n",
    "        head_path = (\n",
    "            processed_destination_path / METADATA_FOLDER_PATH / DS_HEAD_FILE_NAME\n",
    "        )\n",
    "        actual_head = pd.read_parquet(head_path)\n",
    "\n",
    "    pd.testing.assert_frame_equal(actual_head, expected_head)\n",
    "    # assert datasource.hash == \"2f96b39df0f1f71a05d3ff5509c160e7\", datasource.hash\n",
    "    assert len(datasource.hash) == 32, len(datasource.hash)\n",
    "    assert datasource.no_of_rows == 498961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def remove_tag_from_previous_datasources(\n",
    "    self: DataSource, tag_name: str, session: Session\n",
    "):\n",
    "    \"\"\"Remove the tag_name associated with other/previous datasources\n",
    "\n",
    "    Args:\n",
    "        tag_name: Tag name to remove from other datasources\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "    tag_to_remove = Tag.get_by_name(name=tag_name, session=session)\n",
    "    try:\n",
    "        datasources = session.exec(\n",
    "            select(DataSource).where(\n",
    "                DataSource.datablob == self.datablob,\n",
    "                DataSource.user == self.user,\n",
    "            )\n",
    "        ).all()\n",
    "    except NoResultFound:\n",
    "        return\n",
    "\n",
    "    for datasource in datasources:\n",
    "        if tag_to_remove in datasource.tags:\n",
    "            datasource.tags.remove(tag_to_remove)\n",
    "            session.add(datasource)\n",
    "            session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9667baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSource(id=67, uuid=UUID('fe8af992-4c27-469a-9204-6a48aa5c3821'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 16), user_id=92, pulled_on=None, tags=[Tag(name='test', id=1, created=datetime.datetime(2022, 10, 18, 12, 28, 54), uuid=UUID('e9a39788-684b-455d-b15d-32b485206855'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=67, uuid=UUID('fe8af992-4c27-469a-9204-6a48aa5c3821'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 16), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=68, uuid=UUID('06281d8d-22da-4d80-a9c8-995fa97da773'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 16), user_id=92, pulled_on=None, tags=[Tag()])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    test_tag = Tag.get_by_name(name=\"test\", session=session)\n",
    "\n",
    "    uri = \"s3://bucket\"\n",
    "    db_uri = create_db_uri_for_s3_datablob(\n",
    "        uri=uri,\n",
    "        access_key=\"access\",\n",
    "        secret_key=\"secret\",\n",
    "    )\n",
    "    datablob_with_tag = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=db_uri,\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        tags=[test_tag],\n",
    "    )\n",
    "    datasource_with_tag = DataSource(\n",
    "        datablob=datablob_with_tag,\n",
    "        cloud_provider=datablob_with_tag.cloud_provider,\n",
    "        region=datablob_with_tag.region,\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        tags=datablob_with_tag.tags,\n",
    "    )\n",
    "    session.add(datasource_with_tag)\n",
    "    session.commit()\n",
    "    session.refresh(datasource_with_tag)\n",
    "    display(datasource_with_tag)\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        new_ds_with_test_tag = DataSource(\n",
    "            datablob=datablob_with_tag,\n",
    "            cloud_provider=datablob_with_tag.cloud_provider,\n",
    "            region=datablob_with_tag.region,\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "        new_ds_with_test_tag.remove_tag_from_previous_datasources(\n",
    "            tag_name=test_tag.name, session=session\n",
    "        )\n",
    "\n",
    "        datasource_without_test_tag = session.exec(\n",
    "            select(DataSource).where(DataSource.uuid == datasource_with_tag.uuid)\n",
    "        ).one()\n",
    "        display(datasource_without_test_tag)\n",
    "        session.add(datasource_without_test_tag)\n",
    "        assert test_tag not in datasource_without_test_tag\n",
    "        assert not datasource_without_test_tag.tags\n",
    "\n",
    "        new_ds_with_test_tag.tags.append(test_tag)\n",
    "        session.add(new_ds_with_test_tag)\n",
    "        display(new_ds_with_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0839af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "get_datasource_responses = {\n",
    "    400: {\"model\": HTTPError, \"description\": ERRORS[\"INCORRECT_DATASOURCE_ID\"]},\n",
    "    422: {\"model\": HTTPError, \"description\": \"DataSource error\"},\n",
    "}\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def get(cls: DataSource, uuid: str, user: User, session: Session) -> DataSource:\n",
    "    \"\"\"Get datasource based on uuid\n",
    "\n",
    "    Args:\n",
    "        uuid: Datasource uuid\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        Datasource object for given datasource uuid\n",
    "\n",
    "    Raises:\n",
    "        HTTPException: if datasource id is incorrect or if datasource is deleted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.uuid == uuid, DataSource.user == user)\n",
    "        ).one()\n",
    "    except NoResultFound:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"INCORRECT_DATASOURCE_ID\"],\n",
    "        )\n",
    "\n",
    "    if datasource.disabled:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"DATASOURCE_IS_DELETED\"],\n",
    "        )\n",
    "\n",
    "    if datasource.error is not None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=datasource.error\n",
    "        )\n",
    "\n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810ed5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSource(id=65, uuid=UUID('6aacca90-cf55-433a-b1b6-d850e1fbce8d'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 11), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Incorrect datasource id') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Incorrect datasource id') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    expected = user.datasources[0]\n",
    "\n",
    "    actual = DataSource.get(uuid=expected.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual == expected\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataSource.get(\n",
    "            uuid=\"00000000-0000-0000-0000-000000000000\", user=user, session=session\n",
    "        )\n",
    "    display(e)\n",
    "\n",
    "    user_kumaran = session.exec(select(User).where(User.username == \"kumaran\")).one()\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataSource.get(uuid=expected.uuid, user=user_kumaran, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d691f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Datasource is deleted') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    uri = \"s3://\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri, access_key=\"access\", secret_key=\"secret\"\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    datasource_disabled = DataSource(\n",
    "        datablob=datablob,\n",
    "        cloud_provider=datablob.cloud_provider,\n",
    "        region=datablob.region,\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        disabled=True,\n",
    "    )\n",
    "    session.add(datasource_disabled)\n",
    "    session.commit()\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataSource.get(uuid=datasource_disabled.uuid, user=user, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed95d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.get(\n",
    "    \"/{datasource_uuid}\",\n",
    "    response_model=DataSourceRead,\n",
    "    responses=get_datasource_responses,  # type: ignore\n",
    ")\n",
    "def get_details_of_datasource(\n",
    "    datasource_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataSource:\n",
    "    \"\"\"Get details of the datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datasource = DataSource.get(uuid=datasource_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a42d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSource(id=65, uuid=UUID('6aacca90-cf55-433a-b1b6-d850e1fbce8d'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 11), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    expected = user.datasources[0]\n",
    "\n",
    "    actual = get_details_of_datasource(\n",
    "        datasource_uuid=expected.uuid, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada680b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=422, detail='test error') tblen=3>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datasource errored out while importing\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    uri = \"s3://\"\n",
    "    datasource_errored = DataSource(\n",
    "        datablob=DataBlob(\n",
    "            type=\"s3\",\n",
    "            uri=create_db_uri_for_s3_datablob(\n",
    "                uri=uri, access_key=\"access\", secret_key=\"secret\"\n",
    "            ),\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            source=uri,\n",
    "            user=user,\n",
    "        ),\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        error=\"test error\",\n",
    "    )\n",
    "    session.add(datasource_errored)\n",
    "    session.commit()\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_details_of_datasource(\n",
    "            datasource_uuid=datasource_errored.uuid, user=user, session=session\n",
    "        )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def delete(self: DataSource, user: User, session: Session):\n",
    "    \"\"\"Delete a datasource\n",
    "\n",
    "    Args:\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "    delete_data_object_files_in_cloud(data_object=self)\n",
    "    self.disabled = True\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(self)\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.delete(\n",
    "    \"/{datasource_uuid}\",\n",
    "    response_model=DataSourceRead,\n",
    "    responses=get_datasource_responses,  # type: ignore\n",
    ")\n",
    "def delete_datasource(\n",
    "    datasource_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataSource:\n",
    "    \"\"\"Delete datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datasource = DataSource.get(uuid=datasource_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datasource.delete(user, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e432b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_9701xh6v\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_9701xh6v\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_9701xh6v\n",
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datasource/71\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datasource71_cached_wwqowm62\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datasource/71 locally in /tmp/s3kumaran-airt-service-eu-west-192datasource71_cached_wwqowm62\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-192datasource71_cached_wwqowm62 to s3://kumaran-airt-service-eu-west-1/92/datasource/71\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datasource71_cached_wwqowm62\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_9701xh6v\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=71, uuid=UUID('d163e287-9d79-43a4-8104-d6f8938305f0'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 17), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=71, uuid=UUID('d163e287-9d79-43a4-8104-d6f8938305f0'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=True, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 17), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    uri = \"s3://\"\n",
    "    datasource = DataSource(\n",
    "        datablob=DataBlob(\n",
    "            type=\"s3\",\n",
    "            uri=create_db_uri_for_s3_datablob(\n",
    "                uri=uri, access_key=\"access\", secret_key=\"secret\"\n",
    "            ),\n",
    "            source=uri,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        ),\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as source_s3_path:\n",
    "        bucket, s3_path = create_s3_datasource_path(\n",
    "            user_id=user.id, datasource_id=datasource.id, region=datasource.region\n",
    "        )\n",
    "        sleep(10)\n",
    "        with RemotePath.from_url(\n",
    "            remote_url=f\"s3://{bucket.name}/{s3_path}\",\n",
    "            pull_on_enter=False,\n",
    "            push_on_exit=True,\n",
    "            exist_ok=True,\n",
    "            parents=True,\n",
    "        ) as destination_s3_path:\n",
    "            shutil.copytree(\n",
    "                source_s3_path.as_path(),\n",
    "                destination_s3_path.as_path(),\n",
    "                dirs_exist_ok=True,\n",
    "            )\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource.id)\n",
    "    ).one()\n",
    "    display(datasource)\n",
    "    actual = delete_datasource(\n",
    "        datasource_uuid=datasource.uuid, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual.disabled == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7ea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('e8cb121a-66a5-4a1f-8cc5-2bb80c35665f'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'AWSAccessKeyId': '********************', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AccountId DefinitionId        OccurredTime  \\\n",
       "__null_dask_index__                                               \n",
       "0                       312571   loadTests2 2019-12-31 21:30:02   \n",
       "1                       312571   loadTests3 2020-01-03 23:53:22   \n",
       "2                       312571   loadTests1 2020-01-07 02:16:42   \n",
       "3                       312571   loadTests2 2020-01-10 04:40:02   \n",
       "4                       312571   loadTests3 2020-01-13 07:03:22   \n",
       "\n",
       "                     OccurredTimeTicks  PersonId  \n",
       "__null_dask_index__                               \n",
       "0                        1577836802678         2  \n",
       "1                        1578104602678         2  \n",
       "2                        1578372402678         2  \n",
       "3                        1578640202678         2  \n",
       "4                        1578908002678         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/file.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccountId,DefinitionId,OccurredTime,OccurredTimeTicks,PersonId\n",
      "312571,loadTests2,2019-12-31 21:30:02,1577836802678,2\n",
      "312571,loadTests3,2020-01-03 23:53:22,1578104602678,2\n",
      "312571,loadTests1,2020-01-07 02:16:42,1578372402678,2\n",
      "312571,loadTests2,2020-01-10 04:40:02,1578640202678,2\n",
      "312571,loadTests3,2020-01-13 07:03:22,1578908002678,2\n",
      "312571,loadTests1,2020-01-16 09:26:42,1579175802678,2\n",
      "312571,loadTests2,2020-01-19 11:50:02,1579443602678,2\n",
      "312571,loadTests3,2020-01-22 14:13:22,1579711402678,2\n",
      "312571,loadTests1,2020-01-25 16:36:42,1579979202678,2\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_rtn1ty_8\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): processing user uploaded csv file for datablob_id=296 and uploading parquet back to S3 for datasource_id=74\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/92/datablob/296\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datablob/296\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datablob/296 locally in /tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/92/datablob/296 to /tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datasource/74\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datasource/74 locally in /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51/file.csv,..., /tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51/file.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u\n",
      "[INFO] airt.dask_manager: Starting cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/distributed/node.py:183: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45067 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:45165' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:45067/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/distributed/node.py:183: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40243 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:33627' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:40243/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.8/site-packages/distributed/node.py:183: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 41435 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:46535' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:41435/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/92/datasource/74, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u to s3://kumaran-airt-service-eu-west-1/92/datasource/74\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datasource74_cached_jro6198u\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datablob296_cached_oy5woc51\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=296, datasource_id=74): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=74, uuid=UUID('c3118331-0df9-4061-9efd-05922e545d5d'), hash='64ab63985d6651f495ddccd4d96d16cb', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/92/datasource/74', created=datetime.datetime(2022, 10, 19, 10, 38, 53), user_id=92, pulled_on=datetime.datetime(2022, 10, 19, 10, 39, 9), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and pull datasource to use in following tests\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    from_local_request = FromLocalRequest(\n",
    "        path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "    )\n",
    "    from_local_response = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        df = pd.read_parquet(test_s3_path.as_path())\n",
    "        display(df.head())\n",
    "        df.to_csv(test_s3_path.as_path() / \"file.csv\", index=False)\n",
    "        display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "        !head -n 10 {test_s3_path.as_path()/\"file.csv\"}\n",
    "\n",
    "        upload_to_s3_with_retry(\n",
    "            test_s3_path.as_path() / \"file.csv\",\n",
    "            from_local_response.presigned[\"url\"],\n",
    "            from_local_response.presigned[\"fields\"],\n",
    "        )\n",
    "\n",
    "    datablob_id = (\n",
    "        session.exec(select(DataBlob).where(DataBlob.uuid == from_local_response.uuid))\n",
    "        .one()\n",
    "        .id\n",
    "    )\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    datasource_id = (\n",
    "        session.exec(select(DataSource).where(DataSource.uuid == datasource.uuid))\n",
    "        .one()\n",
    "        .id\n",
    "    )\n",
    "    process_csv(\n",
    "        datablob_id=datablob_id,\n",
    "        datasource_id=datasource_id,\n",
    "        deduplicate_data=True,\n",
    "        index_column=\"PersonId\",\n",
    "        sort_by=\"OccurredTime\",\n",
    "        blocksize=\"256MB\",\n",
    "        kwargs_json=json.dumps(\n",
    "            dict(\n",
    "                usecols=[0, 1, 2, 3, 4],\n",
    "                parse_dates=[\"OccurredTime\"],\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource_synced = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_id)\n",
    "    ).one()\n",
    "    display(datasource_synced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _get_ds_head_and_dtypes(datasource_s3_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Read the head metadata file and return its contents as a dict\n",
    "\n",
    "    Args:\n",
    "        datasource_s3_path: Input datasource S3 path\n",
    "\n",
    "    Returns:\n",
    "        The head along with its dtypes as a dict\n",
    "    \"\"\"\n",
    "    s3_metadata_path = f\"{datasource_s3_path}/{METADATA_FOLDER_PATH}\"\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=s3_metadata_path,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as local_metadata_path:\n",
    "        processed_local_metadata_path = local_metadata_path.as_path()\n",
    "\n",
    "        df = pd.read_parquet(processed_local_metadata_path / DS_HEAD_FILE_NAME)\n",
    "        return df_to_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_o_ymhbfj\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt locally in /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_o_ymhbfj\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt to /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_o_ymhbfj\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_o_ymhbfj\n"
     ]
    }
   ],
   "source": [
    "df_dict = _get_ds_head_and_dtypes(datasource_synced.path)\n",
    "\n",
    "assert df_dict[\"data\"][\"columns\"] == [\n",
    "    \"AccountId\",\n",
    "    \"DefinitionId\",\n",
    "    \"OccurredTime\",\n",
    "    \"OccurredTimeTicks\",\n",
    "]\n",
    "assert df_dict[\"data\"][\"index_names\"] == [\"PersonId\"]\n",
    "assert df_dict[\"dtypes\"] == {\n",
    "    \"AccountId\": \"int64\",\n",
    "    \"DefinitionId\": \"object\",\n",
    "    \"OccurredTime\": \"datetime64[ns]\",\n",
    "    \"OccurredTimeTicks\": \"int64\",\n",
    "}\n",
    "assert len(df_dict[\"data\"][\"data\"]) == 10\n",
    "assert len(df_dict) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b241ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def is_ready(self: DataSource):\n",
    "    \"\"\"Check if the datasource's completed steps equal to total steps, else raise HTTPException\"\"\"\n",
    "    if self.completed_steps != self.total_steps:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_412_PRECONDITION_FAILED,\n",
    "            detail=ERRORS[\"DATASOURCE_IS_NOT_PULLED\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d211a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.get(\n",
    "    \"/{datasource_uuid}/head\",\n",
    "    responses={\n",
    "        **get_datasource_responses,  # type: ignore\n",
    "        412: {\"model\": HTTPError, \"description\": ERRORS[\"DATASOURCE_IS_NOT_PULLED\"]},\n",
    "    },\n",
    ")\n",
    "def datasource_head_route(\n",
    "    datasource_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Dict[str, List[Any]]:\n",
    "    \"\"\"Get head of the datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datasource = DataSource.get(uuid=datasource_uuid, user=user, session=session)  # type: ignore\n",
    "    datasource.is_ready()\n",
    "\n",
    "    df_dict = _get_ds_head_and_dtypes(datasource_s3_path=datasource.path)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548b6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_0baie8pi\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt locally in /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_0baie8pi\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt to /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_0baie8pi\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_0baie8pi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PersonId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-16 09:26:42</td>\n",
       "      <td>1579175802678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-19 11:50:02</td>\n",
       "      <td>1579443602678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-22 14:13:22</td>\n",
       "      <td>1579711402678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-25 16:36:42</td>\n",
       "      <td>1579979202678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-28 19:00:02</td>\n",
       "      <td>1580247002678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AccountId DefinitionId        OccurredTime  OccurredTimeTicks\n",
       "PersonId                                                               \n",
       "2            312571   loadTests2 2019-12-31 21:30:02      1577836802678\n",
       "2            312571   loadTests3 2020-01-03 23:53:22      1578104602678\n",
       "2            312571   loadTests1 2020-01-07 02:16:42      1578372402678\n",
       "2            312571   loadTests2 2020-01-10 04:40:02      1578640202678\n",
       "2            312571   loadTests3 2020-01-13 07:03:22      1578908002678\n",
       "2            312571   loadTests1 2020-01-16 09:26:42      1579175802678\n",
       "2            312571   loadTests2 2020-01-19 11:50:02      1579443602678\n",
       "2            312571   loadTests3 2020-01-22 14:13:22      1579711402678\n",
       "2            312571   loadTests1 2020-01-25 16:36:42      1579979202678\n",
       "2            312571   loadTests2 2020-01-28 19:00:02      1580247002678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datasource_synced = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_synced.id)\n",
    "    ).one()\n",
    "\n",
    "    actual = datasource_head_route(\n",
    "        datasource_uuid=datasource_synced.uuid, user=user, session=session\n",
    "    )\n",
    "    assert isinstance(actual, dict)\n",
    "\n",
    "    actual_df = dict_to_df(actual)\n",
    "    assert actual_df.index.name == \"PersonId\"\n",
    "    pd.testing.assert_series_equal(actual_df.dtypes, pd.Series(actual[\"dtypes\"]))\n",
    "\n",
    "    display(actual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://\"\n",
    "\n",
    "    datasource_without_pull = DataSource(\n",
    "        datablob=DataBlob(\n",
    "            type=\"s3\",\n",
    "            uri=create_db_uri_for_s3_datablob(\n",
    "                uri=uri, access_key=\"access\", secret_key=\"secret\"\n",
    "            ),\n",
    "            source=uri,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        ),\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource_without_pull)\n",
    "    session.commit()\n",
    "    with pytest.raises(HTTPException):\n",
    "        datasource_head_route(\n",
    "            datasource_uuid=datasource_without_pull.uuid, user=user, session=session\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.get(\n",
    "    \"/{datasource_uuid}/dtypes\",\n",
    "    responses={\n",
    "        **get_datasource_responses,  # type: ignore\n",
    "        412: {\"model\": HTTPError, \"description\": ERRORS[\"DATASOURCE_IS_NOT_PULLED\"]},\n",
    "    },\n",
    ")\n",
    "def datasource_dtypes_route(\n",
    "    datasource_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Get columns and its dtypes of the datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    # get locally saved parquet file path, read it and return columns and its dtypes\n",
    "    datasource = DataSource.get(uuid=datasource_uuid, user=user, session=session)  # type: ignore\n",
    "    datasource.is_ready()\n",
    "\n",
    "    df_dict = _get_ds_head_and_dtypes(datasource_s3_path=datasource.path)\n",
    "    return df_dict[\"dtypes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_yl7bu38p\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt locally in /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_yl7bu38p\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/92/datasource/74/.metadata_by_airt to /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_yl7bu38p\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-192datasource74metadata_by_airt_cached_yl7bu38p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AccountId': 'int64',\n",
       " 'DefinitionId': 'object',\n",
       " 'OccurredTime': 'datetime64[ns]',\n",
       " 'OccurredTimeTicks': 'int64'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    session.commit()\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datasource_synced = session.exec(\n",
    "        select(DataSource).where(DataSource.uuid == datasource_synced.uuid)\n",
    "    ).one()\n",
    "    # expected = user.datasources[-1]\n",
    "\n",
    "    actual = datasource_dtypes_route(\n",
    "        datasource_uuid=datasource_synced.uuid, user=user, session=session\n",
    "    )\n",
    "    assert actual == {\n",
    "        \"AccountId\": \"int64\",\n",
    "        \"DefinitionId\": \"object\",\n",
    "        \"OccurredTime\": \"datetime64[ns]\",\n",
    "        \"OccurredTimeTicks\": \"int64\",\n",
    "    }\n",
    "\n",
    "    display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ea129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datasource_without_pull = session.merge(datasource_without_pull)\n",
    "\n",
    "    with pytest.raises(HTTPException):\n",
    "        datasource_dtypes_route(\n",
    "            datasource_uuid=datasource_without_pull.uuid, user=user, session=session\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def get_all(\n",
    "    cls: DataSource,\n",
    "    disabled: bool,\n",
    "    completed: bool,\n",
    "    offset: int,\n",
    "    limit: int,\n",
    "    user: User,\n",
    "    session: Session,\n",
    ") -> List[DataSource]:\n",
    "    \"\"\"Get all datasources created by the user\n",
    "\n",
    "    Args:\n",
    "        disabled: Whether to get disabled datasources\n",
    "        completed: Whether to include only datasources which are pulled from its source\n",
    "        offset: Offset results by given integer\n",
    "        limit: Limit results by given integer\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        A list of datasource objects\n",
    "    \"\"\"\n",
    "    statement = select(DataSource).where(DataSource.user == user)\n",
    "    statement = statement.where(DataSource.disabled == disabled)\n",
    "    if completed:\n",
    "        statement = statement.where(\n",
    "            DataSource.completed_steps == DataSource.total_steps\n",
    "        )\n",
    "    # get all data sources from db\n",
    "    return session.exec(statement.offset(offset).limit(limit)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.get(\"/\", response_model=List[DataSourceRead])\n",
    "def get_all_datasources(\n",
    "    disabled: bool = False,\n",
    "    completed: bool = False,\n",
    "    offset: int = 0,\n",
    "    limit: int = Query(default=100, lte=100),\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> List[DataSource]:\n",
    "    \"\"\"Get all datasources created by user\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataSource.get_all(\n",
    "        disabled=disabled,\n",
    "        completed=completed,\n",
    "        offset=offset,\n",
    "        limit=limit,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e78fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataSource(id=65, uuid=UUID('6aacca90-cf55-433a-b1b6-d850e1fbce8d'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 11), user_id=92, pulled_on=None, tags=[])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    actual = get_all_datasources(\n",
    "        disabled=False, completed=False, offset=0, limit=1, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "\n",
    "    assert len(actual) == 1\n",
    "    assert isinstance(actual[0], DataSource)\n",
    "    assert actual[0] == user.datasources[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ced41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(actual)=6'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = get_all_datasources(\n",
    "    disabled=False, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datasource in actual:\n",
    "    assert not datasource.disabled\n",
    "assert actual[0] == user.datasources[0]\n",
    "actual = get_all_datasources(\n",
    "    disabled=True, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datasource in actual:\n",
    "    assert datasource.disabled\n",
    "\n",
    "actual = get_all_datasources(\n",
    "    disabled=False, completed=True, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datasource in actual:\n",
    "    assert datasource.completed_steps == datasource.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e15085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def tag(self: DataSource, tag_name: str, session: Session):\n",
    "    \"\"\"Tag an existing datasource\n",
    "\n",
    "    Args:\n",
    "        tag_name: A string to tag the datasource\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "\n",
    "    user_tag = Tag.get_by_name(name=tag_name, session=session)\n",
    "\n",
    "    self.remove_tag_from_previous_datasources(tag_name=user_tag.name, session=session)  # type: ignore\n",
    "    self.tags.append(user_tag)\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(self)\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac43bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@datasource_router.post(\"/{datasource_uuid}/tag\", response_model=DataSourceRead)\n",
    "def tag_datasource(\n",
    "    datasource_uuid: str,\n",
    "    tag_to_create: TagCreate,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataSource:\n",
    "    \"\"\"Add tag to datasource\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datasource = DataSource.get(uuid=datasource_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datasource.tag(tag_name=tag_to_create.name, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f3192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSource(id=65, uuid=UUID('6aacca90-cf55-433a-b1b6-d850e1fbce8d'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 38, 11), user_id=92, pulled_on=None, tags=[Tag(name='new_tag', id=9, created=datetime.datetime(2022, 10, 18, 12, 30, 43), uuid=UUID('1662ea89-5018-4728-a71a-ecc502a22fd7'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datasource = user.datasources[0]\n",
    "\n",
    "    tag_name = \"new_tag\"\n",
    "    tag_to_create = TagCreate(name=tag_name)\n",
    "    actual = tag_datasource(\n",
    "        datasource_uuid=datasource.uuid,\n",
    "        tag_to_create=tag_to_create,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "    display(actual)\n",
    "    tag_found = False\n",
    "    for tag in actual.tags:\n",
    "        if tag.name == tag_name:\n",
    "            tag_found = True\n",
    "            break\n",
    "    assert tag_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aead57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def _create(\n",
    "    cls: DataSource,\n",
    "    *,\n",
    "    datablob: DataBlob,\n",
    "    total_steps: int = 1,\n",
    "    user: User,\n",
    "    session: Session,\n",
    ") -> DataSource:\n",
    "    \"\"\"Create new datasource based on given params\n",
    "\n",
    "    Args:\n",
    "        datablob: Datablob object\n",
    "        total_steps: Total steps\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        The created datasource object\n",
    "    \"\"\"\n",
    "    with commit_or_rollback(session):\n",
    "        datasource = DataSource(\n",
    "            datablob=datablob,\n",
    "            cloud_provider=datablob.cloud_provider,\n",
    "            region=datablob.region,\n",
    "            total_steps=total_steps,\n",
    "            user=user,\n",
    "        )\n",
    "\n",
    "    for tag in datablob.tags:\n",
    "        datasource.tag(tag_name=tag.name, session=session)  # type: ignore\n",
    "\n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3331dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataSource(id=82, uuid=UUID('4f51e904-3cea-4d42-8839-d1ecf300bce2'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 19, 10, 39, 38), user_id=92, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    uri = \"s3://\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri, access_key=\"access\", secret_key=\"secret\"\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    actual = DataSource._create(\n",
    "        datablob=datablob, total_steps=1, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual.uuid is not None\n",
    "\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.uuid == actual.uuid)\n",
    "    ).one()\n",
    "    assert datasource.tags == datablob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652b23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
