{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to delete objects created by user and to delete user\n",
    "output-file: cleanup.html\n",
    "title: Cleanup\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a0a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from typing import *\n",
    "\n",
    "from sqlmodel import Session, select\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt.logger import get_logger\n",
    "from airt_service.auth import delete_apikey\n",
    "from airt_service.data.datasource import delete_datasource\n",
    "from airt_service.data.datablob import delete_datablob\n",
    "from airt_service.db.models import (\n",
    "    User,\n",
    "    Prediction,\n",
    "    Model,\n",
    "    DataSource,\n",
    "    DataBlob,\n",
    "    APIKey,\n",
    ")\n",
    "from airt_service.aws.utils import get_s3_storage_bucket\n",
    "from airt_service.model.prediction import delete_prediction\n",
    "from airt_service.model.train import delete_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.5.1\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.10.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import requests\n",
    "from fastapi import BackgroundTasks\n",
    "from sqlalchemy.exc import NoResultFound\n",
    "\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.auth import create_apikey\n",
    "from airt_service.aws.utils import upload_to_s3_with_retry\n",
    "from airt_service.data.csv import process_csv\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.db.models import (\n",
    "    get_session,\n",
    "    get_session_with_context,\n",
    "    create_user_for_testing,\n",
    "    APIKeyCreate,\n",
    ")\n",
    "from airt_service.helpers import set_env_variable_context\n",
    "from airt_service.model.train import TrainRequest, train_model, predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ngkjpwvkdg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "User(id=4, uuid=UUID('c23cdcaf-293b-47c8-96ff-655b50d18fc3'), username='ngkjpwvkdg', first_name='unittest', last_name='user', email='ngkjpwvkdg@email.com', subscription_type=<SubscriptionType.small: 'small'>, super_user=False, disabled=False, created=datetime.datetime(2022, 11, 7, 9, 9, 59), phone_number=None, is_phone_number_verified=False, mfa_secret=****, is_mfa_active=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)\n",
    "with get_session_with_context() as session:\n",
    "    display(session.exec(select(User).where(User.username == test_username)).one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and pull datasource to use in following tests\n",
    "\n",
    "\n",
    "def _populate_user(username: str):\n",
    "    \"\"\"\n",
    "    Helper function to create valid apikey, datablob, datasource and predictions for given user\n",
    "\n",
    "    Args:\n",
    "        username: username to use to create objects\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(select(User).where(User.username == username)).one()\n",
    "\n",
    "        create_apikey(\n",
    "            apikey_to_create=APIKeyCreate(expiry=datetime.utcnow() + timedelta(days=1)),\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        from_local_request = FromLocalRequest(\n",
    "            path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "        )\n",
    "        from_local_response = from_local_start_route(\n",
    "            from_local_request=from_local_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        with RemotePath.from_url(\n",
    "            remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "            pull_on_enter=True,\n",
    "            push_on_exit=False,\n",
    "            exist_ok=True,\n",
    "            parents=False,\n",
    "            access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ) as test_s3_path:\n",
    "            df = pd.read_parquet(test_s3_path.as_path())\n",
    "            display(df.head())\n",
    "            df.to_csv(test_s3_path.as_path() / \"file.csv\", index=False)\n",
    "            display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "            #         !head -n 10 {test_s3_path.as_path()/\"file.csv\"}\n",
    "\n",
    "            upload_to_s3_with_retry(\n",
    "                test_s3_path.as_path() / \"file.csv\",\n",
    "                from_local_response.presigned[\"url\"],\n",
    "                from_local_response.presigned[\"fields\"],\n",
    "            )\n",
    "\n",
    "        datablob_id = session.exec(\n",
    "            select(DataBlob).where(DataBlob.uuid == from_local_response.uuid)\n",
    "        ).one().id\n",
    "        \n",
    "        display(datablob_id)\n",
    "        assert datablob_id > 0\n",
    "        \n",
    "        datasource = DataSource(\n",
    "            datablob_id=datablob_id,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "        session.add(datasource)\n",
    "        session.commit()\n",
    "\n",
    "        process_csv(\n",
    "            datablob_id=datablob_id,\n",
    "            datasource_id=datasource.id,\n",
    "            deduplicate_data=True,\n",
    "            index_column=\"PersonId\",\n",
    "            sort_by=\"OccurredTime\",\n",
    "            blocksize=\"256MB\",\n",
    "            kwargs_json=json.dumps(\n",
    "                dict(\n",
    "                    usecols=[0, 1, 2, 3, 4],\n",
    "                    parse_dates=[\"OccurredTime\"],\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    with get_session_with_context() as session:\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.id == datasource.id)\n",
    "        ).one()\n",
    "        display(datasource)\n",
    "\n",
    "        train_request = TrainRequest(\n",
    "            data_uuid=datasource.uuid,\n",
    "            client_column=\"AccountId\",\n",
    "            target_column=\"DefinitionId\",\n",
    "            target=\"load*\",\n",
    "            predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "        )\n",
    "\n",
    "        model = train_model(train_request=train_request, user=user, session=session)\n",
    "        display(model)\n",
    "        # Call exec_cli train_model\n",
    "\n",
    "        b = BackgroundTasks()\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            predicted = predict_model(\n",
    "                model_uuid=model.uuid, user=user, session=session, background_tasks=b\n",
    "            )\n",
    "        display(predicted)\n",
    "        # Call exec_cli predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62618e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('334ee5c9-79a5-4751-be27-69a6cf185e3e'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'x-amz-algorithm': 'AWS4-HMAC-SHA256', 'x-amz-credential': '********************/20221107/eu-west-1/s3/aws4_request', 'x-amz-date': '20221107T091000Z', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'x-amz-signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AccountId DefinitionId        OccurredTime  \\\n",
       "__null_dask_index__                                               \n",
       "0                       312571   loadTests2 2019-12-31 21:30:02   \n",
       "1                       312571   loadTests3 2020-01-03 23:53:22   \n",
       "2                       312571   loadTests1 2020-01-07 02:16:42   \n",
       "3                       312571   loadTests2 2020-01-10 04:40:02   \n",
       "4                       312571   loadTests3 2020-01-13 07:03:22   \n",
       "\n",
       "                     OccurredTimeTicks  PersonId  \n",
       "__null_dask_index__                               \n",
       "0                        1577836802678         2  \n",
       "1                        1578104602678         2  \n",
       "2                        1578372402678         2  \n",
       "3                        1578640202678         2  \n",
       "4                        1578908002678         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/file.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_bfs0fv0k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): processing user uploaded csv file for datablob_id=1 and uploading parquet back to S3 for datasource_id=2\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/4/datablob/1\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/4/datablob/1\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/4/datablob/1 locally in /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/4/datablob/1 to /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/4/datasource/2\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/4/datasource/2 locally in /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp/file.csv,..., /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp/file.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:39649' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:41955' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:46597' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/4/datasource/2, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya to s3://kumaran-airt-service-eu-west-1/4/datasource/2\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-14datasource2_cached_0omu8nya\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_iapbe7vp\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=1, datasource_id=2): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=2, uuid=UUID('03c6196f-0042-4733-80d7-0e285b353be6'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/4/datasource/2', created=datetime.datetime(2022, 11, 7, 9, 10, 13), user_id=4, pulled_on=datetime.datetime(2022, 11, 7, 9, 10, 19), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Model(total_steps=5, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, datasource_id=2, client_column='AccountId', error=None, user_id=4, target_column='DefinitionId', region='eu-west-1', target='load*', disabled=False, predict_after=datetime.timedelta(days=20), created=datetime.datetime(2022, 11, 7, 9, 10, 38), timestamp_column=None, id=1, uuid=UUID('91e40f4e-9f9d-45a1-a016-dde4c1d5b29a'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 1', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 1', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(id=1, datasource_id=2, uuid=UUID('973cc1af-23d0-45b8-a519-cc0644c3186d'), error=None, total_steps=3, disabled=False, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, model_id=1, created=datetime.datetime(2022, 11, 7, 9, 10, 38), path=None, region='eu-west-1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_populate_user(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fedbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_predictions(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup predictions\"\"\"\n",
    "    logger.info(\"deleting predictions\")\n",
    "    predictions = session.exec(\n",
    "        select(Prediction).join(Model).where(Model.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for prediction in predictions:\n",
    "        delete_prediction(\n",
    "            prediction_uuid=prediction.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(prediction)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting predictions\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_predictions(user_to_cleanup, session)\n",
    "\n",
    "    predictions = session.exec(\n",
    "        select(Prediction).join(Model).where(Model.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(predictions) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a20c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_models(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup models\"\"\"\n",
    "    logger.info(\"deleting models\")\n",
    "    models = session.exec(select(Model).where(Model.user == user_to_cleanup)).all()\n",
    "\n",
    "    for model in models:\n",
    "        delete_model(\n",
    "            model_uuid=model.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(model)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ee190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting models\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_models(user_to_cleanup, session)\n",
    "\n",
    "    models = session.exec(select(Model).where(Model.user == user_to_cleanup)).all()\n",
    "    assert len(models) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8948032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_datasources(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup datasources\"\"\"\n",
    "    logger.info(\"deleting datasources\")\n",
    "    datasources = session.exec(\n",
    "        select(DataSource).where(DataSource.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for datasource in datasources:\n",
    "        delete_datasource(\n",
    "            datasource_uuid=datasource.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(datasource)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting datasources\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_datasources(user_to_cleanup, session)\n",
    "\n",
    "    datasources = session.exec(\n",
    "        select(DataSource).where(DataSource.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(datasources) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_datablobs(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup datablobs\"\"\"\n",
    "    logger.info(\"deleting datablobs\")\n",
    "    datablobs = session.exec(\n",
    "        select(DataBlob).where(DataBlob.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for datablob in datablobs:\n",
    "        delete_datablob(\n",
    "            datablob_uuid=datablob.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(datablob)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fa825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting datablobs\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_datablobs(user_to_cleanup, session)\n",
    "\n",
    "    datablobs = session.exec(\n",
    "        select(DataBlob).where(DataBlob.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(datablobs) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_apikeys(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup apikeys\"\"\"\n",
    "    logger.info(\"deleting apikeys\")\n",
    "    apikeys = session.exec(select(APIKey).where(APIKey.user == user_to_cleanup)).all()\n",
    "\n",
    "    for apikey in apikeys:\n",
    "        delete_apikey(\n",
    "            user_uuid_or_name=str(user_to_cleanup.uuid),  # type: ignore\n",
    "            key_uuid_or_name=str(apikey.uuid),  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(apikey)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting apikeys\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_apikeys(user_to_cleanup, session)\n",
    "\n",
    "    apikeys = session.exec(select(APIKey).where(APIKey.user == user_to_cleanup)).all()\n",
    "    assert len(apikeys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def cleanup_user(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup user\"\"\"\n",
    "    cleanup_predictions(user_to_cleanup, session)\n",
    "    cleanup_models(user_to_cleanup, session)\n",
    "    cleanup_datasources(user_to_cleanup, session)\n",
    "    cleanup_datablobs(user_to_cleanup, session)\n",
    "    cleanup_apikeys(user_to_cleanup, session)\n",
    "\n",
    "    bucket, base_path = get_s3_storage_bucket()  # type: ignore\n",
    "    s3_path = (\n",
    "        f\"{base_path}/{user_to_cleanup.id}\" if base_path else str(user_to_cleanup.id)\n",
    "    )\n",
    "    logger.info(f\"Deleting user files in s3://{bucket.name}/{s3_path}\")\n",
    "    bucket.objects.filter(Prefix=s3_path + \"/\").delete()\n",
    "\n",
    "    logger.info(\"deleting user\")\n",
    "    session.delete(user_to_cleanup)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5393eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting predictions\n",
      "[INFO] __main__: deleting models\n",
      "[INFO] __main__: deleting datasources\n",
      "[INFO] __main__: deleting datablobs\n",
      "[INFO] __main__: deleting apikeys\n",
      "[INFO] __main__: Deleting user files in s3://kumaran-airt-service-eu-west-1/4\n",
      "[INFO] __main__: deleting user\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_user(user_to_cleanup, session)\n",
    "\n",
    "    with pytest.raises(NoResultFound):\n",
    "        session.exec(select(User).where(User.username == test_username)).one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff990657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
