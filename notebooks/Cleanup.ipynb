{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fce53d9c",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to delete objects created by user and to delete user\n",
    "output-file: cleanup.html\n",
    "title: Cleanup\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699cb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 09:14:30.423110: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[WARNING] airt.testing.activate_by_import: Failed to set gpu memory limit for tf; This could happen because of no gpu availability\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a0a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt.logger import get_logger\n",
    "from airt_service.auth import delete_apikey\n",
    "from airt_service.aws.utils import get_s3_storage_bucket\n",
    "from airt_service.confluent import delete_topics_for_user\n",
    "from airt_service.data.datablob import delete_datablob\n",
    "from airt_service.data.datasource import delete_datasource\n",
    "from airt_service.db.models import APIKey, DataBlob, DataSource, Model, Prediction, User\n",
    "from airt_service.model.prediction import delete_prediction\n",
    "from airt_service.model.train import delete_model\n",
    "from sqlmodel import Session, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.5.1\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.10.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from os import environ\n",
    "\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import requests\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.auth import create_apikey\n",
    "from airt_service.aws.utils import upload_to_s3_with_retry\n",
    "from airt_service.confluent import create_topics_for_user\n",
    "from airt_service.data.csv import process_csv\n",
    "from airt_service.data.datablob import FromLocalRequest, from_local_start_route\n",
    "from airt_service.db.models import (\n",
    "    APIKeyCreate,\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    "    get_session_with_context,\n",
    ")\n",
    "from airt_service.helpers import set_env_variable_context\n",
    "from airt_service.model.train import TrainRequest, predict_model, train_model\n",
    "from fastapi import BackgroundTasks\n",
    "from sqlalchemy.exc import NoResultFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glmfgiaofy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "User(id=135, uuid=UUID('e40c4ceb-3c66-4ba8-bb8b-2a367921b180'), username='glmfgiaofy', first_name='unittest', last_name='user', email='glmfgiaofy@email.com', subscription_type=<SubscriptionType.small: 'small'>, super_user=False, disabled=False, created=datetime.datetime(2023, 1, 6, 9, 14, 34), phone_number=None, is_phone_number_verified=False, mfa_secret=****, is_mfa_active=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)\n",
    "with get_session_with_context() as session:\n",
    "    display(session.exec(select(User).where(User.username == test_username)).one())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and pull datasource to use in following tests\n",
    "\n",
    "\n",
    "def _populate_user(username: str):\n",
    "    \"\"\"\n",
    "    Helper function to create valid apikey, datablob, datasource and predictions for given user\n",
    "\n",
    "    Args:\n",
    "        username: username to use to create objects\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(select(User).where(User.username == username)).one()\n",
    "\n",
    "        create_topics_for_user(username=username)\n",
    "\n",
    "        create_apikey(\n",
    "            apikey_to_create=APIKeyCreate(expiry=datetime.utcnow() + timedelta(days=1)),\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        from_local_request = FromLocalRequest(\n",
    "            path=\"tmp/test-folder/\", tag=\"my_csv_datasource_tag\"\n",
    "        )\n",
    "        from_local_response = from_local_start_route(\n",
    "            from_local_request=from_local_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        with RemotePath.from_url(\n",
    "            remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "            pull_on_enter=True,\n",
    "            push_on_exit=False,\n",
    "            exist_ok=True,\n",
    "            parents=False,\n",
    "            access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ) as test_s3_path:\n",
    "            df = pd.read_parquet(test_s3_path.as_path())\n",
    "            display(df.head())\n",
    "            df.to_csv(test_s3_path.as_path() / \"file.csv\", index=False)\n",
    "            display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "            #         !head -n 10 {test_s3_path.as_path()/\"file.csv\"}\n",
    "\n",
    "            upload_to_s3_with_retry(\n",
    "                test_s3_path.as_path() / \"file.csv\",\n",
    "                from_local_response.presigned[\"url\"],\n",
    "                from_local_response.presigned[\"fields\"],\n",
    "            )\n",
    "\n",
    "        datablob_id = (\n",
    "            session.exec(\n",
    "                select(DataBlob).where(DataBlob.uuid == from_local_response.uuid)\n",
    "            )\n",
    "            .one()\n",
    "            .id\n",
    "        )\n",
    "\n",
    "        display(datablob_id)\n",
    "        assert datablob_id > 0\n",
    "\n",
    "        datasource = DataSource(\n",
    "            datablob_id=datablob_id,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "        session.add(datasource)\n",
    "        session.commit()\n",
    "\n",
    "        process_csv(\n",
    "            datablob_id=datablob_id,\n",
    "            datasource_id=datasource.id,\n",
    "            deduplicate_data=True,\n",
    "            index_column=\"PersonId\",\n",
    "            sort_by=\"OccurredTime\",\n",
    "            blocksize=\"256MB\",\n",
    "            kwargs_json=json.dumps(\n",
    "                dict(\n",
    "                    usecols=[0, 1, 2, 3, 4],\n",
    "                    parse_dates=[\"OccurredTime\"],\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    with get_session_with_context() as session:\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.id == datasource.id)\n",
    "        ).one()\n",
    "        display(datasource)\n",
    "\n",
    "        train_request = TrainRequest(\n",
    "            data_uuid=datasource.uuid,\n",
    "            client_column=\"AccountId\",\n",
    "            target_column=\"DefinitionId\",\n",
    "            target=\"load*\",\n",
    "            predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "        )\n",
    "\n",
    "        model = train_model(train_request=train_request, user=user, session=session)\n",
    "        display(model)\n",
    "        # Call exec_cli train_model\n",
    "\n",
    "        b = BackgroundTasks()\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            predicted = predict_model(\n",
    "                model_uuid=model.uuid, user=user, session=session, background_tasks=b\n",
    "            )\n",
    "        display(predicted)\n",
    "        # Call exec_cli predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62618e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_data created\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_realitime_data created\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_data_status created\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_model_status created\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_model_metrics created\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_prediction created\n",
      "[INFO] botocore.credentials: Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1672996474.003|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1672996474.003|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.datablob: DataBlob.from_local(): FromLocalResponse(uuid=UUID('e478478c-258e-4622-818d-595bbcd2c5a4'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'x-amz-algorithm': 'AWS4-HMAC-SHA256', 'x-amz-credential': '********************/20230106/eu-west-1/s3/aws4_request', 'x-amz-date': '20230106T091434Z', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'x-amz-signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__null_dask_index__</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AccountId DefinitionId        OccurredTime  \\\n",
       "__null_dask_index__                                               \n",
       "0                       312571   loadTests2 2019-12-31 21:30:02   \n",
       "1                       312571   loadTests3 2020-01-03 23:53:22   \n",
       "2                       312571   loadTests1 2020-01-07 02:16:42   \n",
       "3                       312571   loadTests2 2020-01-10 04:40:02   \n",
       "4                       312571   loadTests3 2020-01-13 07:03:22   \n",
       "\n",
       "                     OccurredTimeTicks  PersonId  \n",
       "__null_dask_index__                               \n",
       "0                        1577836802678         2  \n",
       "1                        1578104602678         2  \n",
       "2                        1578372402678         2  \n",
       "3                        1578640202678         2  \n",
       "4                        1578908002678         2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/file.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_uhj2c4yl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): processing user uploaded csv file for datablob_id=46 and uploading parquet back to S3 for datasource_id=29\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/135/datablob/46\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/135/datablob/46\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/135/datablob/46 locally in /tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/135/datablob/46 to /tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): step 2/4: running import_csv()\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/135/datasource/29\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/135/datasource/29 locally in /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h\n",
      "[INFO] airt.data.importers: import_csv(): importing CSV file(s) from [/tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e/file.csv,..., /tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e/file.csv] using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:34051' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:33259' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:39715' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_csv(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_csv(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/135/datasource/29, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h to s3://kumaran-airt-service-eu-west-1/135/datasource/29\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1135datasource29_cached_wbjcpo5h\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1135datablob46_cached_oq_me54e\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] airt_service.data.csv: process_csv(datablob_id=46, datasource_id=29): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=29, uuid=UUID('d7bbea03-52c0-4704-b348-4016c2654734'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/135/datasource/29', created=datetime.datetime(2023, 1, 6, 9, 14, 56), user_id=135, pulled_on=datetime.datetime(2023, 1, 6, 9, 15, 5), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Model(predict_after=datetime.timedelta(days=20), id=13, timestamp_column=None, uuid=UUID('27ec9df7-07f0-4b42-85e0-94644470b3fb'), total_steps=5, path=None, client_column='AccountId', completed_steps=0, datasource_id=29, cloud_provider=<CloudProvider.aws: 'aws'>, error=None, user_id=135, target_column='DefinitionId', region='eu-west-1', target='load*', disabled=False, created=datetime.datetime(2023, 1, 6, 9, 15, 26))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 14', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 14', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(error=None, uuid=UUID('e6ed0994-ee26-4fcc-9609-7ba789e884c9'), datasource_id=29, cloud_provider=<CloudProvider.aws: 'aws'>, disabled=False, created=datetime.datetime(2023, 1, 6, 9, 15, 26), region='eu-west-1', id=14, model_id=13, total_steps=3, path=None, completed_steps=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_populate_user(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fedbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_predictions(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup predictions\"\"\"\n",
    "    logger.info(\"deleting predictions\")\n",
    "    predictions = session.exec(\n",
    "        select(Prediction).join(Model).where(Model.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for prediction in predictions:\n",
    "        delete_prediction(\n",
    "            prediction_uuid=prediction.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(prediction)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef7d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting predictions\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_predictions(user_to_cleanup, session)\n",
    "\n",
    "    predictions = session.exec(\n",
    "        select(Prediction).join(Model).where(Model.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(predictions) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a20c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_models(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup models\"\"\"\n",
    "    logger.info(\"deleting models\")\n",
    "    models = session.exec(select(Model).where(Model.user == user_to_cleanup)).all()\n",
    "\n",
    "    for model in models:\n",
    "        delete_model(\n",
    "            model_uuid=model.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(model)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ee190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting models\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_models(user_to_cleanup, session)\n",
    "\n",
    "    models = session.exec(select(Model).where(Model.user == user_to_cleanup)).all()\n",
    "    assert len(models) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8948032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_datasources(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup datasources\"\"\"\n",
    "    logger.info(\"deleting datasources\")\n",
    "    datasources = session.exec(\n",
    "        select(DataSource).where(DataSource.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for datasource in datasources:\n",
    "        delete_datasource(\n",
    "            datasource_uuid=datasource.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(datasource)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2bc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting datasources\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_datasources(user_to_cleanup, session)\n",
    "\n",
    "    datasources = session.exec(\n",
    "        select(DataSource).where(DataSource.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(datasources) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe3bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_datablobs(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup datablobs\"\"\"\n",
    "    logger.info(\"deleting datablobs\")\n",
    "    datablobs = session.exec(\n",
    "        select(DataBlob).where(DataBlob.user == user_to_cleanup)\n",
    "    ).all()\n",
    "\n",
    "    for datablob in datablobs:\n",
    "        delete_datablob(\n",
    "            datablob_uuid=datablob.uuid,  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(datablob)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fa825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting datablobs\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_datablobs(user_to_cleanup, session)\n",
    "\n",
    "    datablobs = session.exec(\n",
    "        select(DataBlob).where(DataBlob.user == user_to_cleanup)\n",
    "    ).all()\n",
    "    assert len(datablobs) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_apikeys(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup apikeys\"\"\"\n",
    "    logger.info(\"deleting apikeys\")\n",
    "    apikeys = session.exec(select(APIKey).where(APIKey.user == user_to_cleanup)).all()\n",
    "\n",
    "    for apikey in apikeys:\n",
    "        delete_apikey(\n",
    "            user_uuid_or_name=str(user_to_cleanup.uuid),  # type: ignore\n",
    "            key_uuid_or_name=str(apikey.uuid),  # type: ignore\n",
    "            user=user_to_cleanup,\n",
    "            session=session,\n",
    "        )\n",
    "        session.delete(apikey)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973eb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting apikeys\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_apikeys(user_to_cleanup, session)\n",
    "\n",
    "    apikeys = session.exec(select(APIKey).where(APIKey.user == user_to_cleanup)).all()\n",
    "    assert len(apikeys) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9123ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def cleanup_user(user_to_cleanup: User, session: Session):\n",
    "    \"\"\"Cleanup user\"\"\"\n",
    "    cleanup_predictions(user_to_cleanup, session)\n",
    "    cleanup_models(user_to_cleanup, session)\n",
    "    cleanup_datasources(user_to_cleanup, session)\n",
    "    cleanup_datablobs(user_to_cleanup, session)\n",
    "    cleanup_apikeys(user_to_cleanup, session)\n",
    "\n",
    "    bucket, base_path = get_s3_storage_bucket()  # type: ignore\n",
    "    s3_path = (\n",
    "        f\"{base_path}/{user_to_cleanup.id}\" if base_path else str(user_to_cleanup.id)\n",
    "    )\n",
    "    logger.info(f\"Deleting user files in s3://{bucket.name}/{s3_path}\")\n",
    "    bucket.objects.filter(Prefix=s3_path + \"/\").delete()\n",
    "\n",
    "    delete_topics_for_user(username=user_to_cleanup.username)\n",
    "\n",
    "    logger.info(\"deleting user\")\n",
    "    session.delete(user_to_cleanup)\n",
    "    session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5393eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: deleting predictions\n",
      "[INFO] __main__: deleting models\n",
      "[INFO] __main__: deleting datasources\n",
      "[INFO] __main__: deleting datablobs\n",
      "[INFO] __main__: deleting apikeys\n",
      "[INFO] __main__: Deleting user files in s3://kumaran-airt-service-eu-west-1/135\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_data deleted\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_realitime_data deleted\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_data_status deleted\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_training_model_status deleted\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_model_metrics deleted\n",
      "[INFO] airt_service.confluent: Topic airt_service_glmfgiaofy_prediction deleted\n",
      "[INFO] __main__: deleting user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1672996529.940|CONFWARN|rdkafka#producer-2| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1672996529.940|CONFWARN|rdkafka#producer-2| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"
     ]
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user_to_cleanup = session.exec(\n",
    "        select(User).where(User.username == test_username)\n",
    "    ).one()\n",
    "\n",
    "    cleanup_user(user_to_cleanup, session)\n",
    "\n",
    "    with pytest.raises(NoResultFound):\n",
    "        session.exec(select(User).where(User.username == test_username)).one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff990657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
