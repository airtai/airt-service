{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c6d88ccd",
   "metadata": {},
   "source": [
    "---\n",
    "description: REST API Routes to import, train, predict events data\n",
    "output-file: api_web_service.html\n",
    "title: REST Server\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a302e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049c8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "23-03-30 13:24:47.333 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import asyncio\n",
    "import yaml\n",
    "from aiokafka.helpers import create_ssl_context\n",
    "from airt.logger import get_logger\n",
    "from asyncer import asyncify\n",
    "from fastapi import FastAPI, Request, Response\n",
    "from fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html\n",
    "from fastapi.openapi.utils import get_openapi\n",
    "from fastapi.responses import FileResponse, HTMLResponse, RedirectResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastkafka import FastKafka\n",
    "from pydantic import BaseModel, EmailStr, Field, HttpUrl, NonNegativeInt, validator\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service\n",
    "from airt_service.auth import auth_router\n",
    "from airt_service.confluent import aio_kafka_config\n",
    "from airt_service.data.datablob import datablob_router\n",
    "from airt_service.data.datasource import datasource_router\n",
    "from airt_service.db.models import User, get_session_with_context\n",
    "from airt_service.model.prediction import model_prediction_router\n",
    "from airt_service.model.train import model_train_router\n",
    "from airt_service.sanitizer import sanitized_print\n",
    "from airt_service.training_status_process import (\n",
    "    TrainingStreamStatus,\n",
    "    process_training_status,\n",
    ")\n",
    "from airt_service.users import user_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb446218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "import uvicorn\n",
    "from _pytest.monkeypatch import MonkeyPatch\n",
    "from confluent_kafka import Consumer, Producer\n",
    "from fastapi.testclient import TestClient\n",
    "from fastkafka.testing import Tester\n",
    "from starlette.datastructures import Headers\n",
    "\n",
    "from airt_service.confluent import confluent_kafka_config, create_topics_for_user\n",
    "from airt_service.db.models import create_user_for_testing\n",
    "from airt_service.helpers import set_env_variable_context\n",
    "from airt_service.uvicorn_helpers import run_uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c0f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8b0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "description = \"\"\"\n",
    "# airt service to import, train and predict events data\n",
    "\n",
    "## Python client\n",
    "\n",
    "To use python library please visit: <a href=\"https://docs.airt.ai\" target=\"_blank\">https://docs.airt.ai</a>\n",
    "\n",
    "## How to use\n",
    "\n",
    "To access the airt service, you must create a developer account. Please fill out the signup form below to get one:\n",
    "\n",
    "[https://bit.ly/3hbXQLY](https://bit.ly/3hbXQLY)\n",
    "\n",
    "Upon successful verification, you will receive the username and password for the developer account to your email.\n",
    "\n",
    "### 0. Authenticate\n",
    "\n",
    "Once you receive the username and password, please authenticate the same by calling the `/token` API. The API \n",
    "will return a bearer token if the authentication is successful.\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/token' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/x-www-form-urlencoded' \\\n",
    "  -d 'grant_type=&username=<username>&password=<password>&scope=&client_id=&client_secret='\n",
    "```\n",
    "\n",
    "You can either use the above bearer token or create additional apikey's for accessing the rest of the API's. \n",
    "\n",
    "To create additional apikey's, please call the `/apikey` API by passing the bearer token along with the \n",
    "details of the new apikey in the request. e.g:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/apikey' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"name\": \"<apikey_name>\",\n",
    "  \"expiry\": \"<datetime_in_ISO_8601_format>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "### 1. Connect data\n",
    "\n",
    "Establishing the connection with the data source is a two-step process. The first step allows \n",
    "you to pull the data into airt servers and the second step allows you to perform necessary data \n",
    "pre-processing that are required model training.\n",
    "\n",
    "Currently, we support importing data from:\n",
    "\n",
    "- files stored in the AWS S3 bucket,\n",
    "- databases like MySql, ClickHouse, and \n",
    "- local CSV/Parquet files,\n",
    "\n",
    "We plan to support other databases and storage medium in the future.\n",
    "\n",
    "To pull the data from a S3 bucket, please call the `/from_s3` API\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/datablob/from_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\",\n",
    "  \"access_key\": \"<access_key>\",\n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  \"tag\": \"<tag_name>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start importing the data in the background. This may take a while to complete depending on the size of the data.\n",
    "\n",
    "You can also check the data importing progress by calling the `/datablob/<datablob_id>` API\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/datablob/<datablob_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "Once the data import is completed, you can either call `/from_csv` or `/from_parquet` API for data pre-processing. Below is an \n",
    "example to pre-process an imported CSV data.\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "'https://api.airt.ai/datablob/<datablob_id>/from_csv' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Authorization: Bearer <bearer_token>' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\n",
    "  \"deduplicate_data\": <deduplicate_data>,\n",
    "  \"index_column\": \"<index_column>\",\n",
    "  \"sort_by\": \"<sort_by>\",\n",
    "  \"blocksize\": \"<block_size>\",\n",
    "  \"kwargs\": {}\n",
    "}'\n",
    "```\n",
    "\n",
    "### 2. Train\n",
    "\n",
    "For model training, we assume the input data includes the following:\n",
    "\n",
    "- a column identifying a client client_column (person, car, business, etc.),\n",
    "- a column specifying a type of event we will try to predict target_column (buy, checkout, click on form submit, etc.), and\n",
    "- a timestamp column specifying the time of an occurred event.\n",
    "\n",
    "The input data can have additional features of any type and will be used to make predictions more accurate. Finally, we need to \n",
    "know how much ahead we wish to make predictions. Please use the parameter predict_after to specify the period based on your needs.\n",
    "\n",
    "In the following example, we will train a model to predict which users will perform a purchase event (*purchase) 3 hours before they acctually do it:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/train' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>,\n",
    "  \"client_column\": \"<client_column>\",\n",
    "  \"target_column\": \"<target_column>\",\n",
    "  \"target\": \"*checkout\",\n",
    "  \"predict_after\": 10800\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start the model training in the background. This may take a while to complete and you can check the \n",
    "training progress by calling the `/model/<model_id>` API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "After training is complete, you can check the quality of the model by calling the `/model/<model_id>/evaluate` API. This API \n",
    "will return model validation metrics like model accuracy, precision and recall.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>/evaluate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "### 3. Predict\n",
    "\n",
    "Finally, you can run the predictions by calling the /model/<model_id>/predict API:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/<model_id>/predict' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>\n",
    "}'\n",
    "```\n",
    "Calling the above API will start running the model prediction in the background. This may take a while to complete and you can check the training progress by calling the /prediction/<prediction_id> API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "If the dataset is small, then you can call `/prediction/<prediction_id>/pandas` to get prediction results as a pandas dataframe convertible json format:\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/pandas' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "In many cases, it's much better to push the prediction results to remote destinations. Currently, we support pushing the prediction results to a AWS S3 bucket, MySql database and download to the local machine.\n",
    "\n",
    "To push the predictions to a S3 bucket, please call the `/prediction/<prediction_id>/to_s3` API\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/to_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\", \n",
    "  \"access_key\": \"<access_key>\", \n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  }'\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc5554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ModelType(str, Enum):\n",
    "    churn = \"churn\"\n",
    "    propensity_to_buy = \"propensity_to_buy\"\n",
    "\n",
    "\n",
    "class ModelTrainingRequest(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ..., description=\"Model type, only 'churn' is supported right now\"\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"approximate total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "\n",
    "class EventData(BaseModel):\n",
    "    \"\"\"\n",
    "    A sequence of events for a fixed account_id\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    DefinitionId: str = Field(\n",
    "        ...,\n",
    "        example=\"appLaunch\",\n",
    "        description=\"name of the event\",\n",
    "        min_length=1,\n",
    "    )\n",
    "    OccurredTime: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"local time of the event\",\n",
    "    )\n",
    "    OccurredTimeTicks: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1616891648496,\n",
    "        description=\"local time of the event as the number of ticks\",\n",
    "    )\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RealtimeData(EventData):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TrainingDataStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=12_345,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "class TrainingModelStart(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "    no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"number of records (rows) in the DB used for training\",\n",
    "    )\n",
    "\n",
    "class TrainingModelStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    current_step: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=0,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    current_step_percentage: float = Field(\n",
    "        ...,\n",
    "        example=0.21,\n",
    "        description=\"the percentage of the current step completed\",\n",
    "    )\n",
    "    total_no_of_steps: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of steps for training the model\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ModelMetrics(BaseModel):\n",
    "    \"\"\"The standard metrics for classification models.\n",
    "\n",
    "    The most important metrics is AUC for unbalanced classes such as churn. Metrics such as\n",
    "    accuracy are not very useful since they are easily maximized by outputting the most common\n",
    "    class all the time.\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    timestamp: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time when the model was trained\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "\n",
    "    auc: float = Field(\n",
    "        ..., example=0.91, description=\"Area under ROC curve\", ge=0.0, le=1.0\n",
    "    )\n",
    "    f1: float = Field(..., example=0.89, description=\"F-1 score\", ge=0.0, le=1.0)\n",
    "    precission: float = Field(\n",
    "        ..., example=0.84, description=\"precission\", ge=0.0, le=1.0\n",
    "    )\n",
    "    recall: float = Field(..., example=0.82, description=\"recall\", ge=0.0, le=1.0)\n",
    "    accuracy: float = Field(..., example=0.82, description=\"accuracy\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "    prediction_time: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time of prediction\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "    score: float = Field(\n",
    "        ...,\n",
    "        example=0.4321,\n",
    "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0438235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_total_no_of_records = 1000000\n",
    "_no_of_records_received = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077befcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _construct_kafka_brokers() -> Dict[str, Dict[str, Any]]:\n",
    "    url, port = aio_kafka_config[\"bootstrap_servers\"].split(\":\")\n",
    "\n",
    "    kafka_brokers = {\n",
    "        \"staging\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Staging Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "        \"production\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Production Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if (url != kafka_brokers[\"staging\"][\"url\"]) and (\n",
    "        url != kafka_brokers[\"production\"][\"url\"]\n",
    "    ):\n",
    "        kafka_brokers[\"dev\"] = {\n",
    "            \"url\": url,\n",
    "            \"description\": \"Development Kafka broker\",\n",
    "            \"port\": port,\n",
    "        }\n",
    "    \n",
    "    return kafka_brokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b827f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'staging': {'url': 'pkc-1wvvj.westeurope.azure.confluent.cloud',\n",
       "  'description': 'Staging Kafka broker',\n",
       "  'port': 9092,\n",
       "  'protocol': 'kafka-secure',\n",
       "  'security': {'type': 'plain'}},\n",
       " 'production': {'url': 'pkc-1wvvj.westeurope.azure.confluent.cloud',\n",
       "  'description': 'Production Kafka broker',\n",
       "  'port': 9092,\n",
       "  'protocol': 'kafka-secure',\n",
       "  'security': {'type': 'plain'}},\n",
       " 'dev': {'url': 'davor-redpanda',\n",
       "  'description': 'Development Kafka broker',\n",
       "  'port': '9092'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_construct_kafka_brokers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0905864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_fastkafka_application(\n",
    "    start_process_for_username: Optional[str] = \"infobip\",\n",
    "    *,\n",
    "    sleep_min: int = 5,\n",
    "    sleep_max: int = 20,\n",
    ") -> FastKafka:\n",
    "    \"\"\"Create a FastKafka service\n",
    "\n",
    "    Args:\n",
    "        start_process_for_username: prefix for topics used\n",
    "\n",
    "    Returns:\n",
    "        A FastKafka application\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_brokers = _construct_kafka_brokers()\n",
    "\n",
    "    # global description\n",
    "    version = airt_service.__version__\n",
    "    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n",
    "\n",
    "    fastkafka_app = FastKafka(\n",
    "        title=\"airt service kafka api\",\n",
    "        description=\"kafka api for airt service\",\n",
    "        kafka_brokers=kafka_brokers,\n",
    "        version=version,\n",
    "        contact=contact,\n",
    "        group_id=\"airt-service-kafka-group\",\n",
    "        auto_offset_reset=\"earliest\",\n",
    "    )\n",
    "\n",
    "    @fastkafka_app.consumes(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_start_training_data\"\n",
    "    )\n",
    "    async def on_infobip_start_training_data(msg: ModelTrainingRequest):\n",
    "        logger.info(f\"start training msg={msg}\")\n",
    "        with get_session_with_context() as session:\n",
    "            user = session.exec(\n",
    "                select(User).where(User.username == start_process_for_username)\n",
    "            ).one()\n",
    "            start_event = TrainingStreamStatus(\n",
    "                event=\"start\",\n",
    "                account_id=msg.AccountId,\n",
    "                application_id=msg.ApplicationId,\n",
    "                model_id=msg.ModelId,\n",
    "                model_type=msg.model_type,\n",
    "                count=0,\n",
    "                total=msg.total_no_of_records,\n",
    "                user=user,\n",
    "            )\n",
    "            session.add(start_event)\n",
    "            session.commit()\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_training_data\")  # type: ignore\n",
    "    async def on_infobip_training_data(msg: EventData):\n",
    "        pass\n",
    "        # ToDo: this is not showing up in logs\n",
    "\n",
    "    #         logger.debug(f\"msg={msg}\")\n",
    "\n",
    "    #         global _total_no_of_records\n",
    "    #         global _no_of_records_received\n",
    "    #         _no_of_records_received = _no_of_records_received + 1\n",
    "\n",
    "    #         if _no_of_records_received % 100 == 0:\n",
    "    #             training_data_status = TrainingDataStatus(\n",
    "    #                 AccountId=msg.AccountId,\n",
    "    #                 no_of_records=_no_of_records_received,\n",
    "    #                 total_no_of_records=_total_no_of_records,\n",
    "    #             )\n",
    "    #             await to_infobip_training_data_status(msg=training_data_status)\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_realtime_data\")  # type: ignore\n",
    "    async def on_infobip_realtime_data(msg: RealtimeData):\n",
    "        pass\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_data_status\"\n",
    "    )\n",
    "    async def to_infobip_training_data_status(\n",
    "        account_id: int,\n",
    "        *,\n",
    "        application_id: Optional[str] = None,\n",
    "        model_id: str,\n",
    "        no_of_records: int,\n",
    "        total_no_of_records: int,\n",
    "    ) -> TrainingDataStatus:\n",
    "        logger.debug(\n",
    "            f\"on_infobip_training_data_status({account_id=}, {no_of_records=}, {total_no_of_records=})\"\n",
    "        )\n",
    "        msg = TrainingDataStatus(\n",
    "            AccountId=account_id,\n",
    "            ApplicationId=application_id,\n",
    "            ModelId=model_id,\n",
    "            no_of_records=no_of_records,\n",
    "            total_no_of_records=total_no_of_records,\n",
    "        )\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_start_training\"\n",
    "    )\n",
    "    async def to_infobip_start_training(\n",
    "        account_id: int,\n",
    "        *,\n",
    "        application_id: Optional[str] = None,\n",
    "        model_id: str,\n",
    "        no_of_records: int,\n",
    "    ) -> TrainingModelStart:\n",
    "        msg = TrainingModelStart(\n",
    "            AccountId=account_id,\n",
    "            ApplicationId=application_id,\n",
    "            ModelId=model_id,\n",
    "            no_of_records=no_of_records,\n",
    "        )\n",
    "        print(f\"to_infobip_start_training({msg})\")\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_start_training\")  # type: ignore\n",
    "    async def on_infobip_start_training(msg: TrainingModelStart):\n",
    "        total_no_of_steps = 5\n",
    "        for i in range(total_no_of_steps + 1):\n",
    "            for j in range(3):\n",
    "                await to_infobip_training_model_status(\n",
    "                    TrainingModelStatus(\n",
    "                        AccountId=msg.AccountId,\n",
    "                        ApplicationId=msg.ApplicationId,\n",
    "                        ModelId=msg.ModelId,\n",
    "                        current_step=i,\n",
    "                        current_step_percentage=0.5 * j\n",
    "                        if j != 1\n",
    "                        else round(np.random.uniform(), ndigits=3),\n",
    "                        total_no_of_steps=total_no_of_steps,\n",
    "                    )\n",
    "                )\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "            await to_infobip_model_metrics(\n",
    "                ModelMetrics(\n",
    "                    AccountId=msg.AccountId,\n",
    "                    ApplicationId=msg.ApplicationId,\n",
    "                    ModelId=msg.ModelId,\n",
    "                    timestamp=datetime.now(),\n",
    "                    model_type=\"churn\",\n",
    "                    auc=0.946,\n",
    "                    f1=0.934,\n",
    "                    precission=0.976,\n",
    "                    recall=0.987,\n",
    "                    accuracy=0.992,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # todo: make predictions\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_model_status\"\n",
    "    )\n",
    "    async def to_infobip_training_model_status(\n",
    "        msg: TrainingModelStatus,\n",
    "    ) -> TrainingModelStatus:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_model_metrics\")  # type: ignore\n",
    "    async def to_infobip_model_metrics(msg: ModelMetrics) -> ModelMetrics:\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_model_metrics\")  # type: ignore\n",
    "    async def on_infobip_model_metrics(msg: ModelMetrics):\n",
    "        return msg\n",
    "    \n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_prediction\")  # type: ignore\n",
    "    async def to_infobip_prediction(msg: Prediction) -> Prediction:\n",
    "        return msg\n",
    "\n",
    "    # todo: move to fastkafka lib\n",
    "    fastkafka_app.to_infobip_training_data_status = to_infobip_training_data_status\n",
    "    fastkafka_app.to_infobip_start_training = to_infobip_start_training\n",
    "    fastkafka_app.to_infobip_training_model_status = to_infobip_training_model_status\n",
    "\n",
    "    if start_process_for_username is not None:\n",
    "\n",
    "        @fastkafka_app.run_in_background()  # type: ignore\n",
    "        async def startup_event(fastkafka_app: FastKafka = fastkafka_app) -> None:\n",
    "            await process_training_status(\n",
    "                username=start_process_for_username,  # type: ignore\n",
    "                fast_kafka_api_app=fastkafka_app,\n",
    "                sleep_min=sleep_min,\n",
    "                sleep_max=sleep_max,\n",
    "            )\n",
    "\n",
    "    return fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398bf2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'infobip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_user_for_testing(username=\"infobip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b1d9324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curr_count</th>\n",
       "      <th>curr_check_on</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AccountId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12345</th>\n",
       "      <td>10</td>\n",
       "      <td>2023-03-30 13:24:48.578408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           curr_count              curr_check_on\n",
       "AccountId                                       \n",
       "12345              10 2023-03-30 13:24:48.578408"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@contextlib.contextmanager\n",
    "def monkeypatch_clickhouse(account_id: int, curr_count: int, curr_check_on: Optional[datetime]=None) -> None:\n",
    "    with MonkeyPatch.context() as monkeypatch:\n",
    "        monkeypatch.setattr(\n",
    "            \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "            lambda account_ids: pd.DataFrame(\n",
    "                {\n",
    "                    \"curr_count\": [curr_count],\n",
    "                    \"AccountId\": [account_id],\n",
    "                    \"curr_check_on\": [datetime.utcnow()],\n",
    "                }\n",
    "            ).set_index(\"AccountId\"),\n",
    "        )\n",
    "        yield\n",
    " \n",
    "with monkeypatch_clickhouse(account_id=12345, curr_count=10):\n",
    "    df = airt_service.training_status_process.get_count_from_training_data_ch_table([12345])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b51074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:24:48.593 [INFO] fastkafka._application.app: run_in_background() : Adding function 'startup_event' as background task\n",
      "23-03-30 13:24:48.595 [INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "23-03-30 13:24:48.596 [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "23-03-30 13:24:48.596 [INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "23-03-30 13:24:48.597 [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...\n",
      "23-03-30 13:24:48.597 [INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "23-03-30 13:24:49.368 [INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "23-03-30 13:24:51.292 [INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "23-03-30 13:24:53.268 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.284 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.291 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.298 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.304 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.310 [INFO] fastkafka._application.app: _populate_bg_tasks() : Starting background task 'startup_event'\n",
      "23-03-30 13:24:53.332 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.507 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.507 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.508 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.509 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.510 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.510 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.511 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.512 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.513 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.513 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.533 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.535 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data'})\n",
      "23-03-30 13:24:53.535 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data'}\n",
      "23-03-30 13:24:53.536 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.537 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.538 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_realtime_data'})\n",
      "23-03-30 13:24:53.538 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_realtime_data'}\n",
      "23-03-30 13:24:53.539 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.541 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.542 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training_data'})\n",
      "23-03-30 13:24:53.543 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training_data'}\n",
      "23-03-30 13:24:53.543 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.547 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.548 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_model_metrics'})\n",
      "23-03-30 13:24:53.549 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_model_metrics'}\n",
      "23-03-30 13:24:53.550 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.552 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.553 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.554 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training'})\n",
      "23-03-30 13:24:53.554 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training'}\n",
      "23-03-30 13:24:53.555 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.562 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.565 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.566 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.566 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.581 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.585 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "to_infobip_start_training(AccountId=12356 ApplicationId=None ModelId='drivers' no_of_records=0)\n",
      "23-03-30 13:24:53.624 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.631 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-30 13:24:53.640 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.641 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.642 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.643 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:24:53.644 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.645 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.647 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.648 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.649 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:24:53.649 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:24:53.660 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.661 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_model_status'})\n",
      "23-03-30 13:24:53.662 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_model_status'}\n",
      "23-03-30 13:24:53.662 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.664 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.665 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data_status'})\n",
      "23-03-30 13:24:53.665 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data_status'}\n",
      "23-03-30 13:24:53.666 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.668 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.668 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training'})\n",
      "23-03-30 13:24:53.669 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training'}\n",
      "23-03-30 13:24:53.670 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.672 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.672 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_model_metrics'})\n",
      "23-03-30 13:24:53.673 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_model_metrics'}\n",
      "23-03-30 13:24:53.674 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.675 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:24:53.675 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_prediction'})\n",
      "23-03-30 13:24:53.676 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_prediction'}\n",
      "23-03-30 13:24:53.677 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:24:53.683 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.683 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.684 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.687 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_model_status': 1}. \n",
      "23-03-30 13:24:53.688 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.688 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_data_status': 1}. \n",
      "23-03-30 13:24:53.690 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_start_training': 1}. \n",
      "23-03-30 13:24:53.690 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_model_metrics': 1}. \n",
      "23-03-30 13:24:53.691 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_prediction': 1}. \n",
      "23-03-30 13:24:53.692 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.795 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.796 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.798 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.799 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.799 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.902 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.903 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.904 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.905 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:53.907 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.010 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.011 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.012 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.013 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.015 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.118 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.119 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.120 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.122 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.123 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.226 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.227 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.229 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.230 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:24:54.231 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:24:54.341 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.342 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.343 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:24:54.345 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.346 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.346 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:24:54.347 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.348 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.349 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:24:54.349 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.350 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.351 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:24:54.352 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.353 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.353 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:24:54.372 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-90658d2a-d13f-4363-bd9c-fc8cb2cf146a\n",
      "23-03-30 13:24:54.373 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-bb686a23-0ab5-4753-8c46-a9504ec61785\n",
      "23-03-30 13:24:54.375 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-f3c3c85b-91d1-4af1-9739-20f1f418edfd\n",
      "23-03-30 13:24:54.376 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-99570cce-945e-4624-9428-9b658102701e\n",
      "23-03-30 13:24:54.377 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-748046dc-d064-43cc-b9f8-98cf3c57ab23\n",
      "23-03-30 13:24:54.377 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-03-30 13:24:54.380 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_model_metrics': 1, 'infobip_training_data': 1, 'infobip_start_training_data': 1, 'infobip_start_training': 1, 'infobip_realtime_data': 1}. \n",
      "23-03-30 13:24:54.397 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:24:54.397 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.399 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:24:54.400 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_realtime_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.401 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:24:54.402 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_model_metrics', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.402 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:24:54.403 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:24:54.404 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:24:54.405 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:24:56.649 [INFO] __main__: start training msg=AccountId=12356 ApplicationId=None ModelId='drivers' model_type=<ModelType.churn: 'churn'> total_no_of_records=10000\n",
      "tester.mocks.on_infobip_training_data_status.await_args_list=[call(TrainingDataStatus(AccountId=12356, ApplicationId=None, ModelId='drivers', no_of_records=0, total_no_of_records=10000)),\n",
      " call(TrainingDataStatus(AccountId=12356, ApplicationId=None, ModelId='drivers', no_of_records=2000, total_no_of_records=10000))]\n",
      "to_infobip_start_training(AccountId=12356 ApplicationId=None ModelId='drivers' no_of_records=10000)\n",
      "23-03-30 13:25:05.722 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.723 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.724 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.725 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.725 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.726 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.727 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.728 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.729 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.729 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.732 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Cancelling background task 'startup_event'\n",
      "23-03-30 13:25:05.733 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Waiting for background task 'startup_event' to finish\n",
      "23-03-30 13:25:05.734 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Execution finished for background task 'startup_event'\n",
      "23-03-30 13:25:05.779 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:05.781 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.781 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.827 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:05.828 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:05.829 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:05.830 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.831 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:05.832 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.833 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:05.833 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:05.834 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:06.411 [WARNING] aiokafka.consumer.group_coordinator: Heartbeat failed for group airt-service-kafka-group because it is rebalancing\n",
      "23-03-30 13:25:06.414 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions frozenset({TopicPartition(topic='infobip_start_training', partition=0)}) for group airt-service-kafka-group\n",
      "23-03-30 13:25:06.415 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:06.419 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 2) with member_id aiokafka-0.8.0-748046dc-d064-43cc-b9f8-98cf3c57ab23\n",
      "23-03-30 13:25:06.420 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-03-30 13:25:06.423 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 2\n",
      "23-03-30 13:25:06.424 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:06.529 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:06.530 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:06.531 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:06.533 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 263564...\n",
      "23-03-30 13:25:08.288 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 263564 terminated.\n",
      "23-03-30 13:25:08.289 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 263182...\n",
      "23-03-30 13:25:09.622 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 263182 terminated.\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "fastkafka_app = create_fastkafka_application(sleep_min=1, sleep_max=1)\n",
    "fastkafka_app\n",
    "\n",
    "i = 11\n",
    "\n",
    "with monkeypatch_clickhouse(account_id=12345 + i, curr_count=0):\n",
    "    #     !mysql -u root -pSuperSecretPassword -h davor-mysql -e 'select * from airt_service.trainingstreamstatus order by id desc limit 5'\n",
    "\n",
    "    async with Tester(fastkafka_app).using_local_kafka() as tester:\n",
    "        training_req = ModelTrainingRequest(\n",
    "            AccountId=12345 + i,\n",
    "            ModelId=\"drivers\",\n",
    "            model_type=\"churn\",\n",
    "            total_no_of_records=10_000,\n",
    "        )\n",
    "        await tester.to_infobip_start_training_data(training_req)\n",
    "\n",
    "        #         !mysql -u root -pSuperSecretPassword -h davor-mysql -e 'select * from airt_service.trainingstreamstatus order by id desc limit 5'\n",
    "\n",
    "        # send 10 messages\n",
    "        for _ in range(10):\n",
    "            training_data = EventData(\n",
    "                AccountId=12345 + i,\n",
    "                ModelId=\"drivers\",\n",
    "                DefinitionId=\"event_name\",\n",
    "                PersonId=12,\n",
    "                OccurredTime=\"2023-03-28T13:41:22.628141\",\n",
    "                OccurredTimeTicks=1680010882628,\n",
    "            )\n",
    "            await tester.to_infobip_training_data(training_data)\n",
    "\n",
    "        #         !mysql -u root -pSuperSecretPassword -h davor-mysql -e 'select * from airt_service.trainingstreamstatus order by id desc limit 5'\n",
    "\n",
    "        # patch clickhouse fetch function to show 2_000 records were in the database\n",
    "        with monkeypatch_clickhouse(account_id=12345 + i, curr_count=2_000):\n",
    "            await tester.awaited_mocks.on_infobip_training_data_status.assert_called_with(\n",
    "                TrainingDataStatus(\n",
    "                    AccountId=12345 + i,\n",
    "                    ApplicationId=None,\n",
    "                    ModelId=\"drivers\",\n",
    "                    no_of_records=2000,\n",
    "                    total_no_of_records=10000,\n",
    "                ),\n",
    "                timeout=5,\n",
    "            )\n",
    "            print(f\"{tester.mocks.on_infobip_training_data_status.await_args_list=}\")\n",
    "\n",
    "        #         # patch clickhouse fetch function to show 10_000 records were in the database\n",
    "        with monkeypatch_clickhouse(account_id=12345 + i, curr_count=10_000):\n",
    "            await tester.awaited_mocks.on_infobip_training_data_status.assert_called_with(\n",
    "                TrainingDataStatus(\n",
    "                    AccountId=12345 + i,\n",
    "                    ApplicationId=None,\n",
    "                    ModelId=\"drivers\",\n",
    "                    no_of_records=10_000,\n",
    "                    total_no_of_records=10_000,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            await tester.awaited_mocks.on_infobip_start_training.assert_called_with(\n",
    "                TrainingModelStart(\n",
    "                    AccountId=12345 + i,\n",
    "                    ApplicationId=None,\n",
    "                    ModelId=\"drivers\",\n",
    "                    no_of_records=10_000,\n",
    "                ),\n",
    "                timeout=2,\n",
    "            )\n",
    "\n",
    "            for k in range(6):\n",
    "                await tester.awaited_mocks.on_infobip_training_model_status.assert_called_with(\n",
    "                    TrainingModelStatus(\n",
    "                        AccountId=12345 + i,\n",
    "                        ApplicationId=None,\n",
    "                        ModelId=\"drivers\",\n",
    "                        current_step=k,\n",
    "                        current_step_percentage=1.0,\n",
    "                        total_no_of_steps=5,\n",
    "                    ),\n",
    "                    timeout=10,\n",
    "                )\n",
    "                \n",
    "            await tester.awaited_mocks.on_infobip_model_metrics.assert_called(timeout=5)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb85d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "243dff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_ws_server(\n",
    "    assets_path: Path = Path(\"./assets\"),\n",
    "    start_process_for_username: Optional[str] = \"infobip\",\n",
    ") -> Tuple[FastAPI, FastKafka]:\n",
    "    \"\"\"Create a FastKafka based web service\n",
    "\n",
    "    Args:\n",
    "        assets_path: Path to assets (should include favicon.ico)\n",
    "\n",
    "    Returns:\n",
    "        A FastKafka server\n",
    "    \"\"\"\n",
    "    global description\n",
    "    title = \"airt service\"\n",
    "    version = airt_service.__version__\n",
    "    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n",
    "    openapi_url = \"/openapi.json\"\n",
    "    favicon_url = \"/assets/images/favicon.ico\"\n",
    "    assets_path = assets_path.resolve()\n",
    "    favicon_path = assets_path / \"images/favicon.ico\"\n",
    "\n",
    "    app = FastAPI(\n",
    "        title=title,\n",
    "        description=description,\n",
    "        version=version,\n",
    "        docs_url=None,\n",
    "        redoc_url=None,\n",
    "    )\n",
    "    app.mount(\"/assets\", StaticFiles(directory=assets_path), name=\"assets\")\n",
    "\n",
    "    asyncapi_path = Path(\"./asyncapi/docs\").resolve()\n",
    "\n",
    "    if asyncapi_path.exists():\n",
    "        app.mount(\n",
    "            \"/asyncapi\",\n",
    "            StaticFiles(directory=asyncapi_path, html=True),\n",
    "            name=\"asyncapi\",\n",
    "        )\n",
    "\n",
    "    # attaches /token to routes\n",
    "    app.include_router(auth_router)\n",
    "\n",
    "    # attaches /datablob/* to routes\n",
    "    app.include_router(datablob_router)\n",
    "\n",
    "    # attaches /datasource/* to routes\n",
    "    app.include_router(datasource_router)\n",
    "\n",
    "    # attaches /model/* to routes\n",
    "    app.include_router(model_train_router)\n",
    "\n",
    "    # attaches /prediction/* to routes\n",
    "    app.include_router(model_prediction_router)\n",
    "\n",
    "    # attaches /user/* to routes\n",
    "    app.include_router(user_router)\n",
    "\n",
    "    @app.middleware(\"http\")\n",
    "    async def add_nosniff_x_content_type_options_header(\n",
    "        request: Request, call_next: Callable[[Request], Response]\n",
    "    ) -> Response:\n",
    "        response: Response = await call_next(request)  # type: ignore\n",
    "        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n",
    "        response.headers[\"Strict-Transport-Security\"] = \"max-age=31536000\"\n",
    "        return response\n",
    "\n",
    "    @app.get(\"/version\")\n",
    "    def get_versions() -> Dict[str, str]:\n",
    "        return {\"airt_service\": airt_service.__version__}\n",
    "\n",
    "    @app.get(\"/\", include_in_schema=False)\n",
    "    def redirect_root() -> RedirectResponse:\n",
    "        return RedirectResponse(\"/docs\")\n",
    "\n",
    "    @app.get(\"/docs\", include_in_schema=False)\n",
    "    def overridden_swagger() -> HTMLResponse:\n",
    "        return get_swagger_ui_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            swagger_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/redoc\", include_in_schema=False)\n",
    "    def overridden_redoc() -> HTMLResponse:\n",
    "        return get_redoc_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            redoc_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/favicon.ico\", include_in_schema=False)\n",
    "    async def serve_favicon() -> FileResponse:\n",
    "        return FileResponse(favicon_path)\n",
    "\n",
    "    def custom_openapi() -> Dict[str, Any]:\n",
    "        if app.openapi_schema:\n",
    "            return app.openapi_schema\n",
    "\n",
    "        fastapi_schema = get_openapi(\n",
    "            title=title,\n",
    "            description=description,\n",
    "            version=version,\n",
    "            routes=app.routes,\n",
    "        )\n",
    "\n",
    "        # ToDo: Figure out recursive dict merge\n",
    "        fastapi_schema[\"servers\"] = [\n",
    "            {\n",
    "                \"url\": \"http://0.0.0.0:6006\"\n",
    "                if (\n",
    "                    environ[\"DOMAIN\"] == \"localhost\"\n",
    "                    or \"airt-service\" in environ[\"DOMAIN\"]\n",
    "                )\n",
    "                else f\"https://{environ['DOMAIN']}\",\n",
    "                \"description\": \"Server\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        app.openapi_schema = fastapi_schema\n",
    "        return app.openapi_schema\n",
    "\n",
    "    app.openapi = custom_openapi  # type: ignore\n",
    "\n",
    "    #     logger.info(f\"kafka_config={aio_kafka_config}\")\n",
    "\n",
    "#     kafka_brokers = _construct_kafka_brokers()\n",
    "    \n",
    "#     exclude_keys = ['bootstrap_servers']\n",
    "#     kafka_config = {k: aio_kafka_config[k] for k in set(list(aio_kafka_config.keys())) - set(exclude_keys)}\n",
    "\n",
    "    fastkafka_app = create_fastkafka_application(start_process_for_username=start_process_for_username)\n",
    "\n",
    "    return app, fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16d89806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fastapi_app(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    ") -> Tuple[FastAPI, FastKafka]:\n",
    "    assets_path = assets_path.resolve()\n",
    "    app, fastkafka_app = create_ws_server(assets_path=assets_path)\n",
    "    return app, fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "804db4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:09.803 [INFO] fastkafka._application.app: run_in_background() : Adding function 'startup_event' as background task\n"
     ]
    }
   ],
   "source": [
    "app, fastkafka_app = create_fastapi_app()\n",
    "client = TestClient(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52b855f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_username = \"johndoe\"\n",
    "# oauth_data = dict(\n",
    "#     username=test_username, password=environ[\"AIRT_SERVICE_SUPER_USER_PASSWORD\"]\n",
    "# )\n",
    "\n",
    "# response = client.post(\"/token\", data=oauth_data)\n",
    "# actual = response.json()\n",
    "# display(actual)\n",
    "# assert \"access_token\" in actual\n",
    "# assert actual[\"token_type\"] == \"bearer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71fa1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "async def test_function():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        while True:\n",
    "            try:\n",
    "                await client.get(\"http://0.0.0.0:6006/docs\")\n",
    "                sanitized_print(\"docs retrieved\")\n",
    "            except httpx.ConnectError:\n",
    "                sanitized_print(\"-\", end=\"\")\n",
    "            except httpx.TimeoutException:\n",
    "                sanitized_print(\".\", end=\"\")\n",
    "            except Exception as e:\n",
    "                sanitized_print(\"?\", end=\"\")\n",
    "                sanitized_print(e)\n",
    "                raise e\n",
    "            try:\n",
    "                await asyncio.sleep(1)\n",
    "            except asyncio.CancelledError:\n",
    "                sanitized_print(\"\\n*** task canceled ***\")\n",
    "                return \"ok\"\n",
    "\n",
    "\n",
    "# task = asyncio.create_task(test_function())\n",
    "# await asyncio.sleep(3)\n",
    "# task.cancel()\n",
    "# await asyncio.wait_for(task, timeout=2)\n",
    "# task.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "992723d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AccountId': 1000,\n",
       " 'ApplicationId': 'DriverApp',\n",
       " 'ModelId': None,\n",
       " 'DefinitionId': 'sign_in',\n",
       " 'OccurredTimeTicks': 1649146037462,\n",
       " 'OccurredTime': '2022-04-05T08:07:17.462000',\n",
       " 'PersonId': 4}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions = [\n",
    "    \"appLaunch\",\n",
    "    \"sign_in\",\n",
    "    \"sign_out\",\n",
    "    \"add_to_cart\",\n",
    "    \"purchase\",\n",
    "    \"custom_event_1\",\n",
    "    \"custom_event_2\",\n",
    "    \"custom_event_3\",\n",
    "]\n",
    "\n",
    "\n",
    "# applications = [\"DriverApp\", \"PUBG\", \"COD\"]\n",
    "applications = [\"DriverApp\"]\n",
    "\n",
    "\n",
    "def generate_n_rows_for_training_data(n: int, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    #     account_id = rng.choice([4000, 5000, 500], size=n)\n",
    "    account_id = 1000\n",
    "    definition_id = rng.choice(definitions, size=n)\n",
    "    application_id = rng.choice(applications, size=n)\n",
    "    model_id = rng.choice([\"ChurnModelForDrivers\", None], size=n)\n",
    "    occurred_time_ticks = rng.integers(\n",
    "        datetime(year=2022, month=1, day=1).timestamp() * 1000,\n",
    "        datetime(year=2022, month=11, day=1).timestamp() * 1000,\n",
    "        size=n,\n",
    "    )\n",
    "    occurred_time = pd.to_datetime(occurred_time_ticks, unit=\"ms\").strftime(\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "    )\n",
    "    person_id = rng.integers(n // 10, size=n)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"AccountId\": account_id,\n",
    "            \"ApplicationId\": application_id,\n",
    "            \"ModelId\": model_id,\n",
    "            \"DefinitionId\": definition_id,\n",
    "            \"OccurredTimeTicks\": occurred_time_ticks,\n",
    "            \"OccurredTime\": occurred_time,\n",
    "            \"PersonId\": person_id,\n",
    "        }\n",
    "    )\n",
    "    return json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "\n",
    "generate_n_rows_for_training_data(100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8eebe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:10.190 [INFO] fastkafka._application.app: run_in_background() : Adding function 'startup_event' as background task\n",
      "23-03-30 13:25:10.193 [INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "23-03-30 13:25:10.194 [INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "23-03-30 13:25:10.195 [INFO] fastkafka._testing.local_broker: Starting zookeeper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1680182710.052|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1680182710.052|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:10.960 [INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "stdout=, stderr=, returncode=1\n",
      "23-03-30 13:25:13.156 [INFO] fastkafka._testing.local_broker: kafka startup falied, generating a new port and retrying...\n",
      "23-03-30 13:25:13.157 [INFO] fastkafka._testing.local_broker: port=40213\n",
      "23-03-30 13:25:15.118 [INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:40213\n",
      "23-03-30 13:25:17.040 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.056 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.063 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.069 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.075 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.082 [INFO] fastkafka._application.app: _populate_bg_tasks() : Starting background task 'startup_event'\n",
      "23-03-30 13:25:17.103 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.103 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.104 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:40213', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.105 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.105 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:40213', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.106 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.107 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:40213', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.107 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.108 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:40213', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.109 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.109 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:40213', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.123 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.125 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.126 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_model_metrics'})\n",
      "23-03-30 13:25:17.126 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_model_metrics'}\n",
      "23-03-30 13:25:17.127 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.130 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.131 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_realtime_data'})\n",
      "23-03-30 13:25:17.132 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_realtime_data'}\n",
      "23-03-30 13:25:17.132 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.133 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.134 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data'})\n",
      "23-03-30 13:25:17.134 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data'}\n",
      "23-03-30 13:25:17.135 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.135 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.136 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training'})\n",
      "23-03-30 13:25:17.136 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training'}\n",
      "23-03-30 13:25:17.137 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.138 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.140 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training_data'})\n",
      "23-03-30 13:25:17.140 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training_data'}\n",
      "23-03-30 13:25:17.141 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.152 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.153 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.154 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.155 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.156 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.163 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.169 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "to_infobip_start_training(AccountId=1000 ApplicationId='DriverApp' ModelId='ChurnModelForDrivers' no_of_records=999)\n",
      "23-03-30 13:25:17.193 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:40213'}'\n",
      "23-03-30 13:25:17.205 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.205 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:40213', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.206 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.207 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:40213', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.209 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.211 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:40213', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:17.213 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.213 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:40213', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.215 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-30 13:25:17.216 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:40213', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-30 13:25:17.230 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.230 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_model_status'})\n",
      "23-03-30 13:25:17.231 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_model_status'}\n",
      "23-03-30 13:25:17.232 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.233 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.233 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data_status'})\n",
      "23-03-30 13:25:17.234 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data_status'}\n",
      "23-03-30 13:25:17.235 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.235 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.236 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_model_metrics'})\n",
      "23-03-30 13:25:17.237 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_model_metrics'}\n",
      "23-03-30 13:25:17.238 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.240 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.241 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_prediction'})\n",
      "23-03-30 13:25:17.242 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_prediction'}\n",
      "23-03-30 13:25:17.242 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.243 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-30 13:25:17.244 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training'})\n",
      "23-03-30 13:25:17.244 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training'}\n",
      "23-03-30 13:25:17.245 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-30 13:25:17.251 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_model_status': 1}. \n",
      "23-03-30 13:25:17.253 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_data_status': 1}. \n",
      "23-03-30 13:25:17.253 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_model_metrics': 1}. \n",
      "23-03-30 13:25:17.254 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_prediction': 1}. \n",
      "23-03-30 13:25:17.255 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_start_training': 1}. \n",
      "23-03-30 13:25:17.259 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.259 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.260 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.262 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.280 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.363 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.364 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.365 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.367 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.385 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.468 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.469 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.470 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.472 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.489 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.573 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.575 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.576 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.578 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.594 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.681 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.682 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.683 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.684 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.698 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.788 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.789 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.790 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.791 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.803 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.894 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:17.895 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.896 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.897 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:17.908 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-30 13:25:18.007 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.008 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.009 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:18.010 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.011 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.011 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:18.013 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.013 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.014 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:18.015 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.015 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.016 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:18.019 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.020 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.021 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:18.038 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-5c665ec5-44d7-4938-ad58-0fad92c6bd48\n",
      "23-03-30 13:25:18.039 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-6bd22ff8-e265-44b6-8912-64eff7457966\n",
      "23-03-30 13:25:18.040 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-cc2b38a9-edab-4c53-a1e5-1a96d0d074b6\n",
      "23-03-30 13:25:18.041 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-03-30 13:25:18.042 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-01d08977-c878-4ba9-bca5-fe6be9bcb6ca\n",
      "23-03-30 13:25:18.043 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-97b946be-6572-41cb-acf8-7a1d29e9b0b4\n",
      "23-03-30 13:25:18.044 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_model_metrics': 1, 'infobip_training_data': 1, 'infobip_start_training_data': 1, 'infobip_start_training': 1, 'infobip_realtime_data': 1}. \n",
      "23-03-30 13:25:18.062 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:25:18.062 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.064 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:25:18.064 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.065 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:25:18.066 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_realtime_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.067 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:25:18.068 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:18.068 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-30 13:25:18.069 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_model_metrics', partition=0)} for group airt-service-kafka-group\n",
      "server started\n",
      "Starting test production\n",
      "23-03-30 13:25:20.225 [INFO] __main__: start training msg=AccountId=1000 ApplicationId='DriverApp' ModelId='ChurnModelForDrivers' model_type=<ModelType.churn: 'churn'> total_no_of_records=1000\n",
      "Stopping test production\n",
      "Starting test consumption\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All events for account id 1000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TrainingStreamStatus(event=<TrainingEvent.start: 'start'>, account_id=1000, model_id='ChurnModelForDrivers', count=0, created=datetime.datetime(2023, 3, 30, 13, 2, 55), user_id=10, uuid=UUID('45b224e8-0c6d-4fa8-b8d4-c1cc555bf4e7'), id=185, application_id='DriverApp', model_type='churn', total=1000),\n",
       " TrainingStreamStatus(event=<TrainingEvent.upload: 'upload'>, account_id=1000, model_id='ChurnModelForDrivers', count=999, created=datetime.datetime(2023, 3, 30, 13, 3, 11), user_id=10, uuid=UUID('08b2b1f8-a128-4cbe-95ee-9ec631d68fb7'), id=186, application_id='DriverApp', model_type='churn', total=1000),\n",
       " TrainingStreamStatus(event=<TrainingEvent.end: 'end'>, account_id=1000, model_id='ChurnModelForDrivers', count=999, created=datetime.datetime(2023, 3, 30, 13, 25, 17), user_id=10, uuid=UUID('020c854e-d9e4-46f8-b44b-9fcae0e96ad9'), id=232, application_id='DriverApp', model_type='churn', total=1000),\n",
       " TrainingStreamStatus(event=<TrainingEvent.start: 'start'>, account_id=1000, model_id='ChurnModelForDrivers', count=0, created=datetime.datetime(2023, 3, 30, 13, 25, 20), user_id=10, uuid=UUID('ac72fea0-6051-4655-a22a-839fdec6cdd8'), id=234, application_id='DriverApp', model_type='churn', total=1000)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:20.991 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:20.991 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.001 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.002 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.006 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.007 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.062 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.062 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.063 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.064 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.067 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Cancelling background task 'startup_event'\n",
      "23-03-30 13:25:21.068 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Waiting for background task 'startup_event' to finish\n",
      "23-03-30 13:25:21.068 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Execution finished for background task 'startup_event'\n",
      "23-03-30 13:25:21.111 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:21.111 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:21.112 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:21.114 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.114 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.116 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.116 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.117 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.118 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:21.131 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:21.132 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:21.133 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:24.068 [WARNING] aiokafka.consumer.group_coordinator: Heartbeat failed for group airt-service-kafka-group because it is rebalancing\n",
      "23-03-30 13:25:24.072 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions frozenset({TopicPartition(topic='infobip_start_training', partition=0)}) for group airt-service-kafka-group\n",
      "23-03-30 13:25:24.073 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-30 13:25:24.077 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 2) with member_id aiokafka-0.8.0-5c665ec5-44d7-4938-ad58-0fad92c6bd48\n",
      "23-03-30 13:25:24.078 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-03-30 13:25:24.080 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 2\n",
      "23-03-30 13:25:24.081 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training', partition=0)} for group airt-service-kafka-group\n",
      "23-03-30 13:25:24.123 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-30 13:25:24.124 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-30 13:25:24.125 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-30 13:25:24.127 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 272303...\n",
      "23-03-30 13:25:25.892 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 272303 terminated.\n",
      "23-03-30 13:25:25.893 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 268060...\n",
      "23-03-30 13:25:27.230 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 268060 terminated.\n",
      "server stopped\n"
     ]
    }
   ],
   "source": [
    "async def test_kafka_integration(tester):\n",
    "    msg_count = 1000\n",
    "    seed = 42\n",
    "\n",
    "    mtr = ModelTrainingRequest(\n",
    "        AccountId=1000,\n",
    "        ApplicationId=\"DriverApp\",\n",
    "        model_type=\"churn\",\n",
    "        ModelId=\"ChurnModelForDrivers\",\n",
    "        total_no_of_records=msg_count,\n",
    "    )\n",
    "\n",
    "    await tester.to_infobip_start_training_data(mtr)\n",
    "\n",
    "    training_data = generate_n_rows_for_training_data(msg_count, seed=seed)\n",
    "    sanitized_print(\"Starting test production\")\n",
    "    for i in range(msg_count):\n",
    "        await tester.to_infobip_training_data(EventData(**training_data[i]))\n",
    "    sanitized_print(\"Stopping test production\")\n",
    "\n",
    "    sanitized_print(\"Starting test consumption\")\n",
    "    await tester.awaited_mocks.on_infobip_training_data_status.assert_awaited_with(\n",
    "        TrainingDataStatus(\n",
    "            AccountId=1000,\n",
    "            ApplicationId=\"DriverApp\",\n",
    "            ModelId=\"ChurnModelForDrivers\",\n",
    "            no_of_records=999,\n",
    "            total_no_of_records=msg_count,\n",
    "        ),\n",
    "        timeout=5 * 60,\n",
    "    )\n",
    "\n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(select(User).where(User.username == \"infobip\")).one()\n",
    "\n",
    "        display(f\"All events for account id {1000}\")\n",
    "        all_events = session.exec(\n",
    "            select(TrainingStreamStatus)\n",
    "            .where(TrainingStreamStatus.user == user)\n",
    "            .where(TrainingStreamStatus.account_id == 1000)\n",
    "        )\n",
    "        display([e for e in all_events])\n",
    "\n",
    "\n",
    "create_user_for_testing(username=\"infobip\")\n",
    "create_topics_for_user(username=\"infobip\")\n",
    "with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "    with MonkeyPatch.context() as monkeypatch:\n",
    "        monkeypatch.setattr(\n",
    "            \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "            lambda account_ids: pd.DataFrame(\n",
    "                {\n",
    "                    \"curr_count\": [999],\n",
    "                    \"AccountId\": [1000],\n",
    "                    \"curr_check_on\": [datetime.utcnow()],\n",
    "                }\n",
    "            ).set_index(\"AccountId\"),\n",
    "        )\n",
    "        app, fastkafka_app = create_ws_server(assets_path=Path(\"../assets\"))\n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=6009, log_level=\"debug\")\n",
    "\n",
    "        async with Tester(fastkafka_app) as tester:\n",
    "            # Server started.\n",
    "            sanitized_print(\"server started\")\n",
    "\n",
    "            await test_kafka_integration(tester)\n",
    "\n",
    "        sanitized_print(\"server stopped\")\n",
    "        # Server stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0753d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# patching async.run so we can run FastAPI within notebook (Jupyter started its own processing loop already)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9373efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = None\n",
    "\n",
    "\n",
    "def start_fastapi_server(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    "    host: str = \"0.0.0.0\",\n",
    "    port: int = 6006,\n",
    "    test_function: Optional[Callable[[], Any]] = None,\n",
    "):\n",
    "    app, fastkafka_app = create_fastapi_app(\n",
    "        assets_path=assets_path,\n",
    "    )\n",
    "\n",
    "    if test_function is not None:\n",
    "\n",
    "        @app.on_event(\"startup\")\n",
    "        async def startup_event():\n",
    "            global task\n",
    "            task = asyncio.create_task(test_function())\n",
    "\n",
    "        @app.on_event(\"shutdown\")\n",
    "        async def shutdown_event():\n",
    "            global task\n",
    "            task.cancel()\n",
    "            await asyncio.wait_for(task, timeout=3)\n",
    "            result = task.result()\n",
    "            display(f\"{result=}\")\n",
    "\n",
    "    uvicorn.run(app, host=host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b219816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-30 13:25:27.404 [INFO] fastkafka._application.app: run_in_background() : Adding function 'startup_event' as background task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [263031]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:6006 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:49412 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** task canceled ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"result='ok'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [263031]\n"
     ]
    }
   ],
   "source": [
    "# | notest\n",
    "# | eval: false\n",
    "\n",
    "with MonkeyPatch.context() as monkeypatch:\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "        lambda account_ids: pd.DataFrame(\n",
    "            {\n",
    "                \"curr_count\": [999],\n",
    "                \"AccountId\": [1000],\n",
    "                \"curr_check_on\": [datetime.utcnow()],\n",
    "            }\n",
    "        ).set_index(\"AccountId\"),\n",
    "    )\n",
    "    start_fastapi_server(test_function=test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
