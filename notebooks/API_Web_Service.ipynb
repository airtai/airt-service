{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c6d88ccd",
   "metadata": {},
   "source": [
    "---\n",
    "description: REST API Routes to import, train, predict events data\n",
    "output-file: api_web_service.html\n",
    "title: REST Server\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a302e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049c8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "23-03-28 09:03:33.320 [INFO] matplotlib.font_manager: generated new fontManager\n",
      "23-03-28 09:03:33.557 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from os import environ\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import yaml\n",
    "from aiokafka.helpers import create_ssl_context\n",
    "from airt.logger import get_logger\n",
    "from asyncer import asyncify\n",
    "from fastapi import FastAPI, Request, Response\n",
    "from fastapi.openapi.docs import get_redoc_html, get_swagger_ui_html\n",
    "from fastapi.openapi.utils import get_openapi\n",
    "from fastapi.responses import FileResponse, HTMLResponse, RedirectResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastkafka import FastKafka\n",
    "from pydantic import BaseModel, EmailStr, Field, HttpUrl, NonNegativeInt, validator\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service\n",
    "from airt_service.auth import auth_router\n",
    "from airt_service.confluent import aio_kafka_config\n",
    "from airt_service.data.datablob import datablob_router\n",
    "from airt_service.data.datasource import datasource_router\n",
    "from airt_service.db.models import User, get_session_with_context\n",
    "from airt_service.model.prediction import model_prediction_router\n",
    "from airt_service.model.train import model_train_router\n",
    "from airt_service.sanitizer import sanitized_print\n",
    "from airt_service.training_status_process import (\n",
    "    TrainingStreamStatus,\n",
    "    process_training_status,\n",
    ")\n",
    "from airt_service.users import user_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb446218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uvicorn\n",
    "from _pytest.monkeypatch import MonkeyPatch\n",
    "from confluent_kafka import Consumer, Producer\n",
    "from fastapi.testclient import TestClient\n",
    "from fastkafka.testing import Tester\n",
    "from starlette.datastructures import Headers\n",
    "\n",
    "from airt_service.confluent import confluent_kafka_config, create_topics_for_user\n",
    "from airt_service.db.models import create_user_for_testing\n",
    "from airt_service.helpers import set_env_variable_context\n",
    "from airt_service.uvicorn_helpers import run_uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c0f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8b0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "description = \"\"\"\n",
    "# airt service to import, train and predict events data\n",
    "\n",
    "## Python client\n",
    "\n",
    "To use python library please visit: <a href=\"https://docs.airt.ai\" target=\"_blank\">https://docs.airt.ai</a>\n",
    "\n",
    "## How to use\n",
    "\n",
    "To access the airt service, you must create a developer account. Please fill out the signup form below to get one:\n",
    "\n",
    "[https://bit.ly/3hbXQLY](https://bit.ly/3hbXQLY)\n",
    "\n",
    "Upon successful verification, you will receive the username and password for the developer account to your email.\n",
    "\n",
    "### 0. Authenticate\n",
    "\n",
    "Once you receive the username and password, please authenticate the same by calling the `/token` API. The API \n",
    "will return a bearer token if the authentication is successful.\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/token' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/x-www-form-urlencoded' \\\n",
    "  -d 'grant_type=&username=<username>&password=<password>&scope=&client_id=&client_secret='\n",
    "```\n",
    "\n",
    "You can either use the above bearer token or create additional apikey's for accessing the rest of the API's. \n",
    "\n",
    "To create additional apikey's, please call the `/apikey` API by passing the bearer token along with the \n",
    "details of the new apikey in the request. e.g:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/apikey' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"name\": \"<apikey_name>\",\n",
    "  \"expiry\": \"<datetime_in_ISO_8601_format>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "### 1. Connect data\n",
    "\n",
    "Establishing the connection with the data source is a two-step process. The first step allows \n",
    "you to pull the data into airt servers and the second step allows you to perform necessary data \n",
    "pre-processing that are required model training.\n",
    "\n",
    "Currently, we support importing data from:\n",
    "\n",
    "- files stored in the AWS S3 bucket,\n",
    "- databases like MySql, ClickHouse, and \n",
    "- local CSV/Parquet files,\n",
    "\n",
    "We plan to support other databases and storage medium in the future.\n",
    "\n",
    "To pull the data from a S3 bucket, please call the `/from_s3` API\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/datablob/from_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\",\n",
    "  \"access_key\": \"<access_key>\",\n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  \"tag\": \"<tag_name>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start importing the data in the background. This may take a while to complete depending on the size of the data.\n",
    "\n",
    "You can also check the data importing progress by calling the `/datablob/<datablob_id>` API\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/datablob/<datablob_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "Once the data import is completed, you can either call `/from_csv` or `/from_parquet` API for data pre-processing. Below is an \n",
    "example to pre-process an imported CSV data.\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "'https://api.airt.ai/datablob/<datablob_id>/from_csv' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Authorization: Bearer <bearer_token>' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\n",
    "  \"deduplicate_data\": <deduplicate_data>,\n",
    "  \"index_column\": \"<index_column>\",\n",
    "  \"sort_by\": \"<sort_by>\",\n",
    "  \"blocksize\": \"<block_size>\",\n",
    "  \"kwargs\": {}\n",
    "}'\n",
    "```\n",
    "\n",
    "### 2. Train\n",
    "\n",
    "For model training, we assume the input data includes the following:\n",
    "\n",
    "- a column identifying a client client_column (person, car, business, etc.),\n",
    "- a column specifying a type of event we will try to predict target_column (buy, checkout, click on form submit, etc.), and\n",
    "- a timestamp column specifying the time of an occurred event.\n",
    "\n",
    "The input data can have additional features of any type and will be used to make predictions more accurate. Finally, we need to \n",
    "know how much ahead we wish to make predictions. Please use the parameter predict_after to specify the period based on your needs.\n",
    "\n",
    "In the following example, we will train a model to predict which users will perform a purchase event (*purchase) 3 hours before they acctually do it:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/train' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>,\n",
    "  \"client_column\": \"<client_column>\",\n",
    "  \"target_column\": \"<target_column>\",\n",
    "  \"target\": \"*checkout\",\n",
    "  \"predict_after\": 10800\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start the model training in the background. This may take a while to complete and you can check the \n",
    "training progress by calling the `/model/<model_id>` API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "After training is complete, you can check the quality of the model by calling the `/model/<model_id>/evaluate` API. This API \n",
    "will return model validation metrics like model accuracy, precision and recall.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>/evaluate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "### 3. Predict\n",
    "\n",
    "Finally, you can run the predictions by calling the /model/<model_id>/predict API:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/<model_id>/predict' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>\n",
    "}'\n",
    "```\n",
    "Calling the above API will start running the model prediction in the background. This may take a while to complete and you can check the training progress by calling the /prediction/<prediction_id> API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "If the dataset is small, then you can call `/prediction/<prediction_id>/pandas` to get prediction results as a pandas dataframe convertible json format:\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/pandas' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "In many cases, it's much better to push the prediction results to remote destinations. Currently, we support pushing the prediction results to a AWS S3 bucket, MySql database and download to the local machine.\n",
    "\n",
    "To push the predictions to a S3 bucket, please call the `/prediction/<prediction_id>/to_s3` API\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/to_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\", \n",
    "  \"access_key\": \"<access_key>\", \n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  }'\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc5554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ModelType(str, Enum):\n",
    "    churn = \"churn\"\n",
    "    propensity_to_buy = \"propensity_to_buy\"\n",
    "\n",
    "\n",
    "class ModelTrainingRequest(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ..., description=\"Model type, only 'churn' is supported right now\"\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"approximate total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "\n",
    "class EventData(BaseModel):\n",
    "    \"\"\"\n",
    "    A sequence of events for a fixed account_id\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    DefinitionId: str = Field(\n",
    "        ...,\n",
    "        example=\"appLaunch\",\n",
    "        description=\"name of the event\",\n",
    "        min_length=1,\n",
    "    )\n",
    "    OccurredTime: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"local time of the event\",\n",
    "    )\n",
    "    OccurredTimeTicks: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1616891648496,\n",
    "        description=\"local time of the event as the number of ticks\",\n",
    "    )\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RealtimeData(EventData):\n",
    "    pass\n",
    "\n",
    "\n",
    "class TrainingDataStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=12_345,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "\n",
    "class TrainingModelStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    current_step: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=0,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    current_step_percentage: float = Field(\n",
    "        ...,\n",
    "        example=0.21,\n",
    "        description=\"the percentage of the current step completed\",\n",
    "    )\n",
    "    total_no_of_steps: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of steps for training the model\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ModelMetrics(BaseModel):\n",
    "    \"\"\"The standard metrics for classification models.\n",
    "\n",
    "    The most important metrics is AUC for unbalanced classes such as churn. Metrics such as\n",
    "    accuracy are not very useful since they are easily maximized by outputting the most common\n",
    "    class all the time.\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    timestamp: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time when the model was trained\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "\n",
    "    auc: float = Field(\n",
    "        ..., example=0.91, description=\"Area under ROC curve\", ge=0.0, le=1.0\n",
    "    )\n",
    "    f1: float = Field(..., example=0.89, description=\"F-1 score\", ge=0.0, le=1.0)\n",
    "    precission: float = Field(\n",
    "        ..., example=0.84, description=\"precission\", ge=0.0, le=1.0\n",
    "    )\n",
    "    recall: float = Field(..., example=0.82, description=\"recall\", ge=0.0, le=1.0)\n",
    "    accuracy: float = Field(..., example=0.82, description=\"accuracy\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"TestApplicationId\",\n",
    "        description=\"Id of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    ModelId: Optional[str] = Field(\n",
    "        default=None,\n",
    "        example=\"ChurnModelForDrivers\",\n",
    "        description=\"User supplied ID of the model trained\",\n",
    "    )\n",
    "\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "    prediction_time: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time of prediction\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "    score: float = Field(\n",
    "        ...,\n",
    "        example=0.4321,\n",
    "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0438235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_total_no_of_records = 1000000\n",
    "_no_of_records_received = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0905864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_fastkafka_application(\n",
    "    start_process_for_username: Optional[str] = \"infobip\",\n",
    ") -> FastKafka:\n",
    "    \"\"\"Create a FastKafka service\n",
    "\n",
    "    Args:\n",
    "        start_process_for_username: prefix for topics used\n",
    "\n",
    "    Returns:\n",
    "        A FastKafka application\n",
    "    \"\"\"\n",
    "\n",
    "    kafka_brokers = {\n",
    "        \"staging\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Staging Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "        \"production\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Production Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # add development bootstrap servers if needed\n",
    "    url, port = aio_kafka_config[\"bootstrap_servers\"].split(\":\")\n",
    "    if (url != kafka_brokers[\"staging\"][\"url\"]) and (\n",
    "        url != kafka_brokers[\"production\"][\"url\"]\n",
    "    ):\n",
    "        kafka_brokers[\"dev\"] = {\n",
    "            \"url\": url,\n",
    "            \"description\": \"Development Kafka broker\",\n",
    "            \"port\": port,\n",
    "        }\n",
    "\n",
    "    # global description\n",
    "    version = airt_service.__version__\n",
    "    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n",
    "\n",
    "    fastkafka_app = FastKafka(\n",
    "        title=\"airt service kafka api\",\n",
    "        description=\"kafka api for airt service\",\n",
    "        kafka_brokers=kafka_brokers,\n",
    "        version=version,\n",
    "        contact=contact,\n",
    "        group_id=\"airt-service-kafka-group\",\n",
    "        auto_offset_reset=\"earliest\",\n",
    "    )\n",
    "\n",
    "    @fastkafka_app.consumes(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_start_training_data\"\n",
    "    )\n",
    "    async def on_infobip_start_training_data(msg: ModelTrainingRequest):\n",
    "        logger.info(f\"start training msg={msg}\")\n",
    "        with get_session_with_context() as session:\n",
    "            user = session.exec(\n",
    "                select(User).where(User.username == start_process_for_username)\n",
    "            ).one()\n",
    "            start_event = TrainingStreamStatus(\n",
    "                event=\"start\",\n",
    "                account_id=msg.AccountId,\n",
    "                application_id=msg.ApplicationId,\n",
    "                model_id=msg.ModelId,\n",
    "                model_type=msg.model_type,\n",
    "                count=0,\n",
    "                total=msg.total_no_of_records,\n",
    "                user=user,\n",
    "            )\n",
    "            session.add(start_event)\n",
    "            session.commit()\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_training_data\")  # type: ignore\n",
    "    async def on_infobip_training_data(msg: EventData):\n",
    "        pass\n",
    "        # ToDo: this is not showing up in logs\n",
    "\n",
    "    #         logger.debug(f\"msg={msg}\")\n",
    "\n",
    "    #         global _total_no_of_records\n",
    "    #         global _no_of_records_received\n",
    "    #         _no_of_records_received = _no_of_records_received + 1\n",
    "\n",
    "    #         if _no_of_records_received % 100 == 0:\n",
    "    #             training_data_status = TrainingDataStatus(\n",
    "    #                 AccountId=msg.AccountId,\n",
    "    #                 no_of_records=_no_of_records_received,\n",
    "    #                 total_no_of_records=_total_no_of_records,\n",
    "    #             )\n",
    "    #             await to_infobip_training_data_status(msg=training_data_status)\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_realtime_data\")  # type: ignore\n",
    "    async def on_infobip_realtime_data(msg: RealtimeData):\n",
    "        pass\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_data_status\"\n",
    "    )\n",
    "    async def to_infobip_training_data_status(\n",
    "        account_id: int,\n",
    "        *,\n",
    "        application_id: Optional[str] = None,\n",
    "        model_id: str,\n",
    "        no_of_records: int,\n",
    "        total_no_of_records: int,\n",
    "    ) -> TrainingDataStatus:\n",
    "        logger.debug(\n",
    "            f\"on_infobip_training_data_status({account_id=}, {no_of_records=}, {total_no_of_records=})\"\n",
    "        )\n",
    "        msg = TrainingDataStatus(\n",
    "            AccountId=account_id,\n",
    "            ApplicationId=application_id,\n",
    "            ModelId=model_id,\n",
    "            no_of_records=no_of_records,\n",
    "            total_no_of_records=total_no_of_records,\n",
    "        )\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_model_status\"\n",
    "    )\n",
    "    async def to_infobip_training_model_status(msg: str) -> TrainingModelStatus:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return TrainingModelStatus()\n",
    "\n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_model_metrics\")  # type: ignore\n",
    "    async def to_infobip_model_metrics(msg: ModelMetrics) -> ModelMetrics:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_prediction\")  # type: ignore\n",
    "    async def to_infobip_prediction(msg: Prediction) -> Prediction:\n",
    "        logger.debug(f\"on_infobip_realtime_data_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    fastkafka_app.to_infobip_training_data_status = to_infobip_training_data_status\n",
    "    if start_process_for_username is not None:\n",
    "\n",
    "        @fastkafka_app.run_in_background()  # type: ignore\n",
    "        async def startup_event(fastkafka_app: FastKafka = fastkafka_app) -> None:\n",
    "            await process_training_status(\n",
    "                username=start_process_for_username,  # type: ignore\n",
    "                fast_kafka_api_app=fastkafka_app,\n",
    "            )\n",
    "\n",
    "    return fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "398bf2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'infobip'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_user_for_testing(username=\"infobip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a9de5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680010882.628141"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.fromisoformat('2023-03-28T13:41:22.628141').timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12b51074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-28 14:00:08.607 [INFO] fastkafka._application.app: run_in_background() : Adding function 'startup_event' as background task\n",
      "23-03-28 14:00:08.611 [INFO] fastkafka._components.test_dependencies: Java is already installed.\n",
      "23-03-28 14:00:08.612 [INFO] fastkafka._components.test_dependencies: Kafka is installed.\n",
      "23-03-28 14:00:08.613 [INFO] fastkafka._testing.local_broker: Starting zookeeper...\n",
      "23-03-28 14:00:09.348 [INFO] fastkafka._testing.local_broker: Starting kafka...\n",
      "23-03-28 14:00:11.289 [INFO] fastkafka._testing.local_broker: Local Kafka broker up and running on 127.0.0.1:9092\n",
      "23-03-28 14:00:13.160 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.175 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.183 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.188 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.195 [INFO] fastkafka._application.app: _populate_bg_tasks() : Starting background task 'startup_event'\n",
      "23-03-28 14:00:13.210 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.211 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.212 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.213 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.214 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.215 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.216 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'airt-service-kafka-group', 'auto_offset_reset': 'earliest', 'bootstrap_servers': '127.0.0.1:9092', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.226 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.227 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training_data'})\n",
      "23-03-28 14:00:13.228 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training_data'}\n",
      "23-03-28 14:00:13.229 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.231 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.231 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data'})\n",
      "23-03-28 14:00:13.232 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data'}\n",
      "23-03-28 14:00:13.233 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.234 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.235 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.236 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_realtime_data'})\n",
      "23-03-28 14:00:13.237 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_realtime_data'}\n",
      "23-03-28 14:00:13.240 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.251 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'\n",
      "23-03-28 14:00:13.253 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.254 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.259 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.260 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.262 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.263 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.265 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.266 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.270 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-03-28 14:00:13.271 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}\n",
      "23-03-28 14:00:13.272 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.297 [INFO] airt_service.training_status_process: Error in process_training_status - 'KAFKA_CH_USERNAME', Traceback (most recent call last):\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 238, in process_training_status\n",
      "    ch_df = await async_get_count_from_training_data_ch_table(\n",
      "  File \"/home/davor/.local/lib/python3.9/site-packages/asyncer/_main.py\", line 358, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 143, in get_count_from_training_data_ch_table\n",
      "    username=environ[\"KAFKA_CH_USERNAME\"],\n",
      "  File \"/usr/lib/python3.9/os.py\", line 679, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'KAFKA_CH_USERNAME'\n",
      "\n",
      "23-03-28 14:00:13.301 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.302 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data_status'})\n",
      "23-03-28 14:00:13.302 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data_status'}\n",
      "23-03-28 14:00:13.303 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.305 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.305 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_model_metrics'})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-28 14:00:13.306 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_model_metrics'}\n",
      "23-03-28 14:00:13.306 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.307 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.308 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_model_status'})\n",
      "23-03-28 14:00:13.308 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_model_status'}\n",
      "23-03-28 14:00:13.309 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.311 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-03-28 14:00:13.311 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_prediction'})\n",
      "23-03-28 14:00:13.312 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_prediction'}\n",
      "23-03-28 14:00:13.313 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-03-28 14:00:13.601 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.602 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.604 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_data_status': 1}. \n",
      "23-03-28 14:00:13.605 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_training_model_status': 1}. \n",
      "23-03-28 14:00:13.606 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_model_metrics': 1}. \n",
      "23-03-28 14:00:13.607 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_prediction': 1}. \n",
      "23-03-28 14:00:13.608 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.708 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.711 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.713 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.814 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.816 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.817 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.918 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.920 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:13.921 [ERROR] aiokafka.consumer.group_coordinator: Group Coordinator Request failed: [Error 15] CoordinatorNotAvailableError\n",
      "23-03-28 14:00:14.027 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.028 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.029 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-28 14:00:14.031 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.031 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.032 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-28 14:00:14.033 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.034 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.034 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group airt-service-kafka-group\n",
      "23-03-28 14:00:14.057 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-24871d42-2229-40e8-b654-15bf15d45222\n",
      "23-03-28 14:00:14.058 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-95c33241-8cda-427f-8233-362e217d2ea4\n",
      "23-03-28 14:00:14.059 [INFO] aiokafka.consumer.group_coordinator: Joined group 'airt-service-kafka-group' (generation 1) with member_id aiokafka-0.8.0-501b045a-d100-48ca-9644-21170cfe798b\n",
      "23-03-28 14:00:14.060 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-03-28 14:00:14.063 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_realtime_data': 1, 'infobip_start_training_data': 1, 'infobip_training_data': 1}. \n",
      "23-03-28 14:00:14.097 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-28 14:00:14.097 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.098 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-28 14:00:14.099 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-28 14:00:14.100 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group airt-service-kafka-group with generation 1\n",
      "23-03-28 14:00:14.100 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_realtime_data', partition=0)} for group airt-service-kafka-group\n",
      "23-03-28 14:00:16.276 [INFO] __main__: start training msg=AccountId=12345 ApplicationId=None ModelId='drivers' model_type=<ModelType.churn: 'churn'> total_no_of_records=10000\n",
      "23-03-28 14:00:18.348 [INFO] airt_service.training_status_process: Error in process_training_status - 'KAFKA_CH_USERNAME', Traceback (most recent call last):\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 238, in process_training_status\n",
      "    ch_df = await async_get_count_from_training_data_ch_table(\n",
      "  File \"/home/davor/.local/lib/python3.9/site-packages/asyncer/_main.py\", line 358, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 143, in get_count_from_training_data_ch_table\n",
      "    username=environ[\"KAFKA_CH_USERNAME\"],\n",
      "  File \"/usr/lib/python3.9/os.py\", line 679, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'KAFKA_CH_USERNAME'\n",
      "\n",
      "23-03-28 14:00:24.398 [INFO] airt_service.training_status_process: Error in process_training_status - 'KAFKA_CH_USERNAME', Traceback (most recent call last):\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 238, in process_training_status\n",
      "    ch_df = await async_get_count_from_training_data_ch_table(\n",
      "  File \"/home/davor/.local/lib/python3.9/site-packages/asyncer/_main.py\", line 358, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 143, in get_count_from_training_data_ch_table\n",
      "    username=environ[\"KAFKA_CH_USERNAME\"],\n",
      "  File \"/usr/lib/python3.9/os.py\", line 679, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'KAFKA_CH_USERNAME'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-28 14:00:34.447 [INFO] airt_service.training_status_process: Error in process_training_status - 'KAFKA_CH_USERNAME', Traceback (most recent call last):\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 238, in process_training_status\n",
      "    ch_df = await async_get_count_from_training_data_ch_table(\n",
      "  File \"/home/davor/.local/lib/python3.9/site-packages/asyncer/_main.py\", line 358, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 143, in get_count_from_training_data_ch_table\n",
      "    username=environ[\"KAFKA_CH_USERNAME\"],\n",
      "  File \"/usr/lib/python3.9/os.py\", line 679, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'KAFKA_CH_USERNAME'\n",
      "\n",
      "23-03-28 14:00:46.345 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.346 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.347 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.347 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.348 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.349 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.349 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.350 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.353 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Cancelling background task 'startup_event'\n",
      "23-03-28 14:00:46.353 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Waiting for background task 'startup_event' to finish\n",
      "23-03-28 14:00:46.354 [INFO] fastkafka._application.app: _shutdown_bg_tasks() : Execution finished for background task 'startup_event'\n",
      "23-03-28 14:00:46.383 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-28 14:00:46.384 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.385 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.411 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-28 14:00:46.412 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.412 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.413 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-03-28 14:00:46.415 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-03-28 14:00:46.415 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-03-28 14:00:46.417 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 48074...\n",
      "23-03-28 14:00:47.444 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:00:47.445 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:00:47.446 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:00:47.446 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:00:47.448 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:00:47.449 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:00:47.450 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:00:47.450 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:00:48.148 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 48074 terminated.\n",
      "23-03-28 14:00:48.149 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 47691...\n",
      "23-03-28 14:00:49.486 [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 47691 terminated.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected 'mock' to have been called.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 23\u001b[0m\n\u001b[1;32m     13\u001b[0m training_data \u001b[38;5;241m=\u001b[39m EventData(\n\u001b[1;32m     14\u001b[0m     AccountId\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m,\n\u001b[1;32m     15\u001b[0m     ModelId\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrivers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     OccurredTimeTicks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1680010882628\u001b[39m,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m tester\u001b[38;5;241m.\u001b[39mto_infobip_training_data(training_data)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m tester\u001b[38;5;241m.\u001b[39mawaited_mocks\u001b[38;5;241m.\u001b[39mon_infobip_training_data_status\u001b[38;5;241m.\u001b[39massert_called(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/fastkafka/_application/app.py:1465\u001b[0m, in \u001b[0;36mAwaitedMock._await_for.<locals>.inner\u001b[0;34m(f, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m t0 \u001b[38;5;241m>\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39mtimeout):\n\u001b[1;32m   1463\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1465\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/fastkafka/_application/app.py:1457\u001b[0m, in \u001b[0;36mAwaitedMock._await_for.<locals>.inner\u001b[0;34m(f, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:\n\u001b[1;32m   1459\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/unittest/mock.py:876\u001b[0m, in \u001b[0;36mNonCallableMock.assert_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    874\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have been called.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    875\u001b[0m            (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mock_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmock\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected 'mock' to have been called."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-28 14:02:30.960 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:02:30.961 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:02:30.962 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:02:30.963 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:02:30.963 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:02:30.964 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:02:30.965 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:02:30.965 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:03:35.569 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:03:35.570 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:03:35.571 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:03:35.572 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:03:35.573 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:03:35.573 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:03:35.575 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:03:35.576 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:04:07.849 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:04:07.850 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:04:07.851 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:04:07.852 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:04:07.853 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:04:07.853 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:04:07.854 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:04:07.855 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:05:47.550 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:05:47.551 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:05:47.552 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:05:47.553 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:05:47.554 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:05:47.555 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:05:47.556 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:05:47.557 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:07:30.988 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:07:30.989 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:07:30.990 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:07:30.991 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:07:30.992 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:07:30.993 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:07:30.994 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:07:30.994 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:08:35.637 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:08:35.638 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:08:35.639 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:08:35.640 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:08:35.641 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:08:35.642 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:08:35.643 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:08:35.644 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:09:07.887 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:09:07.888 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:09:07.889 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:09:07.890 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:09:07.891 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:09:07.891 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:09:07.892 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:09:07.893 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:10:47.655 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:10:47.656 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:10:47.657 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:10:47.657 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:10:47.658 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:10:47.659 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:10:47.660 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:10:47.661 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:12:31.024 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:12:31.025 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:12:31.026 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:12:31.028 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:12:31.029 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:12:31.030 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:12:31.030 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:12:31.031 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:13:35.656 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:13:35.657 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:13:35.658 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:13:35.659 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:13:35.660 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:13:35.661 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:13:35.661 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:13:35.662 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:14:07.926 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-03-28 14:14:07.927 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:14:07.928 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:14:07.929 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:14:07.930 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:14:07.930 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:14:07.931 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:14:07.932 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:15:47.760 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:15:47.761 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:15:47.762 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:15:47.763 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:15:47.764 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:15:47.765 [ERROR] aiokafka: Unable to update metadata from [0]\n",
      "23-03-28 14:15:47.766 [ERROR] aiokafka: Unable connect to node with id 0: [Errno 111] Connect call failed ('172.19.0.7', 9092)\n",
      "23-03-28 14:15:47.766 [ERROR] aiokafka: Unable to update metadata from [0]\n"
     ]
    }
   ],
   "source": [
    "fastkafka_app = create_fastkafka_application()\n",
    "fastkafka_app\n",
    "\n",
    "async with Tester(fastkafka_app) as tester:\n",
    "    training_req = ModelTrainingRequest(\n",
    "        AccountId=12345,\n",
    "        ModelId=\"drivers\",\n",
    "        model_type=\"churn\",\n",
    "        total_no_of_records=10_000,\n",
    "    )\n",
    "    await tester.to_infobip_start_training_data(training_req)\n",
    "\n",
    "    training_data = EventData(\n",
    "        AccountId=12345,\n",
    "        ModelId=\"drivers\",\n",
    "        DefinitionId=\"event_name\",\n",
    "        PersonId=12,\n",
    "        OccurredTime=\"2023-03-28T13:41:22.628141\",\n",
    "        OccurredTimeTicks=1680010882628,\n",
    "    )\n",
    "    await tester.to_infobip_training_data(training_data)\n",
    "\n",
    "    await tester.awaited_mocks.on_infobip_training_data_status.assert_called(timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "043c8cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TesterMocks(on_infobip_training_data_status=<fastkafka._application.app.AwaitedMock object at 0x7f24e8fb3e20>, on_infobip_training_model_status=<fastkafka._application.app.AwaitedMock object at 0x7f24e8fabbb0>, on_infobip_model_metrics=<fastkafka._application.app.AwaitedMock object at 0x7f24e8fda2b0>, on_infobip_prediction=<fastkafka._application.app.AwaitedMock object at 0x7f24e8fe39d0>, to_infobip_start_training_data=<fastkafka._application.app.AwaitedMock object at 0x7f24e89c7f70>, to_infobip_training_data=<fastkafka._application.app.AwaitedMock object at 0x7f24e89d36d0>, to_infobip_realtime_data=<fastkafka._application.app.AwaitedMock object at 0x7f24e89ddc70>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tester.awaited_mocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f31d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2030f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c257d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f5c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19328ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0a00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd61bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb85d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0789e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_ws_server(\n",
    "    assets_path: Path = Path(\"./assets\"),\n",
    "    start_process_for_username: Optional[str] = \"infobip\",\n",
    ") -> Tuple[FastAPI, FastKafka]:\n",
    "    \"\"\"Create a FastKafka based web service\n",
    "\n",
    "    Args:\n",
    "        assets_path: Path to assets (should include favicon.ico)\n",
    "\n",
    "    Returns:\n",
    "        A FastKafka server\n",
    "    \"\"\"\n",
    "    global description\n",
    "    title = \"airt service\"\n",
    "    version = airt_service.__version__\n",
    "    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n",
    "    openapi_url = \"/openapi.json\"\n",
    "    favicon_url = \"/assets/images/favicon.ico\"\n",
    "    assets_path = assets_path.resolve()\n",
    "    favicon_path = assets_path / \"images/favicon.ico\"\n",
    "\n",
    "    app = FastAPI(\n",
    "        title=title,\n",
    "        description=description,\n",
    "        version=version,\n",
    "        docs_url=None,\n",
    "        redoc_url=None,\n",
    "    )\n",
    "    app.mount(\"/assets\", StaticFiles(directory=assets_path), name=\"assets\")\n",
    "\n",
    "    asyncapi_path = Path(\"./asyncapi/docs\").resolve()\n",
    "\n",
    "    if asyncapi_path.exists():\n",
    "        app.mount(\n",
    "            \"/asyncapi\",\n",
    "            StaticFiles(directory=asyncapi_path, html=True),\n",
    "            name=\"asyncapi\",\n",
    "        )\n",
    "\n",
    "    # attaches /token to routes\n",
    "    app.include_router(auth_router)\n",
    "\n",
    "    # attaches /datablob/* to routes\n",
    "    app.include_router(datablob_router)\n",
    "\n",
    "    # attaches /datasource/* to routes\n",
    "    app.include_router(datasource_router)\n",
    "\n",
    "    # attaches /model/* to routes\n",
    "    app.include_router(model_train_router)\n",
    "\n",
    "    # attaches /prediction/* to routes\n",
    "    app.include_router(model_prediction_router)\n",
    "\n",
    "    # attaches /user/* to routes\n",
    "    app.include_router(user_router)\n",
    "\n",
    "    @app.middleware(\"http\")\n",
    "    async def add_nosniff_x_content_type_options_header(\n",
    "        request: Request, call_next: Callable[[Request], Response]\n",
    "    ) -> Response:\n",
    "        response: Response = await call_next(request)  # type: ignore\n",
    "        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n",
    "        response.headers[\"Strict-Transport-Security\"] = \"max-age=31536000\"\n",
    "        return response\n",
    "\n",
    "    @app.get(\"/version\")\n",
    "    def get_versions() -> Dict[str, str]:\n",
    "        return {\"airt_service\": airt_service.__version__}\n",
    "\n",
    "    @app.get(\"/\", include_in_schema=False)\n",
    "    def redirect_root() -> RedirectResponse:\n",
    "        return RedirectResponse(\"/docs\")\n",
    "\n",
    "    @app.get(\"/docs\", include_in_schema=False)\n",
    "    def overridden_swagger() -> HTMLResponse:\n",
    "        return get_swagger_ui_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            swagger_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/redoc\", include_in_schema=False)\n",
    "    def overridden_redoc() -> HTMLResponse:\n",
    "        return get_redoc_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            redoc_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/favicon.ico\", include_in_schema=False)\n",
    "    async def serve_favicon() -> FileResponse:\n",
    "        return FileResponse(favicon_path)\n",
    "\n",
    "    def custom_openapi() -> Dict[str, Any]:\n",
    "        if app.openapi_schema:\n",
    "            return app.openapi_schema\n",
    "\n",
    "        fastapi_schema = get_openapi(\n",
    "            title=title,\n",
    "            description=description,\n",
    "            version=version,\n",
    "            routes=app.routes,\n",
    "        )\n",
    "\n",
    "        # ToDo: Figure out recursive dict merge\n",
    "        fastapi_schema[\"servers\"] = [\n",
    "            {\n",
    "                \"url\": \"http://0.0.0.0:6006\"\n",
    "                if (\n",
    "                    environ[\"DOMAIN\"] == \"localhost\"\n",
    "                    or \"airt-service\" in environ[\"DOMAIN\"]\n",
    "                )\n",
    "                else f\"https://{environ['DOMAIN']}\",\n",
    "                \"description\": \"Server\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        app.openapi_schema = fastapi_schema\n",
    "        return app.openapi_schema\n",
    "\n",
    "    app.openapi = custom_openapi  # type: ignore\n",
    "\n",
    "    #     logger.info(f\"kafka_config={aio_kafka_config}\")\n",
    "\n",
    "    url, port = aio_kafka_config[\"bootstrap_servers\"].split(\":\")\n",
    "\n",
    "    kafka_brokers = {\n",
    "        \"staging\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Staging Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "        \"production\": {\n",
    "            \"url\": \"pkc-1wvvj.westeurope.azure.confluent.cloud\",\n",
    "            \"description\": \"Production Kafka broker\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if (url != kafka_brokers[\"staging\"][\"url\"]) and (\n",
    "        url != kafka_brokers[\"production\"][\"url\"]\n",
    "    ):\n",
    "        kafka_brokers[\"dev\"] = {\n",
    "            \"url\": url,\n",
    "            \"description\": \"Development Kafka broker\",\n",
    "            \"port\": port,\n",
    "        }\n",
    "\n",
    "    fastkafka_app = FastKafka(\n",
    "        title=\"airt service kafka api\",\n",
    "        description=\"kafka api for airt service\",\n",
    "        kafka_brokers=kafka_brokers,\n",
    "        version=version,\n",
    "        contact=contact,\n",
    "        group_id=\"airt-service-kafka-group\",\n",
    "        auto_offset_reset=\"earliest\",\n",
    "    )\n",
    "\n",
    "    @fastkafka_app.consumes(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_start_training_data\"\n",
    "    )\n",
    "    async def on_infobip_start_training_data(msg: ModelTrainingRequest):\n",
    "        logger.info(f\"start training msg={msg}\")\n",
    "        with get_session_with_context() as session:\n",
    "            user = session.exec(\n",
    "                select(User).where(User.username == start_process_for_username)\n",
    "            ).one()\n",
    "            start_event = TrainingStreamStatus(\n",
    "                event=\"start\",\n",
    "                account_id=msg.AccountId,\n",
    "                application_id=msg.ApplicationId,\n",
    "                model_id=msg.ModelId,\n",
    "                model_type=msg.model_type,\n",
    "                count=0,\n",
    "                total=msg.total_no_of_records,\n",
    "                user=user,\n",
    "            )\n",
    "            session.add(start_event)\n",
    "            session.commit()\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_training_data\")  # type: ignore\n",
    "    async def on_infobip_training_data(msg: EventData):\n",
    "        pass\n",
    "        # ToDo: this is not showing up in logs\n",
    "\n",
    "    #         logger.debug(f\"msg={msg}\")\n",
    "\n",
    "    #         global _total_no_of_records\n",
    "    #         global _no_of_records_received\n",
    "    #         _no_of_records_received = _no_of_records_received + 1\n",
    "\n",
    "    #         if _no_of_records_received % 100 == 0:\n",
    "    #             training_data_status = TrainingDataStatus(\n",
    "    #                 AccountId=msg.AccountId,\n",
    "    #                 no_of_records=_no_of_records_received,\n",
    "    #                 total_no_of_records=_total_no_of_records,\n",
    "    #             )\n",
    "    #             await to_infobip_training_data_status(msg=training_data_status)\n",
    "\n",
    "    @fastkafka_app.consumes(topic=f\"{start_process_for_username}_realtime_data\")  # type: ignore\n",
    "    async def on_infobip_realtime_data(msg: RealtimeData):\n",
    "        pass\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_data_status\"\n",
    "    )\n",
    "    async def to_infobip_training_data_status(\n",
    "        account_id: int,\n",
    "        *,\n",
    "        application_id: Optional[str] = None,\n",
    "        model_id: str,\n",
    "        no_of_records: int,\n",
    "        total_no_of_records: int,\n",
    "    ) -> TrainingDataStatus:\n",
    "        logger.debug(\n",
    "            f\"on_infobip_training_data_status({account_id=}, {no_of_records=}, {total_no_of_records=})\"\n",
    "        )\n",
    "        msg = TrainingDataStatus(\n",
    "            AccountId=account_id,\n",
    "            ApplicationId=application_id,\n",
    "            ModelId=model_id,\n",
    "            no_of_records=no_of_records,\n",
    "            total_no_of_records=total_no_of_records,\n",
    "        )\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(  # type: ignore\n",
    "        topic=f\"{start_process_for_username}_training_model_status\"\n",
    "    )\n",
    "    async def to_infobip_training_model_status(msg: str) -> TrainingModelStatus:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return TrainingModelStatus()\n",
    "\n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_model_metrics\")  # type: ignore\n",
    "    async def to_infobip_model_metrics(msg: ModelMetrics) -> ModelMetrics:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    @fastkafka_app.produces(topic=f\"{start_process_for_username}_prediction\")  # type: ignore\n",
    "    async def to_infobip_prediction(msg: Prediction) -> Prediction:\n",
    "        logger.debug(f\"on_infobip_realtime_data_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    fastkafka_app.to_infobip_training_data_status = to_infobip_training_data_status\n",
    "    if start_process_for_username is not None:\n",
    "\n",
    "        @fastkafka_app.run_in_background()  # type: ignore\n",
    "        async def startup_event() -> None:\n",
    "            await process_training_status(\n",
    "                username=start_process_for_username,  # type: ignore\n",
    "                fastkafka_app=fastkafka_app,\n",
    "            )\n",
    "\n",
    "    return app, fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d89806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fastapi_app(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    ") -> Tuple[FastAPI, FastKafka]:\n",
    "    assets_path = assets_path.resolve()\n",
    "    app, fastkafka_app = create_ws_server(assets_path=assets_path)\n",
    "    return app, fastkafka_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804db4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "app, fastkafka_app = create_fastapi_app()\n",
    "client = TestClient(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b855f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_username = \"johndoe\"\n",
    "# oauth_data = dict(\n",
    "#     username=test_username, password=environ[\"AIRT_SERVICE_SUPER_USER_PASSWORD\"]\n",
    "# )\n",
    "\n",
    "# response = client.post(\"/token\", data=oauth_data)\n",
    "# actual = response.json()\n",
    "# display(actual)\n",
    "# assert \"access_token\" in actual\n",
    "# assert actual[\"token_type\"] == \"bearer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "async def test_function():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        while True:\n",
    "            try:\n",
    "                await client.get(\"http://0.0.0.0:6006/docs\")\n",
    "                sanitized_print(\"docs retrieved\")\n",
    "            except httpx.ConnectError:\n",
    "                sanitized_print(\"-\", end=\"\")\n",
    "            except httpx.TimeoutException:\n",
    "                sanitized_print(\".\", end=\"\")\n",
    "            except Exception as e:\n",
    "                sanitized_print(\"?\", end=\"\")\n",
    "                sanitized_print(e)\n",
    "                raise e\n",
    "            try:\n",
    "                await asyncio.sleep(1)\n",
    "            except asyncio.CancelledError:\n",
    "                sanitized_print(\"\\n*** task canceled ***\")\n",
    "                return \"ok\"\n",
    "\n",
    "\n",
    "# task = asyncio.create_task(test_function())\n",
    "# await asyncio.sleep(3)\n",
    "# task.cancel()\n",
    "# await asyncio.wait_for(task, timeout=2)\n",
    "# task.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992723d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = [\n",
    "    \"appLaunch\",\n",
    "    \"sign_in\",\n",
    "    \"sign_out\",\n",
    "    \"add_to_cart\",\n",
    "    \"purchase\",\n",
    "    \"custom_event_1\",\n",
    "    \"custom_event_2\",\n",
    "    \"custom_event_3\",\n",
    "]\n",
    "\n",
    "\n",
    "# applications = [\"DriverApp\", \"PUBG\", \"COD\"]\n",
    "applications = [\"DriverApp\"]\n",
    "\n",
    "\n",
    "def generate_n_rows_for_training_data(n: int, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    #     account_id = rng.choice([4000, 5000, 500], size=n)\n",
    "    account_id = 1000\n",
    "    definition_id = rng.choice(definitions, size=n)\n",
    "    application_id = rng.choice(applications, size=n)\n",
    "    model_id = rng.choice([\"ChurnModelForDrivers\", None], size=n)\n",
    "    occurred_time_ticks = rng.integers(\n",
    "        datetime(year=2022, month=1, day=1).timestamp() * 1000,\n",
    "        datetime(year=2022, month=11, day=1).timestamp() * 1000,\n",
    "        size=n,\n",
    "    )\n",
    "    occurred_time = pd.to_datetime(occurred_time_ticks, unit=\"ms\").strftime(\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "    )\n",
    "    person_id = rng.integers(n // 10, size=n)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"AccountId\": account_id,\n",
    "            \"ApplicationId\": application_id,\n",
    "            \"ModelId\": model_id,\n",
    "            \"DefinitionId\": definition_id,\n",
    "            \"OccurredTimeTicks\": occurred_time_ticks,\n",
    "            \"OccurredTime\": occurred_time,\n",
    "            \"PersonId\": person_id,\n",
    "        }\n",
    "    )\n",
    "    return json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "\n",
    "generate_n_rows_for_training_data(100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eebe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_kafka_integration(tester):\n",
    "    msg_count = 1000\n",
    "    seed = 42\n",
    "\n",
    "    mtr = ModelTrainingRequest(\n",
    "        AccountId=1000,\n",
    "        ApplicationId=\"DriverApp\",\n",
    "        model_type=\"churn\",\n",
    "        ModelId=\"ChurnModelForDrivers\",\n",
    "        total_no_of_records=msg_count,\n",
    "    )\n",
    "\n",
    "    await tester.to_infobip_start_training_data(mtr)\n",
    "\n",
    "    training_data = generate_n_rows_for_training_data(msg_count, seed=seed)\n",
    "    sanitized_print(\"Starting test production\")\n",
    "    for i in range(msg_count):\n",
    "        await tester.to_infobip_training_data(EventData(**training_data[i]))\n",
    "    sanitized_print(\"Stopping test production\")\n",
    "\n",
    "    sanitized_print(\"Starting test consumption\")\n",
    "    await tester.awaited_mocks.on_infobip_training_data_status.assert_awaited_with(\n",
    "        TrainingDataStatus(\n",
    "            AccountId=1000,\n",
    "            ApplicationId=\"DriverApp\",\n",
    "            ModelId=\"ChurnModelForDrivers\",\n",
    "            no_of_records=999,\n",
    "            total_no_of_records=msg_count,\n",
    "        ),\n",
    "        timeout=5 * 60,\n",
    "    )\n",
    "\n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(select(User).where(User.username == \"infobip\")).one()\n",
    "\n",
    "        display(f\"All events for account id {1000}\")\n",
    "        all_events = session.exec(\n",
    "            select(TrainingStreamStatus)\n",
    "            .where(TrainingStreamStatus.user == user)\n",
    "            .where(TrainingStreamStatus.account_id == 1000)\n",
    "        )\n",
    "        display([e for e in all_events])\n",
    "\n",
    "\n",
    "create_user_for_testing(username=\"infobip\")\n",
    "create_topics_for_user(username=\"infobip\")\n",
    "with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "    with MonkeyPatch.context() as monkeypatch:\n",
    "        monkeypatch.setattr(\n",
    "            \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "            lambda account_ids: pd.DataFrame(\n",
    "                {\n",
    "                    \"curr_count\": [999],\n",
    "                    \"AccountId\": [1000],\n",
    "                    \"curr_check_on\": [datetime.utcnow()],\n",
    "                }\n",
    "            ).set_index(\"AccountId\"),\n",
    "        )\n",
    "        app, fastkafka_app = create_ws_server(assets_path=Path(\"../assets\"))\n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=6009, log_level=\"debug\")\n",
    "\n",
    "        async with Tester(fastkafka_app) as tester:\n",
    "            # Server started.\n",
    "            sanitized_print(\"server started\")\n",
    "\n",
    "            await test_kafka_integration(tester)\n",
    "\n",
    "        sanitized_print(\"server stopped\")\n",
    "        # Server stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# patching async.run so we can run FastAPI within notebook (Jupyter started its own processing loop already)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = None\n",
    "\n",
    "\n",
    "def start_fastapi_server(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    "    host: str = \"0.0.0.0\",\n",
    "    port: int = 6006,\n",
    "    test_function: Optional[Callable[[], Any]] = None,\n",
    "):\n",
    "    app, fastkafka_app = create_fastapi_app(\n",
    "        assets_path=assets_path,\n",
    "    )\n",
    "\n",
    "    if test_function is not None:\n",
    "\n",
    "        @app.on_event(\"startup\")\n",
    "        async def startup_event():\n",
    "            global task\n",
    "            task = asyncio.create_task(test_function())\n",
    "\n",
    "        @app.on_event(\"shutdown\")\n",
    "        async def shutdown_event():\n",
    "            global task\n",
    "            task.cancel()\n",
    "            await asyncio.wait_for(task, timeout=3)\n",
    "            result = task.result()\n",
    "            display(f\"{result=}\")\n",
    "\n",
    "    uvicorn.run(app, host=host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | notest\n",
    "# | eval: false\n",
    "\n",
    "with MonkeyPatch.context() as monkeypatch:\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "        lambda account_ids: pd.DataFrame(\n",
    "            {\n",
    "                \"curr_count\": [999],\n",
    "                \"AccountId\": [1000],\n",
    "                \"curr_check_on\": [datetime.utcnow()],\n",
    "            }\n",
    "        ).set_index(\"AccountId\"),\n",
    "    )\n",
    "    start_fastapi_server(test_function=test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8e084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
