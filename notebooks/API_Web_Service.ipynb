{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c6d88ccd",
   "metadata": {},
   "source": [
    "---\n",
    "description: REST API Routes to import, train, predict events data\n",
    "output-file: api_web_service.html\n",
    "title: REST Server\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32821794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "23-01-12 13:20:12.074 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "from os import environ\n",
    "\n",
    "from aiokafka.helpers import create_ssl_context\n",
    "from fastapi import Request, FastAPI\n",
    "from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html\n",
    "from fastapi.openapi.utils import get_openapi\n",
    "from fastapi.responses import FileResponse, RedirectResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fast_kafka_api.application import FastKafkaAPI\n",
    "from pydantic import validator, BaseModel, Field, HttpUrl, EmailStr, NonNegativeInt\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service\n",
    "from airt_service.sanitizer import sanitized_print\n",
    "from airt_service.auth import auth_router\n",
    "from airt_service.confluent import aio_kafka_config\n",
    "from airt_service.data.datablob import datablob_router\n",
    "from airt_service.data.datasource import datasource_router\n",
    "from airt_service.db.models import get_session_with_context, User, TrainingStreamStatus\n",
    "from airt_service.model.train import model_train_router\n",
    "from airt_service.model.prediction import model_prediction_router\n",
    "from airt_service.training_status_process import process_training_status\n",
    "from airt_service.users import user_router\n",
    "from airt.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb446218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uvicorn\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from fastapi.testclient import TestClient\n",
    "from _pytest.monkeypatch import MonkeyPatch\n",
    "from starlette.datastructures import Headers\n",
    "\n",
    "from airt_service.confluent import confluent_kafka_config, create_topics_for_user\n",
    "from airt_service.db.models import create_user_for_testing\n",
    "from airt_service.helpers import set_env_variable_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "description = \"\"\"\n",
    "# airt service to import, train and predict events data\n",
    "\n",
    "## Python client\n",
    "\n",
    "To use python library please visit: <a href=\"https://docs.airt.ai\" target=\"_blank\">https://docs.airt.ai</a>\n",
    "\n",
    "## How to use\n",
    "\n",
    "To access the airt service, you must create a developer account. Please fill out the signup form below to get one:\n",
    "\n",
    "[https://bit.ly/3hbXQLY](https://bit.ly/3hbXQLY)\n",
    "\n",
    "Upon successful verification, you will receive the username and password for the developer account to your email.\n",
    "\n",
    "### 0. Authenticate\n",
    "\n",
    "Once you receive the username and password, please authenticate the same by calling the `/token` API. The API \n",
    "will return a bearer token if the authentication is successful.\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/token' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/x-www-form-urlencoded' \\\n",
    "  -d 'grant_type=&username=<username>&password=<password>&scope=&client_id=&client_secret='\n",
    "```\n",
    "\n",
    "You can either use the above bearer token or create additional apikey's for accessing the rest of the API's. \n",
    "\n",
    "To create additional apikey's, please call the `/apikey` API by passing the bearer token along with the \n",
    "details of the new apikey in the request. e.g:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/apikey' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"name\": \"<apikey_name>\",\n",
    "  \"expiry\": \"<datetime_in_ISO_8601_format>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "### 1. Connect data\n",
    "\n",
    "Establishing the connection with the data source is a two-step process. The first step allows \n",
    "you to pull the data into airt servers and the second step allows you to perform necessary data \n",
    "pre-processing that are required model training.\n",
    "\n",
    "Currently, we support importing data from:\n",
    "\n",
    "- files stored in the AWS S3 bucket,\n",
    "- databases like MySql, ClickHouse, and \n",
    "- local CSV/Parquet files,\n",
    "\n",
    "We plan to support other databases and storage medium in the future.\n",
    "\n",
    "To pull the data from a S3 bucket, please call the `/from_s3` API\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/datablob/from_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\",\n",
    "  \"access_key\": \"<access_key>\",\n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  \"tag\": \"<tag_name>\"\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start importing the data in the background. This may take a while to complete depending on the size of the data.\n",
    "\n",
    "You can also check the data importing progress by calling the `/datablob/<datablob_id>` API\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/datablob/<datablob_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "Once the data import is completed, you can either call `/from_csv` or `/from_parquet` API for data pre-processing. Below is an \n",
    "example to pre-process an imported CSV data.\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "'https://api.airt.ai/datablob/<datablob_id>/from_csv' \\\n",
    "-H 'accept: application/json' \\\n",
    "-H 'Authorization: Bearer <bearer_token>' \\\n",
    "-H 'Content-Type: application/json' \\\n",
    "-d '{\n",
    "  \"deduplicate_data\": <deduplicate_data>,\n",
    "  \"index_column\": \"<index_column>\",\n",
    "  \"sort_by\": \"<sort_by>\",\n",
    "  \"blocksize\": \"<block_size>\",\n",
    "  \"kwargs\": {}\n",
    "}'\n",
    "```\n",
    "\n",
    "### 2. Train\n",
    "\n",
    "For model training, we assume the input data includes the following:\n",
    "\n",
    "- a column identifying a client client_column (person, car, business, etc.),\n",
    "- a column specifying a type of event we will try to predict target_column (buy, checkout, click on form submit, etc.), and\n",
    "- a timestamp column specifying the time of an occurred event.\n",
    "\n",
    "The input data can have additional features of any type and will be used to make predictions more accurate. Finally, we need to \n",
    "know how much ahead we wish to make predictions. Please use the parameter predict_after to specify the period based on your needs.\n",
    "\n",
    "In the following example, we will train a model to predict which users will perform a purchase event (*purchase) 3 hours before they acctually do it:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/train' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>,\n",
    "  \"client_column\": \"<client_column>\",\n",
    "  \"target_column\": \"<target_column>\",\n",
    "  \"target\": \"*checkout\",\n",
    "  \"predict_after\": 10800\n",
    "}'\n",
    "```\n",
    "\n",
    "Calling the above API will start the model training in the background. This may take a while to complete and you can check the \n",
    "training progress by calling the `/model/<model_id>` API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "After training is complete, you can check the quality of the model by calling the `/model/<model_id>/evaluate` API. This API \n",
    "will return model validation metrics like model accuracy, precision and recall.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/model/<model_id>/evaluate' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "### 3. Predict\n",
    "\n",
    "Finally, you can run the predictions by calling the /model/<model_id>/predict API:\n",
    "\n",
    "```console\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/model/<model_id>/predict' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"data_id\": <datasource_id>\n",
    "}'\n",
    "```\n",
    "Calling the above API will start running the model prediction in the background. This may take a while to complete and you can check the training progress by calling the /prediction/<prediction_id> API.\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "If the dataset is small, then you can call `/prediction/<prediction_id>/pandas` to get prediction results as a pandas dataframe convertible json format:\n",
    "\n",
    "```console\n",
    "curl -X 'GET' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/pandas' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Authorization: Bearer <bearer_token>'\n",
    "```\n",
    "\n",
    "In many cases, it's much better to push the prediction results to remote destinations. Currently, we support pushing the prediction results to a AWS S3 bucket, MySql database and download to the local machine.\n",
    "\n",
    "To push the predictions to a S3 bucket, please call the `/prediction/<prediction_id>/to_s3` API\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "  'https://api.airt.ai/prediction/<prediction_id>/to_s3' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"uri\": \"s3://bucket/folder\", \n",
    "  \"access_key\": \"<access_key>\", \n",
    "  \"secret_key\": \"<secret_key>\",\n",
    "  }'\n",
    "```\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ModelType(str, Enum):\n",
    "    churn = \"churn\"\n",
    "    propensity_to_buy = \"propensity_to_buy\"\n",
    "\n",
    "\n",
    "class ModelTrainingRequest(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    ApplicationId: Optional[str] = Field(\n",
    "        default=None, example=\"TestApplicationId\", description=\"ID of application\"\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "\n",
    "class EventData(BaseModel):\n",
    "    \"\"\"\n",
    "    A sequence of events for a fixed account_id\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    Application: Optional[str] = Field(\n",
    "        None,\n",
    "        example=\"DriverApp\",\n",
    "        description=\"Name of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    DefinitionId: str = Field(\n",
    "        ...,\n",
    "        example=\"appLaunch\",\n",
    "        description=\"name of the event\",\n",
    "        min_length=1,\n",
    "    )\n",
    "    OccurredTime: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"local time of the event\",\n",
    "    )\n",
    "    OccurredTimeTicks: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1616891648496,\n",
    "        description=\"local time of the event as the number of ticks\",\n",
    "    )\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class RealtimeData(BaseModel):\n",
    "    event_data: EventData = Field(\n",
    "        ...,\n",
    "        example=dict(\n",
    "            AccountId=202020,\n",
    "            Application=\"DriverApp\",\n",
    "            DefinitionId=\"appLaunch\",\n",
    "            OccurredTime=\"2021-03-28T00:34:08\",\n",
    "            OccurredTimeTicks=1616891648496,\n",
    "            PersonId=12345678,\n",
    "        ),\n",
    "        description=\"realtime event data\",\n",
    "    )\n",
    "    make_prediction: bool = Field(\n",
    "        ..., example=True, description=\"trigger prediction message in prediction topic\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TrainingDataStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=12_345,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    total_no_of_records: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of records (rows) to be ingested\",\n",
    "    )\n",
    "\n",
    "\n",
    "class TrainingModelStatus(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    current_step: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=0,\n",
    "        description=\"number of records (rows) ingested\",\n",
    "    )\n",
    "    current_step_percentage: float = Field(\n",
    "        ...,\n",
    "        example=0.21,\n",
    "        description=\"the percentage of the current step completed\",\n",
    "    )\n",
    "    total_no_of_steps: NonNegativeInt = Field(\n",
    "        ...,\n",
    "        example=1_000_000,\n",
    "        description=\"total number of steps for training the model\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ModelMetrics(BaseModel):\n",
    "    \"\"\"The standard metrics for classification models.\n",
    "\n",
    "    The most important metrics is AUC for unbalanced classes such as churn. Metrics such as\n",
    "    accuracy are not very useful since they are easily maximized by outputting the most common\n",
    "    class all the time.\n",
    "    \"\"\"\n",
    "\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    Application: Optional[str] = Field(\n",
    "        None,\n",
    "        example=\"DriverApp\",\n",
    "        description=\"Name of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    timestamp: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time when the model was trained\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "    auc: float = Field(\n",
    "        ..., example=0.91, description=\"Area under ROC curve\", ge=0.0, le=1.0\n",
    "    )\n",
    "    f1: float = Field(..., example=0.89, description=\"F-1 score\", ge=0.0, le=1.0)\n",
    "    precission: float = Field(\n",
    "        ..., example=0.84, description=\"precission\", ge=0.0, le=1.0\n",
    "    )\n",
    "    recall: float = Field(..., example=0.82, description=\"recall\", ge=0.0, le=1.0)\n",
    "    accuracy: float = Field(..., example=0.82, description=\"accuracy\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    AccountId: NonNegativeInt = Field(\n",
    "        ..., example=202020, description=\"ID of an account\"\n",
    "    )\n",
    "    Application: Optional[str] = Field(\n",
    "        None,\n",
    "        example=\"DriverApp\",\n",
    "        description=\"Name of the application in case there is more than one for the AccountId\",\n",
    "    )\n",
    "    PersonId: NonNegativeInt = Field(\n",
    "        ..., example=12345678, description=\"ID of a person\"\n",
    "    )\n",
    "    prediction_time: datetime = Field(\n",
    "        ...,\n",
    "        example=\"2021-03-28T00:34:08\",\n",
    "        description=\"UTC time of prediction\",\n",
    "    )\n",
    "    model_type: ModelType = Field(\n",
    "        ...,\n",
    "        example=\"churn\",\n",
    "        description=\"Name of the model used (churn, propensity to buy)\",\n",
    "    )\n",
    "    score: float = Field(\n",
    "        ...,\n",
    "        example=0.4321,\n",
    "        description=\"Prediction score (e.g. the probability of churn in the next 28 days)\",\n",
    "        ge=0.0,\n",
    "        le=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_total_no_of_records = 1000000\n",
    "_no_of_records_received = 0\n",
    "\n",
    "\n",
    "_to_infobip_training_data_status= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0789e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def create_ws_server(\n",
    "    assets_path: Path = Path(\"./assets\"),\n",
    "    start_process_for_username: Optional[str] = \"infobip\"\n",
    ") -> Tuple[FastAPI, FastKafkaAPI]:\n",
    "    \"\"\"Create a FastKafkaAPI based web service\n",
    "\n",
    "    Args:\n",
    "        assets_path: Path to assets (should include favicon.ico)\n",
    "\n",
    "    Returns:\n",
    "        A FastKafkaAPI server\n",
    "    \"\"\"\n",
    "    global description\n",
    "    title = \"airt service\"\n",
    "    version = airt_service.__version__\n",
    "    contact = dict(name=\"airt.ai\", url=\"https://airt.ai\", email=\"info@airt.ai\")\n",
    "    openapi_url = \"/openapi.json\"\n",
    "    favicon_url = \"/assets/images/favicon.ico\"\n",
    "    assets_path = assets_path.resolve()\n",
    "    favicon_path = assets_path / \"images/favicon.ico\"\n",
    "\n",
    "    app = FastAPI(\n",
    "        title=title,\n",
    "        description=description,\n",
    "        version=version,\n",
    "        docs_url=None,\n",
    "        redoc_url=None,\n",
    "    )\n",
    "    app.mount(\"/assets\", StaticFiles(directory=assets_path), name=\"assets\")  # type: ignore\n",
    "\n",
    "    # attaches /token to routes\n",
    "    app.include_router(auth_router)\n",
    "\n",
    "    # attaches /datablob/* to routes\n",
    "    app.include_router(datablob_router)\n",
    "\n",
    "    # attaches /datasource/* to routes\n",
    "    app.include_router(datasource_router)\n",
    "\n",
    "    # attaches /model/* to routes\n",
    "    app.include_router(model_train_router)\n",
    "\n",
    "    # attaches /prediction/* to routes\n",
    "    app.include_router(model_prediction_router)\n",
    "\n",
    "    # attaches /user/* to routes\n",
    "    app.include_router(user_router)\n",
    "\n",
    "    @app.middleware(\"http\")\n",
    "    async def add_nosniff_x_content_type_options_header(request: Request, call_next):\n",
    "        response = await call_next(request)\n",
    "        response.headers[\"X-Content-Type-Options\"] = \"nosniff\"\n",
    "        response.headers[\"Strict-Transport-Security\"] = \"max-age=31536000\"\n",
    "        return response\n",
    "\n",
    "    @app.get(\"/version\")\n",
    "    def get_versions():\n",
    "        return {\"airt_service\": airt_service.__version__}\n",
    "\n",
    "    #     @app.get(\"/\", include_in_schema=False)\n",
    "    #     def redirect_root():\n",
    "    #         return RedirectResponse(\"/docs\")\n",
    "\n",
    "    @app.get(\"/docs\", include_in_schema=False)\n",
    "    def overridden_swagger():\n",
    "        return get_swagger_ui_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            swagger_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/redoc\", include_in_schema=False)\n",
    "    def overridden_redoc():\n",
    "        return get_redoc_html(\n",
    "            openapi_url=openapi_url,\n",
    "            title=title,\n",
    "            redoc_favicon_url=favicon_url,\n",
    "        )\n",
    "\n",
    "    @app.get(\"/favicon.ico\", include_in_schema=False)\n",
    "    async def serve_favicon():\n",
    "        return FileResponse(favicon_path)\n",
    "\n",
    "    def custom_openapi():\n",
    "        if app.openapi_schema:\n",
    "            return app.openapi_schema\n",
    "\n",
    "        fastapi_schema = get_openapi(\n",
    "            title=title,\n",
    "            description=description,\n",
    "            version=version,\n",
    "            routes=app.routes,\n",
    "        )\n",
    "\n",
    "        # ToDo: Figure out recursive dict merge\n",
    "        fastapi_schema[\"servers\"] = [\n",
    "            {\n",
    "                \"url\": \"http://0.0.0.0:6006\"\n",
    "                if (\n",
    "                    environ[\"DOMAIN\"] == \"localhost\"\n",
    "                    or \"airt-service\" in environ[\"DOMAIN\"]\n",
    "                )\n",
    "                else f\"https://{environ['DOMAIN']}\",\n",
    "                \"description\": \"Server\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        app.openapi_schema = fastapi_schema\n",
    "        return app.openapi_schema\n",
    "\n",
    "    app.openapi = custom_openapi  # type: ignore\n",
    "\n",
    "    kafka_brokers = {\n",
    "        \"localhost\": {\n",
    "            \"url\": \"kafka\",\n",
    "            \"description\": \"local development kafka\",\n",
    "            \"port\": 9092,\n",
    "        },\n",
    "        \"staging\": {\n",
    "            \"url\": \"kafka.staging.airt.ai\",\n",
    "            \"description\": \"staging kafka\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "        \"production\": {\n",
    "            \"url\": \"kafka.airt.ai\",\n",
    "            \"description\": \"production kafka\",\n",
    "            \"port\": 9092,\n",
    "            \"protocol\": \"kafka-secure\",\n",
    "            \"security\": {\"type\": \"plain\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    logger.info(f\"kafka_config={aio_kafka_config}\")\n",
    "\n",
    "    fast_kafka_api_app = FastKafkaAPI(\n",
    "        fast_api_app=app,\n",
    "        title=\"airt service kafka api\",\n",
    "        description=\"kafka api for airt service\",\n",
    "        kafka_brokers=kafka_brokers,\n",
    "        version=version,\n",
    "        contact=contact,\n",
    "        **aio_kafka_config,\n",
    "    )\n",
    "\n",
    "    @fast_kafka_api_app.consumes()  # type: ignore\n",
    "    async def on_infobip_start_training_data(msg: ModelTrainingRequest):\n",
    "        with get_session_with_context() as session:\n",
    "            user = session.exec(select(User).where(User.username == start_process_for_username)).one()\n",
    "            start_event = TrainingStreamStatus(\n",
    "                event=\"start\",\n",
    "                account_id=msg.AccountId,\n",
    "                count=0,\n",
    "                total=msg.total_no_of_records,\n",
    "                user=user,\n",
    "            )\n",
    "            session.add(start_event)\n",
    "            session.commit()\n",
    "\n",
    "    @fast_kafka_api_app.consumes()  # type: ignore\n",
    "    async def on_infobip_training_data(msg: EventData):\n",
    "        # ToDo: this is not showing up in logs\n",
    "        logger.debug(f\"msg={msg}\")\n",
    "#         global _total_no_of_records\n",
    "#         global _no_of_records_received\n",
    "#         _no_of_records_received = _no_of_records_received + 1\n",
    "\n",
    "#         if _no_of_records_received % 100 == 0:\n",
    "#             training_data_status = TrainingDataStatus(\n",
    "#                 AccountId=msg.AccountId,\n",
    "#                 no_of_records=_no_of_records_received,\n",
    "#                 total_no_of_records=_total_no_of_records,\n",
    "#             )\n",
    "#             await to_infobip_training_data_status(msg=training_data_status)\n",
    "\n",
    "    @fast_kafka_api_app.consumes()  # type: ignore\n",
    "    async def on_infobip_realtime_data(msg: RealtimeData):\n",
    "        pass\n",
    "\n",
    "    @fast_kafka_api_app.produces()  # type: ignore\n",
    "    async def to_infobip_training_data_status(\n",
    "        account_id: int,\n",
    "        no_of_records: int,\n",
    "        total_no_of_records: int,\n",
    "    ) -> TrainingDataStatus:\n",
    "        logger.debug(f\"on_infobip_training_data_status({account_id=}, {no_of_records=}, {total_no_of_records=})\")\n",
    "        msg = TrainingDataStatus(\n",
    "            AccountId=account_id,\n",
    "            no_of_records=no_of_records,\n",
    "            total_no_of_records=total_no_of_records,\n",
    "        )\n",
    "        return msg\n",
    "\n",
    "    @fast_kafka_api_app.produces()  # type: ignore\n",
    "    async def to_infobip_training_model_status(msg: str) -> TrainingModelStatus:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return TrainingModelStatus()\n",
    "\n",
    "    @fast_kafka_api_app.produces()  # type: ignore\n",
    "    async def to_infobip_model_metrics(msg: ModelMetrics) -> ModelMetrics:\n",
    "        logger.debug(f\"on_infobip_training_model_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    @fast_kafka_api_app.produces()  # type: ignore\n",
    "    async def to_infobip_prediction(msg: Prediction) -> Prediction:\n",
    "        logger.debug(f\"on_infobip_realtime_data_status(msg={msg})\")\n",
    "        return msg\n",
    "\n",
    "    fast_kafka_api_app.to_infobip_training_data_status = to_infobip_training_data_status\n",
    "    if start_process_for_username is not None:\n",
    "        @fast_kafka_api_app.run_in_background()\n",
    "        async def startup_event():\n",
    "#             _to_infobip_training_data_status = to_infobip_training_data_status\n",
    "#             nonlocal to_infobip_training_data_status\n",
    "            await process_training_status(username=start_process_for_username, fast_kafka_api_app=fast_kafka_api_app)\n",
    "    \n",
    "    return app, fast_kafka_api_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d89806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fastapi_app(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    ") -> Tuple[FastAPI, FastKafkaAPI]:\n",
    "    assets_path = assets_path.resolve()\n",
    "    app, fast_kafka_api_app = create_ws_server(assets_path=assets_path)\n",
    "    return app, fast_kafka_api_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804db4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:13.002 [INFO] __main__: kafka_config={'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'group_id': 'kumaran-airt-service-kafka-1:9092_group', 'auto_offset_reset': 'earliest'}\n"
     ]
    }
   ],
   "source": [
    "app, fast_kafka_api_app = create_fastapi_app()\n",
    "client = TestClient(app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b855f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqb2huZG9lIiwiZXhwIjoxNjg5MDgxNjEzfQ.W71nqs73Ql-P9c3zo20eq6TaJ61ObCqmr47Q0-ix7NE',\n",
       " 'token_type': 'bearer'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = \"johndoe\"\n",
    "oauth_data = dict(\n",
    "    username=test_username, password=environ[\"AIRT_SERVICE_SUPER_USER_PASSWORD\"]\n",
    ")\n",
    "\n",
    "response = client.post(\"/token\", data=oauth_data)\n",
    "actual = response.json()\n",
    "display(actual)\n",
    "assert \"access_token\" in actual\n",
    "assert actual[\"token_type\"] == \"bearer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa1162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "*** task canceled ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "\n",
    "async def test_function():\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        while True:\n",
    "            try:\n",
    "                await client.get(\"http://0.0.0.0:6006/docs\")\n",
    "                sanitized_print(\"docs retrieved\")\n",
    "            except httpx.ConnectError:\n",
    "                sanitized_print(\"-\", end=\"\")\n",
    "            except httpx.TimeoutException:\n",
    "                sanitized_print(\".\", end=\"\")\n",
    "            except Exception as e:\n",
    "                sanitized_print(\"?\", end=\"\")\n",
    "                sanitized_print(e)\n",
    "                raise e\n",
    "            try:\n",
    "                await asyncio.sleep(1)\n",
    "            except asyncio.CancelledError:\n",
    "                sanitized_print(\"\\n*** task canceled ***\")\n",
    "                return \"ok\"\n",
    "\n",
    "\n",
    "task = asyncio.create_task(test_function())\n",
    "await asyncio.sleep(3)\n",
    "task.cancel()\n",
    "await asyncio.wait_for(task, timeout=2)\n",
    "task.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992723d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AccountId': 1000,\n",
       " 'Application': 'COD',\n",
       " 'DefinitionId': 'sign_in',\n",
       " 'OccurredTimeTicks': 1649146037462,\n",
       " 'OccurredTime': '2022-04-05T08:07:17.462000',\n",
       " 'PersonId': 4}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "definitions = [\n",
    "    \"appLaunch\",\n",
    "    \"sign_in\",\n",
    "    \"sign_out\",\n",
    "    \"add_to_cart\",\n",
    "    \"purchase\",\n",
    "    \"custom_event_1\",\n",
    "    \"custom_event_2\",\n",
    "    \"custom_event_3\",\n",
    "]\n",
    "\n",
    "\n",
    "applications = [\"DriverApp\", \"PUBG\", \"COD\"]\n",
    "\n",
    "def generate_n_rows_for_training_data(n: int, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "#     account_id = rng.choice([4000, 5000, 500], size=n)\n",
    "    account_id = 1000\n",
    "    definition_id = rng.choice(definitions, size=n)\n",
    "    application = rng.choice(applications, size=n)\n",
    "    occurred_time_ticks = rng.integers(\n",
    "        datetime(year=2022, month=1, day=1).timestamp() * 1000,\n",
    "        datetime(year=2022, month=11, day=1).timestamp() * 1000,\n",
    "        size=n\n",
    "    )\n",
    "    occurred_time = pd.to_datetime(occurred_time_ticks, unit=\"ms\").strftime(\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f\"\n",
    "    )\n",
    "    person_id = rng.integers(n//10, size=n)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"AccountId\": account_id,\n",
    "            \"Application\": application,\n",
    "            \"DefinitionId\": definition_id,\n",
    "            \"OccurredTimeTicks\": occurred_time_ticks,\n",
    "            \"OccurredTime\": occurred_time,\n",
    "            \"PersonId\": person_id\n",
    "        }\n",
    "    )\n",
    "    return json.loads(df.to_json(orient=\"records\"))\n",
    "\n",
    "generate_n_rows_for_training_data(100)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eebe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:16.801 [INFO] __main__: kafka_config={'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'group_id': 'kumaran-airt-service-kafka-1:9092_group', 'auto_offset_reset': 'earliest'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1673529616.600|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1673529616.600|CONFWARN|rdkafka#producer-1| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n",
      "INFO:     Started server process [4483]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:17.084 [INFO] fast_kafka_api._components.asyncapi: Keeping the old async specifications at: 'asyncapi/spec/asyncapi.yml'\n",
      "23-01-12 13:20:17.085 [INFO] fast_kafka_api._components.asyncapi: Skipping generating async documentation in '/work/airt-service/notebooks/asyncapi/docs'\n",
      "23-01-12 13:20:17.086 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:17.094 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:17.101 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:17.110 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:17.118 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:17.119 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:17.119 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:17.120 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:17.121 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:17.121 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:17.122 [INFO] airt_service.training_status_process: Starting the process loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:6006 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server started\n",
      "Starting test production\n",
      "23-01-12 13:20:17.177 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:17.178 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training_data'})\n",
      "23-01-12 13:20:17.179 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training_data'}\n",
      "23-01-12 13:20:17.180 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:17.181 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:17.182 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data'})\n",
      "23-01-12 13:20:17.182 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data'}\n",
      "23-01-12 13:20:17.183 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:17.184 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:17.184 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_realtime_data'})\n",
      "23-01-12 13:20:17.184 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_realtime_data'}\n",
      "23-01-12 13:20:17.185 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:17.196 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.197 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.197 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.200 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.201 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.201 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.202 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.203 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:17.203 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%4|1673529617.128|CONFWARN|rdkafka#producer-2| [thrd:app]: Configuration property group.id is a consumer property and will be ignored by this producer instance\n",
      "%4|1673529617.128|CONFWARN|rdkafka#producer-2| [thrd:app]: Configuration property auto.offset.reset is a consumer property and will be ignored by this producer instance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping test production\n",
      "Starting test consumption\n",
      "23-01-12 13:20:21.200 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:22.214 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "empty message\n",
      "23-01-12 13:20:26.238 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:28.258 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "empty message\n",
      "23-01-12 13:20:32.281 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "empty message\n",
      "23-01-12 13:20:36.299 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:37.314 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:39.329 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "empty message\n",
      "23-01-12 13:20:43.345 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:44.362 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "empty message\n",
      "23-01-12 13:20:48.379 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:51.416 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:53.429 [INFO] airt_service.training_status_process: Starting the process loop\n",
      "23-01-12 13:20:53.738 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 17) with member_id aiokafka-0.8.0-bfa28dcf-818c-40a0-b5b6-73a21a69d2ea\n",
      "23-01-12 13:20:53.739 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-01-12 13:20:53.740 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 17) with member_id aiokafka-0.8.0-48054c8d-c0cf-449b-843d-2b002a48dcd5\n",
      "23-01-12 13:20:53.741 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 17) with member_id aiokafka-0.8.0-9205d063-c585-452b-8d07-9d6c392bebe3\n",
      "23-01-12 13:20:53.742 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_realtime_data': 1, 'infobip_training_data_status': 6, 'infobip_training_data': 6, 'infobip_start_training_data': 6}. \n",
      "23-01-12 13:20:53.744 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 17\n",
      "23-01-12 13:20:53.745 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_realtime_data', partition=0)} for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:53.746 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 17\n",
      "Received message: {\"AccountId\": 6000, \"no_of_records\": 999, \"total_no_of_records\": 1000}\n",
      "23-01-12 13:20:53.747 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_training_data', partition=1), TopicPartition(topic='infobip_training_data', partition=2), TopicPartition(topic='infobip_training_data', partition=5), TopicPartition(topic='infobip_training_data', partition=0), TopicPartition(topic='infobip_training_data', partition=3), TopicPartition(topic='infobip_training_data', partition=4)} for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:53.748 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 17\n",
      "23-01-12 13:20:53.748 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training_data', partition=2), TopicPartition(topic='infobip_start_training_data', partition=4), TopicPartition(topic='infobip_start_training_data', partition=5), TopicPartition(topic='infobip_start_training_data', partition=0), TopicPartition(topic='infobip_start_training_data', partition=3), TopicPartition(topic='infobip_start_training_data', partition=1)} for group kumaran-airt-service-kafka-1:9092_group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'All events for account id 1000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TrainingStreamStatus(id=1, uuid=UUID('cf69a4c6-090e-4660-be04-a92d5aeaf835'), count=0, created=datetime.datetime(2023, 1, 12, 12, 56, 56), account_id=1000, event=<TrainingEvent.start: 'start'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=2, uuid=UUID('bb9139f2-7086-43e5-ba77-24f27f5cc4b9'), count=999, created=datetime.datetime(2023, 1, 12, 12, 56, 57), account_id=1000, event=<TrainingEvent.upload: 'upload'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=3, uuid=UUID('bd81c3e0-6cbe-4762-ba64-0f4f88fcfb4a'), count=999, created=datetime.datetime(2023, 1, 12, 13, 0, 28), account_id=1000, event=<TrainingEvent.end: 'end'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=4, uuid=UUID('76bdad9b-6ebb-4b28-aef3-84da25d04ec1'), count=0, created=datetime.datetime(2023, 1, 12, 13, 6, 8), account_id=1000, event=<TrainingEvent.start: 'start'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=5, uuid=UUID('84d55fed-084a-472c-b91e-fc6b5d1a936d'), count=0, created=datetime.datetime(2023, 1, 12, 13, 6, 8), account_id=1000, event=<TrainingEvent.start: 'start'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=6, uuid=UUID('3064807a-1e9a-42c4-a156-1db0d4f5cf1c'), count=999, created=datetime.datetime(2023, 1, 12, 13, 6, 10), account_id=1000, event=<TrainingEvent.upload: 'upload'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=7, uuid=UUID('405b319b-b06e-42cf-8b26-bb9f3dd8f748'), count=999, created=datetime.datetime(2023, 1, 12, 13, 6, 42), account_id=1000, event=<TrainingEvent.end: 'end'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=29, uuid=UUID('059b85da-fe18-492c-a54f-e423dafc2b14'), count=0, created=datetime.datetime(2023, 1, 12, 13, 19, 7), account_id=1000, event=<TrainingEvent.start: 'start'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=30, uuid=UUID('83fde03a-9d21-42c1-8290-925759540994'), count=999, created=datetime.datetime(2023, 1, 12, 13, 20, 17), account_id=1000, event=<TrainingEvent.upload: 'upload'>, total=1000, user_id=4),\n",
       " TrainingStreamStatus(id=31, uuid=UUID('ee5652e0-0276-4afa-9c77-38719ac13c88'), count=999, created=datetime.datetime(2023, 1, 12, 13, 20, 48), account_id=1000, event=<TrainingEvent.end: 'end'>, total=1000, user_id=4)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:53.965 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:53.966 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:53.967 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-01-12 13:20:54.037 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:54.038 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:54.039 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-01-12 13:20:54.052 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:54.054 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:54.054 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [4483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "server stopped\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/encode/uvicorn/issues/742\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"Called once for each message produced to indicate delivery result.\n",
    "    Triggered by poll() or flush().\"\"\"\n",
    "    if err is not None:\n",
    "        sanitized_print(\"Message delivery failed: {}\".format(err))\n",
    "    else:\n",
    "        #         sanitized_print('Message delivered to {} [{}]'.format(msg.topic(), msg.partition()))\n",
    "        pass\n",
    "\n",
    "\n",
    "def test_kafka_integration():\n",
    "    p = Producer(confluent_kafka_config)\n",
    "    msg_count = 1000\n",
    "    seed = 42\n",
    "\n",
    "    mtr = ModelTrainingRequest(AccountId=1000, total_no_of_records=msg_count)\n",
    "    p.produce(\n",
    "        \"infobip_start_training_data\",\n",
    "        mtr.json().encode(\"utf-8\"),\n",
    "        on_delivery=delivery_report,\n",
    "    )\n",
    "\n",
    "    training_data = generate_n_rows_for_training_data(msg_count, seed=seed)\n",
    "    sanitized_print(\"Starting test production\")\n",
    "    for i in range(msg_count):\n",
    "        p.produce(\n",
    "            \"infobip_training_data\",\n",
    "            json.dumps(training_data[i]).encode(\"utf-8\"),\n",
    "            on_delivery=delivery_report,\n",
    "        )\n",
    "    p.flush()\n",
    "    sanitized_print(\"Stopping test production\")\n",
    "\n",
    "    sanitized_print(\"Starting test consumption\")\n",
    "    c = Consumer(confluent_kafka_config)\n",
    "    c.subscribe([\"infobip_training_data_status\"])\n",
    "\n",
    "    total_consumed = 0\n",
    "\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        msg = c.poll(1.0)\n",
    "        if msg is None:\n",
    "            sanitized_print(\"empty message\")\n",
    "            continue\n",
    "        if msg.error():\n",
    "            sanitized_print(\"Consumer error: {}\".format(msg.error()))\n",
    "            continue\n",
    "        sanitized_print(\"Received message: {}\".format(msg.value().decode(\"utf-8\")))\n",
    "        break\n",
    "#         total_consumed = total_consumed + 1\n",
    "#         if total_consumed >= 5:\n",
    "#             break\n",
    "    c.close()\n",
    "    \n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(\n",
    "            select(User).where(User.username == \"infobip\")\n",
    "        ).one()\n",
    "        \n",
    "        display(f\"All events for account id {1000}\")\n",
    "        all_events = session.exec(\n",
    "            select(TrainingStreamStatus)\n",
    "            .where(TrainingStreamStatus.user == user)\n",
    "            .where(TrainingStreamStatus.account_id == 1000)\n",
    "        )\n",
    "        display([e for e in all_events])\n",
    "\n",
    "\n",
    "class Server(uvicorn.Server):\n",
    "    def install_signal_handlers(self):\n",
    "        pass\n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def run_in_thread(self):\n",
    "        thread = threading.Thread(target=self.run)\n",
    "        thread.start()\n",
    "        try:\n",
    "            while not self.started:\n",
    "                time.sleep(1e-3)\n",
    "            yield\n",
    "        finally:\n",
    "            self.should_exit = True\n",
    "            thread.join()\n",
    "\n",
    "\n",
    "create_user_for_testing(username=\"infobip\")\n",
    "create_topics_for_user(username=\"infobip\")\n",
    "with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "    with MonkeyPatch.context() as monkeypatch:\n",
    "        monkeypatch.setattr(\n",
    "            \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "            lambda account_id: 999,\n",
    "        )\n",
    "        app, fast_kafka_api_app = create_ws_server(assets_path=Path(\"../assets\"))\n",
    "        config = uvicorn.Config(app, host=\"127.0.0.1\", port=6010, log_level=\"debug\")\n",
    "        server = Server(config=config)\n",
    "\n",
    "        with server.run_in_thread():\n",
    "            # Server started.\n",
    "            sanitized_print(\"server started\")\n",
    "\n",
    "            test_kafka_integration()\n",
    "\n",
    "        sanitized_print(\"server stopped\")\n",
    "        # Server stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# patching async.run so we can run FastAPI within notebook (Jupyter started its own processing loop already)\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = None\n",
    "\n",
    "\n",
    "def start_fastapi_server(\n",
    "    assets_path: Path = Path(\"../assets\"),\n",
    "    host: str = \"0.0.0.0\",\n",
    "    port: int = 6006,\n",
    "    test_function: Optional[Callable[[], Any]] = None,\n",
    "):\n",
    "    app, fast_kafka_api_app = create_fastapi_app(\n",
    "        assets_path=assets_path,\n",
    "    )\n",
    "\n",
    "    if test_function is not None:\n",
    "\n",
    "        @app.on_event(\"startup\")\n",
    "        async def startup_event():\n",
    "            global task\n",
    "            task = asyncio.create_task(test_function())\n",
    "\n",
    "        @app.on_event(\"shutdown\")\n",
    "        async def shutdown_event():\n",
    "            global task\n",
    "            task.cancel()\n",
    "            await asyncio.wait_for(task, timeout=3)\n",
    "            result = task.result()\n",
    "            display(f\"{result=}\")\n",
    "\n",
    "    uvicorn.run(app, host=host, port=port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b219816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:54.250 [INFO] __main__: kafka_config={'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'group_id': 'kumaran-airt-service-kafka-1:9092_group', 'auto_offset_reset': 'earliest'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [4483]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-01-12 13:20:54.344 [INFO] fast_kafka_api._components.asyncapi: Keeping the old async specifications at: 'asyncapi/spec/asyncapi.yml'\n",
      "23-01-12 13:20:54.345 [INFO] fast_kafka_api._components.asyncapi: Skipping generating async documentation in '/work/airt-service/notebooks/asyncapi/docs'\n",
      "23-01-12 13:20:54.346 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:54.355 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:54.363 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:54.371 [INFO] fast_kafka_api.application: _create_producer() : created producer using the config: '{'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092'}'\n",
      "23-01-12 13:20:54.380 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:54.380 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:54.381 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:54.382 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:54.383 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n",
      "23-01-12 13:20:54.384 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': 'kumaran-airt-service-kafka-1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100, 'group_id': 'kumaran-airt-service-kafka-1:9092_group'}\n",
      "23-01-12 13:20:54.385 [INFO] airt_service.training_status_process: Starting the process loop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:6006 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Traceback (most recent call last):\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/starlette/routing.py\", line 674, in lifespan\n",
      "    await receive()\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/starlette/routing.py\", line 569, in __aexit__\n",
      "    await self._router.shutdown()\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/starlette/routing.py\", line 658, in shutdown\n",
      "    await handler()\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/fast_kafka_api/application.py\", line 284, in on_shutdown\n",
      "    await app._on_shutdown()\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/fast_kafka_api/application.py\", line 666, in _on_shutdown\n",
      "    await self._shutdown_bg_tasks()\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/fast_kafka_api/application.py\", line 630, in _shutdown_bg_tasks\n",
      "    await self._bg_task_group_generator.__aexit__(None, None, None)  # type: ignore\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 662, in __aexit__\n",
      "    raise exceptions[0]\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 258, in __step\n",
      "    result = coro.throw(exc)\n",
      "  File \"/tmp/ipykernel_4483/1787978027.py\", line 219, in startup_event\n",
      "    await process_training_status(username=start_process_for_username, fast_kafka_api_app=fast_kafka_api_app)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 199, in process_training_status\n",
      "    await process_recent_event(\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 150, in process_recent_event\n",
      "    curr_count = await asyncify(get_count_from_training_data_ch_table)(\n",
      "  File \"/home/kumaran/.local/lib/python3.9/site-packages/asyncer/_main.py\", line 358, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/lib/python3.9/asyncio/futures.py\", line 284, in __await__\n",
      "    yield self  # This tells Task to wait for completion.\n",
      "  File \"/usr/lib/python3.9/asyncio/tasks.py\", line 328, in __wakeup\n",
      "    future.result()\n",
      "  File \"/usr/lib/python3.9/asyncio/futures.py\", line 201, in result\n",
      "    raise self._exception\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/work/airt-service/airt_service/training_status_process.py\", line 119, in get_count_from_training_data_ch_table\n",
      "    username=environ[\"KAFKA_CH_USERNAME\"],\n",
      "  File \"/usr/lib/python3.9/os.py\", line 679, in __getitem__\n",
      "    raise KeyError(key) from None\n",
      "KeyError: 'KAFKA_CH_USERNAME'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs retrieved\n",
      "23-01-12 13:20:54.474 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:54.475 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_start_training_data'})\n",
      "23-01-12 13:20:54.475 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_start_training_data'}\n",
      "23-01-12 13:20:54.476 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:54.477 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:54.478 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_training_data'})\n",
      "23-01-12 13:20:54.478 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_training_data'}\n",
      "23-01-12 13:20:54.479 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:54.480 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n",
      "23-01-12 13:20:54.481 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'infobip_realtime_data'})\n",
      "23-01-12 13:20:54.482 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'infobip_realtime_data'}\n",
      "23-01-12 13:20:54.482 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n",
      "23-01-12 13:20:54.495 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.497 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.498 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.503 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 19) with member_id aiokafka-0.8.0-61efe38c-e392-4b06-9794-3e88d3934875\n",
      "23-01-12 13:20:54.505 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-01-12 13:20:54.507 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.508 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.508 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.511 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 19\n",
      "23-01-12 13:20:54.512 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_start_training_data', partition=2), TopicPartition(topic='infobip_start_training_data', partition=4), TopicPartition(topic='infobip_start_training_data', partition=5), TopicPartition(topic='infobip_start_training_data', partition=0), TopicPartition(topic='infobip_start_training_data', partition=3), TopicPartition(topic='infobip_start_training_data', partition=1)} for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.514 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 1003 for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.515 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.515 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.519 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 20) with member_id aiokafka-0.8.0-34cff171-e209-4077-a675-faafc3cd1f1f\n",
      "23-01-12 13:20:54.520 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n",
      "23-01-12 13:20:54.521 [INFO] aiokafka.consumer.group_coordinator: Joined group 'kumaran-airt-service-kafka-1:9092_group' (generation 20) with member_id aiokafka-0.8.0-b1344471-17ac-420a-8284-ebaa7bdd53ae\n",
      "23-01-12 13:20:54.523 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:54.526 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:54.527 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-01-12 13:20:54.529 [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'infobip_realtime_data': 1, 'infobip_training_data': 6}. \n",
      "23-01-12 13:20:54.533 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 20\n",
      "23-01-12 13:20:54.536 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_training_data', partition=1), TopicPartition(topic='infobip_training_data', partition=2), TopicPartition(topic='infobip_training_data', partition=5), TopicPartition(topic='infobip_training_data', partition=0), TopicPartition(topic='infobip_training_data', partition=3), TopicPartition(topic='infobip_training_data', partition=4)} for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.538 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group kumaran-airt-service-kafka-1:9092_group with generation 20\n",
      "23-01-12 13:20:54.539 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='infobip_realtime_data', partition=0)} for group kumaran-airt-service-kafka-1:9092_group\n",
      "23-01-12 13:20:54.543 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:54.545 [INFO] aiokafka.consumer.group_coordinator: LeaveGroup request succeeded\n",
      "23-01-12 13:20:54.548 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:54.549 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "23-01-12 13:20:54.550 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.\n",
      "23-01-12 13:20:54.551 [INFO] fast_kafka_api._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n",
      "INFO:     127.0.0.1:46886 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "docs retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Finished server process [4483]\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "with MonkeyPatch.context() as monkeypatch:\n",
    "    monkeypatch.setattr(\n",
    "        \"airt_service.training_status_process.get_count_from_training_data_ch_table\",\n",
    "        lambda account_id: 999,\n",
    "    )\n",
    "    start_fastapi_server(test_function=test_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0233f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
