{
 "cells": [
  {
   "cell_type": "raw",
   "id": "63028e24",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for airflow azure batch executor\n",
    "output-file: airflowazurebatchexecutor.html\n",
    "title: Airflow Azure Batch Executor\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp airflow.azure_batch_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb570e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import shlex\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "from azure.batch.batch_auth import SharedKeyCredentials\n",
    "from fastcore.script import call_parse, Param\n",
    "\n",
    "from airt.executor.subcommand import CLICommandBase, ClassCLICommand\n",
    "from airt.helpers import slugify\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "from airt_service.airflow.base_executor import BaseAirflowExecutor, dag_template\n",
    "from airt_service.airflow.utils import trigger_dag, wait_for_run_to_complete\n",
    "from airt_service.azure.batch_utils import get_random_string, BatchPool, BatchJob\n",
    "from airt_service.azure.batch_utils import AUTO_SCALE_FORMULA\n",
    "from airt_service.azure.utils import get_azure_batch_environment_component_names\n",
    "from airt_service.azure.utils import get_batch_account_pool_job_names\n",
    "from airt_service.batch_job import get_environment_vars_for_batch_job\n",
    "from airt_service.helpers import generate_random_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "from airt.executor.subcommand import SimpleCLICommand\n",
    "from airt.testing import activate_by_import\n",
    "from airt_service.airflow.utils import list_dag_runs\n",
    "from airt_service.db.models import create_user_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "def setup_test_paths(td: str) -> Tuple[str, str]:\n",
    "    d = Path(td)\n",
    "    paths = [d / sd for sd in [\"data\", \"model\"]]\n",
    "    print(f\"{paths=}\")\n",
    "\n",
    "    # create tmp dirs for data and model\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RemotePaths: data_path is \"read-only\", while model_path can be used for both reading and writing between calls\n",
    "    return tuple(f\"local:{p}\" for p in paths)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63982167",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "data_path_url, model_path_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de287057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "DEFAULT_EXEC_ENVIRONMENT = \"preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AirflowAzureBatchExecutor(BaseAirflowExecutor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps: List[CLICommandBase],\n",
    "        region: str,\n",
    "        exec_environments: Optional[List[Optional[str]]] = None,\n",
    "        batch_environment_path: Optional[Union[str, Path]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructs a new AirflowAzureBatchExecutor instance\n",
    "\n",
    "        Args:\n",
    "            steps: List of instances of either ClassCLICommand or SimpleCLICommand\n",
    "            region: Region to execute\n",
    "            exec_environments: List of execution environments to execute steps\n",
    "            batch_environment_path: Path for yaml file in which azure batch environment names are stored\n",
    "        \"\"\"\n",
    "        self.region = region\n",
    "        self.batch_environment_path = batch_environment_path\n",
    "\n",
    "        if exec_environments is None:\n",
    "            exec_environments = [DEFAULT_EXEC_ENVIRONMENT] * len(steps)\n",
    "\n",
    "        if len(exec_environments) != len(steps):\n",
    "            raise ValueError(\n",
    "                f\"len(exec_environments)={len(exec_environments)} != len(steps){len(steps)}\"\n",
    "            )\n",
    "\n",
    "        existing_exec_environments = list(\n",
    "            get_azure_batch_environment_component_names(\n",
    "                self.region, self.batch_environment_path\n",
    "            ).keys()\n",
    "        )\n",
    "\n",
    "        self.exec_environments = []\n",
    "        for exec_env in exec_environments:\n",
    "            if exec_env is None:\n",
    "                self.exec_environments.append(DEFAULT_EXEC_ENVIRONMENT)\n",
    "                continue\n",
    "            if exec_env not in existing_exec_environments:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid value {exec_env} given for exec environment; Allowed values are {existing_exec_environments}\"\n",
    "                )\n",
    "            self.exec_environments.append(exec_env)\n",
    "\n",
    "        self.exec_environments = [\n",
    "            exec_env if exec_env is not None else DEFAULT_EXEC_ENVIRONMENT\n",
    "            for exec_env in exec_environments\n",
    "        ]\n",
    "\n",
    "        super(AirflowAzureBatchExecutor, self).__init__(steps)\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        *,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Path, str]:\n",
    "        \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "        Args:\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            A tuple which contains dag file path and run id\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_azure_batch_environment_names(folder: Path):\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                arn: \"random_azure_batch_env_component_name\"\n",
    "                for arn in [\n",
    "                    \"batch_job_name\",\n",
    "                    \"batch_pool_name\",\n",
    "                    \"batch_account_name\",\n",
    "                ]\n",
    "            }\n",
    "            for task in [\"csv_processing\", \"predictions\", \"preprocessing\", \"training\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    folder = Path(folder)\n",
    "    test_batch_environment_path = folder / \"azure_batch_environment.yml\"\n",
    "    with open(test_batch_environment_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    return test_batch_environment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "    ),\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "    display(abe.exec_environments)\n",
    "    assert abe.exec_environments == [\"preprocessing\"] * len(steps)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"preprocessing\"],\n",
    "            batch_environment_path=test_batch_environment_path,\n",
    "        )\n",
    "    display(e)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"gibberish\", \"gibberish\"],\n",
    "            batch_environment_path=test_batch_environment_path,\n",
    "        )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec82535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_step_template(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    step: CLICommandBase,\n",
    "    exec_environment: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create template for step\n",
    "\n",
    "    Args:\n",
    "        step: step to create template\n",
    "        kwargs: keyword arguments for step\n",
    "    Returns:\n",
    "        Template for step\n",
    "    \"\"\"\n",
    "    cli_command = step.to_cli(**kwargs)\n",
    "    task_id = slugify(cli_command)\n",
    "\n",
    "    azure_batch_environment_vars = \"\"\n",
    "    for name, value in get_environment_vars_for_batch_job().items():\n",
    "        azure_batch_environment_vars = (\n",
    "            azure_batch_environment_vars + f\" --env {name}='{value}'\"\n",
    "        )\n",
    "    #     print(f\"{azure_batch_environment_vars=}\")\n",
    "\n",
    "    (\n",
    "        batch_account_name,\n",
    "        batch_pool_name,\n",
    "        batch_job_name,\n",
    "    ) = get_batch_account_pool_job_names(\n",
    "        task=exec_environment,\n",
    "        region=self.region,\n",
    "        batch_environment_path=self.batch_environment_path,\n",
    "    )\n",
    "\n",
    "    if exec_environment in [\"training\", \"predictions\"]:\n",
    "        batch_pool_vm_size = \"standard_nc6s_v3\"\n",
    "    elif exec_environment in [\"csv_processing\", \"preprocessing\"]:\n",
    "        batch_pool_vm_size = \"standard_d2s_v3\"\n",
    "\n",
    "    batch_task_id = f\"batch-task-{get_random_string()}\"\n",
    "    azure_batch_conn_id = f'azure_batch_conn_id=\"custom_azure_batch_default\"'\n",
    "    task_params = f\"\"\"task_id=\"{task_id}\", batch_pool_id=\"{batch_pool_name}\", batch_pool_vm_size=\"{batch_pool_vm_size}\", batch_job_id=\"{batch_job_name}\", batch_task_command_line=\"{cli_command}\", batch_task_id=\"{batch_task_id}\", {azure_batch_conn_id}\"\"\"\n",
    "\n",
    "    vm_details = f\"\"\"vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" \"\"\"\n",
    "\n",
    "    auto_scale_params = (\n",
    "        f'enable_auto_scale=True, auto_scale_formula=\"\"\"{AUTO_SCALE_FORMULA}\"\"\"'\n",
    "    )\n",
    "\n",
    "    tag = \"dev\"\n",
    "    if os.environ[\"DOMAIN\"] == \"api.airt.ai\":\n",
    "        tag = \"latest\"\n",
    "    batch_task_container_settings = f\"\"\"batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"registry.gitlab.com/airt.ai/airt-service:{tag}\", container_run_options=\"{azure_batch_environment_vars}\")\"\"\"\n",
    "\n",
    "    task = f\"\"\"AzureBatchOperator({task_params}, {vm_details}, {auto_scale_params}, {batch_task_container_settings})\"\"\"\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "    actual = abe._create_step_template(\n",
    "        steps[0],\n",
    "        exec_environment=\"training\",\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "    )\n",
    "#     display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag_template(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create DAG template with steps as tasks\n",
    "\n",
    "    Args:\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments to pass to steps' CLI\n",
    "    Returns:\n",
    "        Generated DAG with steps as tasks\n",
    "    \"\"\"\n",
    "    curr_dag_template = dag_template\n",
    "\n",
    "    downstream_tasks = \"\"\n",
    "    newline = \"\\n\"\n",
    "    tab = \" \" * 4\n",
    "\n",
    "    existing_tasks = 0\n",
    "    for i, step in enumerate(self.steps):\n",
    "        if on_step_start is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_start, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "        curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(step, self.exec_environments[i], **kwargs)}\"\"\"  # type: ignore\n",
    "        existing_tasks += 1\n",
    "\n",
    "        if on_step_end is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_end, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "    downstream_tasks = f\"{newline}{tab}\" + \" >> \".join(\n",
    "        [f\"t{i}\" for i in range(1, existing_tasks + 1)]\n",
    "    )\n",
    "    curr_dag_template += downstream_tasks\n",
    "\n",
    "    return curr_dag_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065626",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "\n",
    "    kwargs = {\"data_path_url\": data_path_url, \"model_path_url\": model_path_url}\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "    template = abe._create_dag_template(\n",
    "        on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "    )\n",
    "    display(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f031b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Test case for AirflowAzureBatchExecutor._create_dag\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", None]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_id, dag_file_path = abe._create_dag(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=None,\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5135ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Test case for AirflowAzureBatchExecutor.schedule\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"csv_processing\", \"preprocessing\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_file_path = abe.schedule(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=timedelta(days=7),\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@patch\n",
    "def execute(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    *,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs\n",
    ") -> Tuple[Path, str]:\n",
    "    \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "    Args:\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        A tuple which contains dag file path and run id\n",
    "    \"\"\"\n",
    "    schedule_interval = None\n",
    "    dag_id, dag_file_path = self._create_dag(\n",
    "        schedule_interval=schedule_interval,\n",
    "        description=description,\n",
    "        tags=tags,\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "    return dag_file_path, run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef996a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "# Test case for AirflowAzureBatchExecutor.execute\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", \"predictions\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_file_path, run_id = abe.execute(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(dag_file_path)\n",
    "    display(run_id)\n",
    "\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _test_azure_batch_executor(region: str = \"northeurope\"):  # type: ignore\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "        steps = [\n",
    "            ClassCLICommand(\n",
    "                executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "            )\n",
    "        ]\n",
    "        exec_environments = [\"training\"]\n",
    "\n",
    "        td = Path(d)\n",
    "        created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "        shared_key_credentials = SharedKeyCredentials(\n",
    "            \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "        )\n",
    "\n",
    "        batch_account_name = \"testbatchnortheurope\"\n",
    "        region = \"northeurope\"\n",
    "\n",
    "        batch_pool = BatchPool.from_name(\n",
    "            name=\"test-cpu-pool\",\n",
    "            batch_account_name=batch_account_name,\n",
    "            region=region,\n",
    "            shared_key_credentials=shared_key_credentials,\n",
    "        )\n",
    "        batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "        print(f\"{batch_pool.name=}\")\n",
    "        print(f\"{batch_job.name=}\")\n",
    "        region = \"northeurope\"\n",
    "        test_batch_environment_names = {\n",
    "            region: {\n",
    "                task: {\n",
    "                    \"batch_job_name\": batch_job.name,\n",
    "                    \"batch_pool_name\": batch_pool.name,\n",
    "                    \"batch_account_name\": batch_account_name,\n",
    "                }\n",
    "                for task in [\n",
    "                    \"csv_processing\",\n",
    "                    \"predictions\",\n",
    "                    \"preprocessing\",\n",
    "                    \"training\",\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        print(f\"{test_batch_environment_names=}\")\n",
    "        with open(created_azure_env_path, \"w\") as f:\n",
    "            yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=exec_environments,  # type: ignore\n",
    "            batch_environment_path=created_azure_env_path,\n",
    "        )\n",
    "        dag_file_path, run_id = abe.execute(\n",
    "            data_path_url=data_path_url,\n",
    "            model_path_url=model_path_url,\n",
    "            description=\"test description\",\n",
    "            tags=\"test_tag\",\n",
    "        )\n",
    "\n",
    "        print(f\"{dag_file_path=}\")\n",
    "        print(f\"{run_id=}\")\n",
    "\n",
    "        dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "        state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "        print(f\"{state=}\")\n",
    "        dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def test_azure_batch_executor(region: Param(\"region\", str) = \"northeurope\"):  # type: ignore\n",
    "    \"\"\"\n",
    "    Create throw away environment for azure batch and execute airflow batch executor\n",
    "    \"\"\"\n",
    "    _test_azure_batch_executor(region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3a2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "test_azure_batch_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bda18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
