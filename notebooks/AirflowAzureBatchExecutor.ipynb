{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2276f633",
   "metadata": {},
   "source": [
    "---\n",
    "description: Notebook for airflow azure batch executor\n",
    "output-file: airflowazurebatchexecutor.html\n",
    "title: Airflow Azure Batch Executor\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp airflow.azure_batch_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb570e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-12-19 09:13:17.140 [INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import os\n",
    "import shlex\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import yaml\n",
    "from airt.executor.subcommand import ClassCLICommand, CLICommandBase\n",
    "from airt.helpers import slugify\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "from azure.batch.batch_auth import SharedKeyCredentials\n",
    "from fastcore.script import Param, call_parse\n",
    "\n",
    "from airt_service.airflow.base_executor import BaseAirflowExecutor, dag_template\n",
    "from airt_service.airflow.utils import trigger_dag, wait_for_run_to_complete\n",
    "from airt_service.azure.batch_utils import (\n",
    "    AUTO_SCALE_FORMULA,\n",
    "    BatchJob,\n",
    "    BatchPool,\n",
    "    get_random_string,\n",
    ")\n",
    "from airt_service.azure.utils import (\n",
    "    get_azure_batch_environment_component_names,\n",
    "    get_batch_account_pool_job_names,\n",
    ")\n",
    "from airt_service.batch_job import get_environment_vars_for_batch_job\n",
    "from airt_service.helpers import generate_random_string\n",
    "from airt_service.sanitizer import sanitized_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 09:13:17.778203: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] airt.testing.activate_by_import: Failed to set gpu memory limit for tf; This could happen because of no gpu availability\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from time import sleep\n",
    "\n",
    "import pytest\n",
    "from airt.executor.subcommand import SimpleCLICommand\n",
    "from airt.testing import activate_by_import\n",
    "\n",
    "from airt_service.airflow.utils import list_dag_runs\n",
    "from airt_service.db.models import create_user_for_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e5aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nwlfmrnfuy'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d935f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Module loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a6689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "def setup_test_paths(td: str) -> Tuple[str, str]:\n",
    "    d = Path(td)\n",
    "    paths = [d / sd for sd in [\"data\", \"model\"]]\n",
    "    sanitized_print(f\"{paths=}\")\n",
    "\n",
    "    # create tmp dirs for data and model\n",
    "    for p in paths:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # RemotePaths: data_path is \"read-only\", while model_path can be used for both reading and writing between calls\n",
    "    return tuple(f\"local:{p}\" for p in paths)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63982167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmphb2g1x0o/data'), Path('/tmp/tmphb2g1x0o/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('local:/tmp/tmphb2g1x0o/data', 'local:/tmp/tmphb2g1x0o/model')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "data_path_url, model_path_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de287057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_EXEC_ENVIRONMENT = \"preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799debe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class AirflowAzureBatchExecutor(BaseAirflowExecutor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps: List[CLICommandBase],\n",
    "        region: str,\n",
    "        exec_environments: Optional[List[Optional[str]]] = None,\n",
    "        batch_environment_path: Optional[Union[str, Path]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructs a new AirflowAzureBatchExecutor instance\n",
    "\n",
    "        Args:\n",
    "            steps: List of instances of either ClassCLICommand or SimpleCLICommand\n",
    "            region: Region to execute\n",
    "            exec_environments: List of execution environments to execute steps\n",
    "            batch_environment_path: Path for yaml file in which azure batch environment names are stored\n",
    "        \"\"\"\n",
    "        self.region = region\n",
    "        self.batch_environment_path = batch_environment_path\n",
    "\n",
    "        if exec_environments is None:\n",
    "            exec_environments = [DEFAULT_EXEC_ENVIRONMENT] * len(steps)\n",
    "\n",
    "        if len(exec_environments) != len(steps):\n",
    "            raise ValueError(\n",
    "                f\"len(exec_environments)={len(exec_environments)} != len(steps){len(steps)}\"\n",
    "            )\n",
    "\n",
    "        existing_exec_environments = list(\n",
    "            get_azure_batch_environment_component_names(\n",
    "                self.region, self.batch_environment_path\n",
    "            ).keys()\n",
    "        )\n",
    "\n",
    "        self.exec_environments = []\n",
    "        for exec_env in exec_environments:\n",
    "            if exec_env is None:\n",
    "                self.exec_environments.append(DEFAULT_EXEC_ENVIRONMENT)\n",
    "                continue\n",
    "            if exec_env not in existing_exec_environments:\n",
    "                raise ValueError(\n",
    "                    f\"Invalid value {exec_env} given for exec environment; Allowed values are {existing_exec_environments}\"\n",
    "                )\n",
    "            self.exec_environments.append(exec_env)\n",
    "\n",
    "        self.exec_environments = [\n",
    "            exec_env if exec_env is not None else DEFAULT_EXEC_ENVIRONMENT\n",
    "            for exec_env in exec_environments\n",
    "        ]\n",
    "\n",
    "        super(AirflowAzureBatchExecutor, self).__init__(steps)\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        *,\n",
    "        description: str,\n",
    "        tags: Union[str, List[str]],\n",
    "        on_step_start: Optional[CLICommandBase] = None,\n",
    "        on_step_end: Optional[CLICommandBase] = None,\n",
    "        **kwargs,\n",
    "    ) -> Tuple[Path, str]:\n",
    "        \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "        Args:\n",
    "            description: description of DAG\n",
    "            tags: tags for DAG\n",
    "            on_step_start: CLI to call before executing step/task in DAG\n",
    "            on_step_end: CLI to call after executing step/task in DAG\n",
    "            kwargs: keyword arguments needed for steps/tasks\n",
    "        Returns:\n",
    "            A tuple which contains dag file path and run id\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Need to implement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d1fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_azure_batch_environment_names(folder: Path):\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                arn: \"random_azure_batch_env_component_name\"\n",
    "                for arn in [\n",
    "                    \"batch_job_name\",\n",
    "                    \"batch_pool_name\",\n",
    "                    \"batch_account_name\",\n",
    "                ]\n",
    "            }\n",
    "            for task in [\"csv_processing\", \"predictions\", \"preprocessing\", \"training\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    folder = Path(folder)\n",
    "    test_batch_environment_path = folder / \"azure_batch_environment.yml\"\n",
    "    with open(test_batch_environment_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    return test_batch_environment_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "    ),\n",
    "    ClassCLICommand(\n",
    "        executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85940f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpy8fba06w/data'), Path('/tmp/tmpy8fba06w/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['preprocessing', 'preprocessing']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo ValueError('len(exec_environments)=1 != len(steps)2') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo ValueError(\"Invalid value gibberish given for exec environment; Allowed values are ['csv_processing', 'predictions', 'preprocessing', 'training']\") tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "    display(abe.exec_environments)\n",
    "    assert abe.exec_environments == [\"preprocessing\"] * len(steps)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"preprocessing\"],\n",
    "            batch_environment_path=test_batch_environment_path,\n",
    "        )\n",
    "    display(e)\n",
    "\n",
    "    with pytest.raises(ValueError) as e:\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=[\"gibberish\", \"gibberish\"],\n",
    "            batch_environment_path=test_batch_environment_path,\n",
    "        )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec82535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_step_template(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    step: CLICommandBase,\n",
    "    exec_environment: str,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create template for step\n",
    "\n",
    "    Args:\n",
    "        step: step to create template\n",
    "        kwargs: keyword arguments for step\n",
    "    Returns:\n",
    "        Template for step\n",
    "    \"\"\"\n",
    "    cli_command = step.to_cli(**kwargs)\n",
    "    task_id = slugify(cli_command)\n",
    "\n",
    "    azure_batch_environment_vars = \"\"\n",
    "    for name, value in get_environment_vars_for_batch_job().items():\n",
    "        azure_batch_environment_vars = (\n",
    "            azure_batch_environment_vars + f\" --env {name}='{value}'\"\n",
    "        )\n",
    "\n",
    "    (\n",
    "        batch_account_name,\n",
    "        batch_pool_name,\n",
    "        batch_job_name,\n",
    "    ) = get_batch_account_pool_job_names(\n",
    "        task=exec_environment,\n",
    "        region=self.region,\n",
    "        batch_environment_path=self.batch_environment_path,\n",
    "    )\n",
    "\n",
    "    if exec_environment in [\"training\", \"predictions\"]:\n",
    "        batch_pool_vm_size = \"standard_nc6s_v3\"\n",
    "    elif exec_environment in [\"csv_processing\", \"preprocessing\"]:\n",
    "        batch_pool_vm_size = \"standard_d2s_v3\"\n",
    "\n",
    "    batch_task_id = f\"batch-task-{get_random_string()}\"\n",
    "    azure_batch_conn_id = f'azure_batch_conn_id=\"custom_azure_batch_default\"'\n",
    "    task_params = f\"\"\"task_id=\"{task_id}\", batch_pool_id=\"{batch_pool_name}\", batch_pool_vm_size=\"{batch_pool_vm_size}\", batch_job_id=\"{batch_job_name}\", batch_task_command_line=\"{cli_command}\", batch_task_id=\"{batch_task_id}\", {azure_batch_conn_id}\"\"\"\n",
    "\n",
    "    vm_details = f\"\"\"vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" \"\"\"\n",
    "\n",
    "    auto_scale_params = (\n",
    "        f'enable_auto_scale=True, auto_scale_formula=\"\"\"{AUTO_SCALE_FORMULA}\"\"\"'\n",
    "    )\n",
    "\n",
    "    tag = \"dev\"\n",
    "    if os.environ[\"DOMAIN\"] == \"api.airt.ai\":\n",
    "        tag = \"latest\"\n",
    "    batch_task_container_settings = f\"\"\"batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:{tag}\", container_run_options=\"{azure_batch_environment_vars}\")\"\"\"\n",
    "\n",
    "    task = f\"\"\"AzureBatchOperator({task_params}, {vm_details}, {auto_scale_params}, {batch_task_container_settings})\"\"\"\n",
    "    return task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ef2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpunjk58sw/data'), Path('/tmp/tmpunjk58sw/model')]\n"
     ]
    }
   ],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "    actual = abe._create_step_template(\n",
    "        steps[0],\n",
    "        exec_environment=\"training\",\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "    )\n",
    "#     display(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ce584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_dag_template(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create DAG template with steps as tasks\n",
    "\n",
    "    Args:\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments to pass to steps' CLI\n",
    "    Returns:\n",
    "        Generated DAG with steps as tasks\n",
    "    \"\"\"\n",
    "    curr_dag_template = dag_template\n",
    "\n",
    "    downstream_tasks = \"\"\n",
    "    newline = \"\\n\"\n",
    "    tab = \" \" * 4\n",
    "\n",
    "    existing_tasks = 0\n",
    "    for i, step in enumerate(self.steps):\n",
    "        if on_step_start is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_start, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "        curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(step, self.exec_environments[i], **kwargs)}\"\"\"  # type: ignore\n",
    "        existing_tasks += 1\n",
    "\n",
    "        if on_step_end is not None:\n",
    "            curr_dag_template += f\"\"\"{newline}{tab}t{existing_tasks+1} = {self._create_step_template(on_step_end, self.exec_environments[i], step_count=i+1, **kwargs)}\"\"\"  # type: ignore\n",
    "            existing_tasks += 1\n",
    "\n",
    "    downstream_tasks = f\"{newline}{tab}\" + \" >> \".join(\n",
    "        [f\"t{i}\" for i in range(1, existing_tasks + 1)]\n",
    "    )\n",
    "    curr_dag_template += downstream_tasks\n",
    "\n",
    "    return curr_dag_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35065626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpvyu0jj29/data'), Path('/tmp/tmpvyu0jj29/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import datetime\\nfrom textwrap import dedent\\n\\n# The DAG object; we\\'ll need this to instantiate a DAG\\nfrom airflow import DAG\\n\\n# Operators; we need this to operate!\\nfrom airflow.providers.amazon.aws.operators.batch import BatchOperator\\nimport azure.batch.models as batchmodels\\nfrom airflow.providers.microsoft.azure.operators.batch import AzureBatchOperator\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\\nwith DAG(\\n    \\'{dag_name}\\',\\n    # These args will get passed on to each operator\\n    # You can override them on a per-task basis during operator initialization\\n    default_args={{\\n        \\'schedule_interval\\': {schedule_interval},\\n        \\'depends_on_past\\': False,\\n        \\'email\\': [\\'info@airt.ai\\'],\\n        \\'email_on_failure\\': False,\\n        \\'email_on_retry\\': False,\\n        \\'retries\\': 1,\\n        \\'retry_delay\\': datetime.timedelta(minutes=5),\\n        # \\'queue\\': \\'queue\\',\\n        # \\'pool\\': \\'backfill\\',\\n        # \\'priority_weight\\': 10,\\n        # \\'end_date\\': datetime.datetime(2016, 1, 1),\\n        # \\'wait_for_downstream\\': False,\\n        # \\'sla\\': datetime.timedelta(hours=2),\\n        # \\'execution_timeout\\': datetime.timedelta(seconds=300),\\n        # \\'on_failure_callback\\': some_function,\\n        # \\'on_success_callback\\': some_other_function,\\n        # \\'on_retry_callback\\': another_function,\\n        # \\'sla_miss_callback\\': yet_another_function,\\n        # \\'trigger_rule\\': \\'all_success\\'\\n    }},\\n    description=\\'{description}\\',\\n    start_date={start_date},\\n    catchup=False,\\n    tags={tags},\\n    is_paused_upon_creation=False,\\n) as dag:\\n\\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\\n\\n    t1 = AzureBatchOperator(task_id=\"sleep-1\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"sleep 1\", batch_task_id=\"batch-task-NXIVZU\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t2 = AzureBatchOperator(task_id=\"test-executor-my_test_executor-f-data-path-urllocaltmptmpvyu0jj29data-model-path-urllocaltmptmpvyu0jj29model\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"test-executor my_test_executor f --data-path-url=local:/tmp/tmpvyu0jj29/data --model-path-url=local:/tmp/tmpvyu0jj29/model\", batch_task_id=\"batch-task-NEV7LG\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t3 = AzureBatchOperator(task_id=\"echo-step-1-completed\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"echo step 1 completed\", batch_task_id=\"batch-task-K1HKTD\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t4 = AzureBatchOperator(task_id=\"sleep-2\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"sleep 2\", batch_task_id=\"batch-task-VFC39X\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t5 = AzureBatchOperator(task_id=\"test-executor-my_test_executor-g-data-path-urllocaltmptmpvyu0jj29data-model-path-urllocaltmptmpvyu0jj29model\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"test-executor my_test_executor g --data-path-url=local:/tmp/tmpvyu0jj29/data --model-path-url=local:/tmp/tmpvyu0jj29/model\", batch_task_id=\"batch-task-JHOJ4O\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t6 = AzureBatchOperator(task_id=\"echo-step-2-completed\", batch_pool_id=\"random_azure_batch_env_component_name\", batch_pool_vm_size=\"standard_d2s_v3\", batch_job_id=\"random_azure_batch_env_component_name\", batch_task_command_line=\"echo step 2 completed\", batch_task_id=\"batch-task-04OIBY\", azure_batch_conn_id=\"custom_azure_batch_default\", vm_publisher=\"microsoft-azure-batch\", vm_offer=\"ubuntu-server-container\", vm_sku=\"20-04-lts\", vm_version=\"latest\", vm_node_agent_sku_id=\"batch.node.ubuntu 20.04\" , enable_auto_scale=True, auto_scale_formula=\"\"\"// Get pending tasks for the past 15 minutes.\\n$samples = $PendingTasks.GetSamplePercent(TimeInterval_Minute * 15);\\n// If we have fewer than 70 percent data points, we use the last sample point,\\n// otherwise we use the maximum of last sample point and the history average.\\n$tasks = $samples < 70 ? max(0,$PendingTasks.GetSample(1)) : max( $PendingTasks.GetSample(1), avg($PendingTasks.GetSample(TimeInterval_Minute * 15)));\\n// If number of pending tasks is not 0, set targetVM to pending tasks, otherwise\\n// half of current dedicated.\\n$targetVMs = $tasks > 0? $tasks:max(0, $TargetDedicatedNodes/2);\\n// The pool size is capped at 20, if target VM value is more than that, set it\\n// to 20. This value should be adjusted according to your use case.\\n$TargetDedicatedNodes = max(0, min($targetVMs, 5));\\n// Set node deallocation mode - let running tasks finish before removing a node\\n$NodeDeallocationOption = taskcompletion;\"\"\", batch_task_container_settings=batchmodels.TaskContainerSettings(image_name=\"ghcr.io/airtai/airt-service:dev\", container_run_options=\" --env AWS_ACCESS_KEY_ID = '********************' --env AWS_SECRET_ACCESS_KEY = '****************************************' --env AWS_DEFAULT_REGION=\\'eu-west-1\\' --env AZURE_SUBSCRIPTION_ID = '************************************' --env AZURE_TENANT_ID = '************************************' --env AZURE_CLIENT_ID = '************************************' --env AZURE_CLIENT_SECRET = '****************************************' --env AZURE_STORAGE_ACCOUNT_PREFIX=\\'kumsairtsdev\\' --env AZURE_RESOURCE_GROUP=\\'kumaran-airt-service-dev\\' --env STORAGE_BUCKET_PREFIX=\\'kumaran-airt-service\\' --env DB_USERNAME=\\'root\\' --env DB_PASSWORD = '****************************************' --env DB_HOST=\\'kumaran-mysql\\' --env DB_PORT=\\'3306\\' --env DB_DATABASE=\\'airt_service\\' --env DB_DATABASE_SERVER=\\'mysql\\'\"))\\n    t1 >> t2 >> t3 >> t4 >> t5 >> t6'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    test_batch_environment_path = save_test_azure_batch_environment_names(d)\n",
    "\n",
    "    kwargs = {\"data_path_url\": data_path_url, \"model_path_url\": model_path_url}\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        batch_environment_path=test_batch_environment_path,\n",
    "    )\n",
    "\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "    template = abe._create_dag_template(\n",
    "        on_step_start=on_step_start, on_step_end=on_step_end, **kwargs\n",
    "    )\n",
    "    display(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f031b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpofdjyvq2/data'), Path('/tmp/tmpofdjyvq2/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"batch_pool.name='test-cpu-pool'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"batch_job.name='test-cpu-job'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"test_batch_environment_names={'northeurope': {'csv_processing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'predictions': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'preprocessing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'training': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=Path('/home/kumaran/airflow/dags/env_test-executor-my_test_executor-f-data-path-urllocaltmptmpofdjyvq2data-model-path-urllocaltmptmpofdjyvq2model.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmpofdjyvq2data-model-path-urllocaltmptmpofdjyvq2model', '--conf', '{}', '--run-id', 'airt-service__2022-12-19T09:13:40.396806'], returncode=0, stdout='[\\x1b[34m2022-12-19 09:13:41,273\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun env_test-executor-my_test_executor-f-data-path-urllocaltmptmpofdjyvq2data-model-path-urllocaltmptmpofdjyvq2model @ 2022-12-19T09:13:41+00:00: airt-service__2022-12-19T09:13:40.396806, state:queued, queued_at: 2022-12-19 09:13:41.364406+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmpofdjyvq2data-model-path-urllocaltmptmpofdjyvq2model', 'run_id': 'airt-service__2022-12-19T09:13:40.396806', 'state': 'running', 'execution_date': '2022-12-19T09:13:41+00:00', 'start_date': '2022-12-19T09:13:41.626883+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-19T09:13:40.396806'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | eval: false\n",
    "# Test case for AirflowAzureBatchExecutor._create_dag\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", None]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_id, dag_file_path = abe._create_dag(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=None,\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5135ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmplcef0chl/data'), Path('/tmp/tmplcef0chl/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"batch_pool.name='test-cpu-pool'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"batch_job.name='test-cpu-job'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"test_batch_environment_names={'northeurope': {'csv_processing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'predictions': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'preprocessing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'training': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"dag_file_path=Path('/home/kumaran/airflow/dags/env_test-executor-my_test_executor-f-data-path-urllocaltmptmplcef0chldata-model-path-urllocaltmptmplcef0chlmodel.py')\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dag_runs=[]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmplcef0chldata-model-path-urllocaltmptmplcef0chlmodel', '--conf', '{}', '--run-id', 'airt-service__2022-12-19T09:31:41.483455'], returncode=0, stdout='[\\x1b[34m2022-12-19 09:31:42,241\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun env_test-executor-my_test_executor-f-data-path-urllocaltmptmplcef0chldata-model-path-urllocaltmptmplcef0chlmodel @ 2022-12-19T09:31:42+00:00: airt-service__2022-12-19T09:31:41.483455, state:queued, queued_at: 2022-12-19 09:31:42.326289+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmplcef0chldata-model-path-urllocaltmptmplcef0chlmodel', 'run_id': 'airt-service__2022-12-19T09:31:41.483455', 'state': 'running', 'execution_date': '2022-12-19T09:31:42+00:00', 'start_date': '2022-12-19T09:31:42.859223+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-19T09:31:41.483455'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | eval: false\n",
    "# Test case for AirflowAzureBatchExecutor.schedule\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"csv_processing\", \"preprocessing\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_file_path = abe.schedule(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        #         schedule_interval=\"@weekly\",\n",
    "        schedule_interval=timedelta(days=7),\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(f\"{dag_file_path=}\")\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "\n",
    "    sleep(15)\n",
    "\n",
    "    dag_runs = list_dag_runs(dag_id=dag_id)\n",
    "    display(f\"{dag_runs=}\")\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "\n",
    "    #     run_id = dag_runs[0][\"run_id\"]\n",
    "    display(run_id)\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch\n",
    "def execute(\n",
    "    self: AirflowAzureBatchExecutor,\n",
    "    *,\n",
    "    description: str,\n",
    "    tags: Union[str, List[str]],\n",
    "    on_step_start: Optional[CLICommandBase] = None,\n",
    "    on_step_end: Optional[CLICommandBase] = None,\n",
    "    **kwargs,\n",
    ") -> Tuple[Path, str]:\n",
    "    \"\"\"Create DAG and execute steps in airflow\n",
    "\n",
    "    Args:\n",
    "        description: description of DAG\n",
    "        tags: tags for DAG\n",
    "        on_step_start: CLI to call before executing step/task in DAG\n",
    "        on_step_end: CLI to call after executing step/task in DAG\n",
    "        kwargs: keyword arguments needed for steps/tasks\n",
    "    Returns:\n",
    "        A tuple which contains dag file path and run id\n",
    "    \"\"\"\n",
    "    schedule_interval = None\n",
    "    dag_id, dag_file_path = self._create_dag(\n",
    "        schedule_interval=schedule_interval,\n",
    "        description=description,\n",
    "        tags=tags,\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    run_id = trigger_dag(dag_id=dag_id, conf={})\n",
    "    return dag_file_path, run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef996a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpu3qns5n6/data'), Path('/tmp/tmpu3qns5n6/model')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"batch_pool.name='test-cpu-pool'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"batch_job.name='test-cpu-job'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"test_batch_environment_names={'northeurope': {'csv_processing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'predictions': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'preprocessing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'training': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}}}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmpu3qns5n6data-model-path-urllocaltmptmpu3qns5n6model', '--conf', '{}', '--run-id', 'airt-service__2022-12-19T09:34:23.711309'], returncode=0, stdout='[\\x1b[34m2022-12-19 09:34:24,649\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun env_test-executor-my_test_executor-f-data-path-urllocaltmptmpu3qns5n6data-model-path-urllocaltmptmpu3qns5n6model @ 2022-12-19T09:34:24+00:00: airt-service__2022-12-19T09:34:23.711309, state:queued, queued_at: 2022-12-19 09:34:24.747226+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'env_test-executor-my_test_executor-f-data-path-urllocaltmptmpu3qns5n6data-model-path-urllocaltmptmpu3qns5n6model', 'run_id': 'airt-service__2022-12-19T09:34:23.711309', 'state': 'running', 'execution_date': '2022-12-19T09:34:24+00:00', 'start_date': '2022-12-19T09:34:25.572298+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path('/home/kumaran/airflow/dags/env_test-executor-my_test_executor-f-data-path-urllocaltmptmpu3qns5n6data-model-path-urllocaltmptmpu3qns5n6model.py')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'airt-service__2022-12-19T09:34:23.711309'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'success'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | eval: false\n",
    "# Test case for AirflowAzureBatchExecutor.execute\n",
    "region = \"northeurope\"\n",
    "with tempfile.TemporaryDirectory() as d:\n",
    "    data_path_url, model_path_url = setup_test_paths(d)\n",
    "    steps = [\n",
    "        SimpleCLICommand(command=\"env\"),\n",
    "        ClassCLICommand(\n",
    "            executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "        ),\n",
    "        #         ClassCLICommand(\n",
    "        #             executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"g\"\n",
    "        #         ),\n",
    "    ]\n",
    "    exec_environments = [\"training\", \"predictions\"]\n",
    "    on_step_start = SimpleCLICommand(command=\"sleep {step_count}\")\n",
    "    on_step_end = SimpleCLICommand(command=\"echo step {step_count} completed\")\n",
    "\n",
    "    td = Path(d)\n",
    "    created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "    shared_key_credentials = SharedKeyCredentials(\n",
    "        \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "    )\n",
    "\n",
    "    batch_account_name = \"testbatchnortheurope\"\n",
    "    region = \"northeurope\"\n",
    "\n",
    "    batch_pool = BatchPool.from_name(\n",
    "        name=\"test-cpu-pool\",\n",
    "        batch_account_name=batch_account_name,\n",
    "        region=region,\n",
    "        shared_key_credentials=shared_key_credentials,\n",
    "    )\n",
    "    batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "    display(f\"{batch_pool.name=}\")\n",
    "    display(f\"{batch_job.name=}\")\n",
    "    region = \"northeurope\"\n",
    "    test_batch_environment_names = {\n",
    "        region: {\n",
    "            task: {\n",
    "                \"batch_job_name\": batch_job.name,\n",
    "                \"batch_pool_name\": batch_pool.name,\n",
    "                \"batch_account_name\": batch_account_name,\n",
    "            }\n",
    "            for task in [\n",
    "                \"csv_processing\",\n",
    "                \"predictions\",\n",
    "                \"preprocessing\",\n",
    "                \"training\",\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    display(f\"{test_batch_environment_names=}\")\n",
    "    with open(created_azure_env_path, \"w\") as f:\n",
    "        yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "    abe = AirflowAzureBatchExecutor(\n",
    "        steps=steps,\n",
    "        region=region,\n",
    "        exec_environments=exec_environments,\n",
    "        batch_environment_path=created_azure_env_path,\n",
    "    )\n",
    "    dag_file_path, run_id = abe.execute(\n",
    "        data_path_url=data_path_url,\n",
    "        model_path_url=model_path_url,\n",
    "        description=\"test description\",\n",
    "        tags=\"test_tag\",\n",
    "        on_step_start=on_step_start,\n",
    "        on_step_end=on_step_end,\n",
    "    )\n",
    "\n",
    "    display(dag_file_path)\n",
    "    display(run_id)\n",
    "\n",
    "    dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "    state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "    display(state)\n",
    "    dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _test_azure_batch_executor(region: str = \"northeurope\"):\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        data_path_url, model_path_url = setup_test_paths(d)\n",
    "\n",
    "        steps = [\n",
    "            ClassCLICommand(\n",
    "                executor_name=\"test-executor\", class_name=\"MyTestExecutor\", f_name=\"f\"\n",
    "            )\n",
    "        ]\n",
    "        exec_environments = [\"training\"]\n",
    "\n",
    "        td = Path(d)\n",
    "        created_azure_env_path = td / \"azure_batch_environment.yml\"\n",
    "\n",
    "        shared_key_credentials = SharedKeyCredentials(\n",
    "            \"testbatchnortheurope\", os.environ[\"SHARED_KEY_CREDENTIALS\"]\n",
    "        )\n",
    "\n",
    "        batch_account_name = \"testbatchnortheurope\"\n",
    "        region = \"northeurope\"\n",
    "\n",
    "        batch_pool = BatchPool.from_name(\n",
    "            name=\"test-cpu-pool\",\n",
    "            batch_account_name=batch_account_name,\n",
    "            region=region,\n",
    "            shared_key_credentials=shared_key_credentials,\n",
    "        )\n",
    "        batch_job = BatchJob.from_name(name=\"test-cpu-job\", batch_pool=batch_pool)\n",
    "\n",
    "        sanitized_print(f\"{batch_pool.name=}\")\n",
    "        sanitized_print(f\"{batch_job.name=}\")\n",
    "        region = \"northeurope\"\n",
    "        test_batch_environment_names = {\n",
    "            region: {\n",
    "                task: {\n",
    "                    \"batch_job_name\": batch_job.name,\n",
    "                    \"batch_pool_name\": batch_pool.name,\n",
    "                    \"batch_account_name\": batch_account_name,\n",
    "                }\n",
    "                for task in [\n",
    "                    \"csv_processing\",\n",
    "                    \"predictions\",\n",
    "                    \"preprocessing\",\n",
    "                    \"training\",\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        sanitized_print(f\"{test_batch_environment_names=}\")\n",
    "        with open(created_azure_env_path, \"w\") as f:\n",
    "            yaml.dump(test_batch_environment_names, f, default_flow_style=False)\n",
    "\n",
    "        abe = AirflowAzureBatchExecutor(\n",
    "            steps=steps,\n",
    "            region=region,\n",
    "            exec_environments=exec_environments,  # type: ignore\n",
    "            batch_environment_path=created_azure_env_path,\n",
    "        )\n",
    "        dag_file_path, run_id = abe.execute(\n",
    "            data_path_url=data_path_url,\n",
    "            model_path_url=model_path_url,\n",
    "            description=\"test description\",\n",
    "            tags=\"test_tag\",\n",
    "        )\n",
    "\n",
    "        sanitized_print(f\"{dag_file_path=}\")\n",
    "        sanitized_print(f\"{run_id=}\")\n",
    "\n",
    "        dag_id = str(dag_file_path).split(\"/\")[-1].split(\".py\")[0]\n",
    "        state = wait_for_run_to_complete(dag_id=dag_id, run_id=run_id, timeout=3600)\n",
    "        sanitized_print(f\"{state=}\")\n",
    "        dag_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def test_azure_batch_executor(region: Param(\"region\", str) = \"northeurope\"):  # type: ignore\n",
    "    \"\"\"\n",
    "    Create throw away environment for azure batch and execute airflow batch executor\n",
    "    \"\"\"\n",
    "    _test_azure_batch_executor(region=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3a2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paths=[Path('/tmp/tmpvs06gink/data'), Path('/tmp/tmpvs06gink/model')]\n",
      "batch_pool.name='test-cpu-pool'\n",
      "batch_job.name='test-cpu-job'\n",
      "test_batch_environment_names={'northeurope': {'csv_processing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'predictions': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'preprocessing': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}, 'training': {'batch_job_name': 'test-cpu-job', 'batch_pool_name': 'test-cpu-pool', 'batch_account_name': 'testbatchnortheurope'}}}\n",
      "CompletedProcess(args=['/home/kumaran/airflow_venv/bin/airflow', 'dags', 'trigger', 'test-executor-my_test_executor-f-data-path-urllocaltmptmpvs06ginkdata-model-path-urllocaltmptmpvs06ginkmodel', '--conf', '{}', '--run-id', 'airt-service__2022-12-19T09:37:09.414889'], returncode=0, stdout='[\\x1b[34m2022-12-19 09:37:10,386\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun test-executor-my_test_executor-f-data-path-urllocaltmptmpvs06ginkdata-model-path-urllocaltmptmpvs06ginkmodel @ 2022-12-19T09:37:10+00:00: airt-service__2022-12-19T09:37:09.414889, state:queued, queued_at: 2022-12-19 09:37:10.480273+00:00. externally triggered: True>\\n', stderr='/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/home/kumaran/airflow_venv/lib/python3.9/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 'test-executor-my_test_executor-f-data-path-urllocaltmptmpvs06ginkdata-model-path-urllocaltmptmpvs06ginkmodel', 'run_id': 'airt-service__2022-12-19T09:37:09.414889', 'state': 'running', 'execution_date': '2022-12-19T09:37:10+00:00', 'start_date': '2022-12-19T09:37:11.086799+00:00', 'end_date': ''}]\n",
      "dag_file_path=Path('/home/kumaran/airflow/dags/test-executor-my_test_executor-f-data-path-urllocaltmptmpvs06ginkdata-model-path-urllocaltmptmpvs06ginkmodel.py')\n",
      "run_id='airt-service__2022-12-19T09:37:09.414889'\n",
      "state='success'\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "test_azure_batch_executor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bda18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
