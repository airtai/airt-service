{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Router to combine all datablob functionalities\n",
    "output-file: datablob_router.html\n",
    "title: DataBlob Router\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a68a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ff042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d1cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import json\n",
    "import shlex\n",
    "from enum import Enum\n",
    "from time import sleep\n",
    "from typing import *\n",
    "import uuid as uuid_pkg\n",
    "\n",
    "import numpy as np\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from pydantic import BaseModel\n",
    "from fastapi import APIRouter, Depends, HTTPException, status, Query, BackgroundTasks\n",
    "from sqlalchemy.exc import NoResultFound\n",
    "from sqlalchemy.orm.exc import StaleDataError\n",
    "from sqlmodel import Session, select\n",
    "\n",
    "from airt.executor.subcommand import SimpleCLICommand\n",
    "from airt.helpers import get_s3_bucket_name_and_folder_from_uri\n",
    "from airt.logger import get_logger\n",
    "from airt.patching import patch\n",
    "\n",
    "import airt_service\n",
    "import airt_service.sanitizer\n",
    "from airt_service.airflow.executor import AirflowExecutor\n",
    "from airt_service.auth import get_current_active_user\n",
    "from airt_service.aws.utils import (\n",
    "    create_s3_datablob_path,\n",
    "    get_s3_bucket_and_path_from_uri,\n",
    "    verify_aws_region,\n",
    ")\n",
    "from airt_service.azure.utils import verify_azure_region\n",
    "from airt_service.batch_job import create_batch_job\n",
    "from airt_service.data.clickhouse import create_db_uri_for_clickhouse_datablob\n",
    "from airt_service.data.datasource import DataSource\n",
    "from airt_service.data.utils import (\n",
    "    create_db_uri_for_azure_blob_storage_datablob,\n",
    "    create_db_uri_for_s3_datablob,\n",
    "    create_db_uri_for_db_datablob,\n",
    "    create_db_uri_for_local_datablob,\n",
    "    delete_data_object_files_in_cloud,\n",
    ")\n",
    "from airt_service.db.models import (\n",
    "    User,\n",
    "    DataBlob,\n",
    "    DataBlobRead,\n",
    "    DataSourceRead,\n",
    "    TagCreate,\n",
    "    get_session,\n",
    "    Tag,\n",
    ")\n",
    "from airt_service.errors import HTTPError, ERRORS\n",
    "from airt_service.helpers import commit_or_rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pytest\n",
    "import requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "\n",
    "from airt_service.aws.utils import get_queue_definition_arns, upload_to_s3_with_retry\n",
    "from airt_service.background_task import execute_cli\n",
    "from airt_service.db.models import (\n",
    "    create_user_for_testing,\n",
    "    get_db_params_from_env_vars,\n",
    "    get_session_with_context,\n",
    ")\n",
    "from airt_service.data.s3 import s3_pull\n",
    "from airt_service.helpers import set_env_variable_context\n",
    "from airt.remote_path import RemotePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7512bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gwynhfyfgx'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing()\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bdc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: log a random string\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"log a random string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# Default router for all data sources\n",
    "datablob_router = APIRouter(\n",
    "    prefix=\"/datablob\",\n",
    "    tags=[\"datablob\"],\n",
    "    #     dependencies=[Depends(get_current_active_user)],\n",
    "    responses={\n",
    "        404: {\"description\": \"Not found\"},\n",
    "        500: {\n",
    "            \"model\": HTTPError,\n",
    "            \"description\": ERRORS[\"INTERNAL_SERVER_ERROR\"],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def remove_tag_from_previous_datablobs(self: DataBlob, tag_name: str, session: Session):\n",
    "    \"\"\"Remove tag_name associated with other/previous datablobs\n",
    "\n",
    "    Args:\n",
    "        tag_name: Tag name to remove from other datablobs\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "    tag_to_remove = Tag.get_by_name(name=tag_name, session=session)  # type: ignore\n",
    "    try:\n",
    "        datablobs = session.exec(\n",
    "            select(DataBlob).where(\n",
    "                DataBlob.type == self.type,\n",
    "                DataBlob.uri == self.uri,\n",
    "                DataBlob.user == self.user,\n",
    "            )\n",
    "        ).all()\n",
    "    except NoResultFound:\n",
    "        return\n",
    "\n",
    "    for datablob in datablobs:\n",
    "        if tag_to_remove in datablob.tags:\n",
    "            try:\n",
    "                datablob.tags.remove(tag_to_remove)\n",
    "                session.add(datablob)\n",
    "                session.commit()\n",
    "            except StaleDataError:\n",
    "                session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e656d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[Tag(id=3, created=datetime.datetime(2022, 11, 7, 9, 10, 5), name='test', uuid=UUID('e1e3ca56-290b-41be-b877-6d6bcd1b4078'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=4, uuid=UUID('71904d1d-8a0b-4f01-b93d-ae6281c09dc1'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 6), user_id=6, pulled_on=None, tags=[Tag()])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://bucket\"\n",
    "\n",
    "    test_tag = Tag.get_by_name(name=\"test\", session=session)\n",
    "    db_uri = create_db_uri_for_s3_datablob(\n",
    "        uri=uri,\n",
    "        access_key=\"access\",\n",
    "        secret_key=\"secret\",\n",
    "    )\n",
    "    datablob_with_tag = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=db_uri,\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        tags=[test_tag],\n",
    "    )\n",
    "    session.add(datablob_with_tag)\n",
    "    session.commit()\n",
    "    display(datablob_with_tag)\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        new_datablob_with_test_tag = DataBlob(\n",
    "            type=\"s3\",\n",
    "            uri=db_uri,\n",
    "            source=uri,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "            #     tags=[test_tag],\n",
    "        )\n",
    "        new_datablob_with_test_tag.remove_tag_from_previous_datablobs(\n",
    "            tag_name=test_tag.name, session=session\n",
    "        )\n",
    "\n",
    "        datablob_without_test_tag = session.exec(\n",
    "            select(DataBlob).where(DataBlob.uuid == datablob_with_tag.uuid)\n",
    "        ).one()\n",
    "        display(datablob_without_test_tag)\n",
    "        session.add(datablob_without_test_tag)\n",
    "        assert test_tag not in datablob_without_test_tag\n",
    "        assert not datablob_without_test_tag.tags\n",
    "\n",
    "        new_datablob_with_test_tag.tags.append(test_tag)\n",
    "        session.add(new_datablob_with_test_tag)\n",
    "        display(new_datablob_with_test_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10117741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "create_datablob_responses = {\n",
    "    400: {\"model\": HTTPError, \"description\": \"DataBlob error\"},\n",
    "}\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def _create(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    type: str,\n",
    "    uri: str,\n",
    "    source: str,\n",
    "    cloud_provider: str,\n",
    "    region: str,\n",
    "    total_steps: int,\n",
    "    user_tag: Optional[str] = None,\n",
    "    user: User,\n",
    "    session: Session\n",
    ") -> DataBlob:\n",
    "    \"\"\"Function to create new datablob based on given params\n",
    "\n",
    "    Args:\n",
    "        type: Datablob type\n",
    "        uri: DB uri of datablob\n",
    "        source: Datablob uri\n",
    "        cloud_provider: Cloud provider to store datablob files\n",
    "        region: Region of cloud provider\n",
    "        total_steps: Total steps\n",
    "        user_tag: Tag created by user to add to new datablob\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        The created datablob object\n",
    "    Raises:\n",
    "        HTTPException if request has bad parameters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datablob = DataBlob(\n",
    "            type=type,\n",
    "            uri=uri,\n",
    "            source=source,\n",
    "            cloud_provider=cloud_provider,\n",
    "            region=region,\n",
    "            total_steps=total_steps,\n",
    "            user=user,\n",
    "        )\n",
    "\n",
    "        for tag_name in [user_tag, \"latest\"] if user_tag is not None else [\"latest\"]:\n",
    "            datablob.remove_tag_from_previous_datablobs(  # type: ignore\n",
    "                tag_name=tag_name, session=session\n",
    "            )\n",
    "            datablob.tags.append(Tag.get_by_name(name=tag_name, session=session))  # type: ignore\n",
    "\n",
    "        session.add(datablob)\n",
    "        session.commit()\n",
    "        return datablob\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        error_message = (\n",
    "            e._message() if callable(getattr(e, \"_message\", None)) else str(e)  # type: ignore\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=error_message,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc52caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=5, uuid=UUID('702ef2e8-fafd-452f-b5db-d6305b32a973'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 6), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=4, created=datetime.datetime(2022, 11, 7, 9, 10, 6), name='my_s3_datablob_tag', uuid=UUID('93076773-196f-4144-aa16-66d18889460e'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://bucket\"\n",
    "    with commit_or_rollback(session):\n",
    "        actual = DataBlob._create(\n",
    "            type=\"s3\",\n",
    "            uri=create_db_uri_for_s3_datablob(\n",
    "                uri=uri,\n",
    "                access_key=\"access\",\n",
    "                secret_key=\"secret\",\n",
    "            ),\n",
    "            source=uri,\n",
    "            cloud_provider=\"aws\",\n",
    "            region=\"eu-west-1\",\n",
    "            total_steps=1,\n",
    "            user_tag=\"my_s3_datablob_tag\",\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "        session.add(actual)\n",
    "    display(actual)\n",
    "    assert actual.id is not None\n",
    "\n",
    "    assert actual.source == uri\n",
    "\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == actual.id)).one()\n",
    "    assert datablob.type == \"s3\"\n",
    "    assert (Tag.get_by_name(\"my_s3_datablob_tag\", session)) in datablob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class S3Request(BaseModel):\n",
    "    \"\"\"Base Request object for from_s3 and to_s3 routes\"\"\"\n",
    "\n",
    "    uri: str\n",
    "    access_key: str\n",
    "    secret_key: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93565851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class CloudProvider(str, Enum):\n",
    "    aws = \"aws\"\n",
    "    azure = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b314cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FromS3Request(S3Request):\n",
    "    \"\"\"Request object for the /data/s3 route\n",
    "\n",
    "    Args:\n",
    "        uri: S3 uri of the folder where parquet files are stored\n",
    "        access_key: Access key for the s3 bucket\n",
    "        secret_key: Secret key for the s3 bucket\n",
    "        cloud_provider: Cloud provider to save files\n",
    "        region: Region of the cloud provider\n",
    "        tag: Tag to add to the datablob\n",
    "    \"\"\"\n",
    "\n",
    "    cloud_provider: CloudProvider = \"aws\"  # type: ignore\n",
    "    region: Optional[str] = None\n",
    "    tag: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc24bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def from_s3(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    from_s3_request: FromS3Request,\n",
    "    user: User,\n",
    "    session: Session,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    no_retries: int = 3,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from an S3 bucket\n",
    "\n",
    "    Args:\n",
    "        from_s3_request: The from_s3 request\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "        background_tasks: BackgroundTasks object\n",
    "        no_retries: Number of times to retry before raising an exception\n",
    "\n",
    "    Returns:\n",
    "        A new datablob created from a S3\n",
    "    \"\"\"\n",
    "\n",
    "    uri = create_db_uri_for_s3_datablob(\n",
    "        uri=from_s3_request.uri,\n",
    "        access_key=from_s3_request.access_key,\n",
    "        secret_key=from_s3_request.secret_key,\n",
    "    )\n",
    "\n",
    "    cloud_provider = from_s3_request.cloud_provider\n",
    "    region = from_s3_request.region\n",
    "    if region is None:\n",
    "        s3_client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=from_s3_request.access_key,\n",
    "            aws_secret_access_key=from_s3_request.secret_key,\n",
    "            config=Config(signature_version=\"s3v4\"),\n",
    "        )\n",
    "        bucket_name, folder = get_s3_bucket_name_and_folder_from_uri(\n",
    "            from_s3_request.uri\n",
    "        )\n",
    "        region = s3_client.get_bucket_location(Bucket=bucket_name)[\"LocationConstraint\"]\n",
    "\n",
    "    verify_aws_region(region) if cloud_provider == \"aws\" else verify_azure_region(\n",
    "        region\n",
    "    )\n",
    "    source = from_s3_request.uri\n",
    "\n",
    "    for i in range(no_retries):\n",
    "        e: Optional[Exception] = None\n",
    "        try:\n",
    "            datablob = DataBlob._create(  # type: ignore\n",
    "                type=\"s3\",\n",
    "                uri=uri,\n",
    "                source=source,\n",
    "                cloud_provider=cloud_provider,\n",
    "                region=region,\n",
    "                total_steps=1,\n",
    "                user_tag=from_s3_request.tag,\n",
    "                user=user,\n",
    "                session=session,\n",
    "            )\n",
    "            break\n",
    "        except Exception as _e:\n",
    "            e = _e\n",
    "            sleep(np.random.uniform(1, 5))\n",
    "\n",
    "    if e:\n",
    "        logger.exception(f\"DataBlob.from_s3() failed\", exc_info=e)\n",
    "        raise HTTPException(status_code=500, detail=f\"Unexpected exception: {e}\")\n",
    "\n",
    "    #     command = f\"s3_pull {datablob.id}\"\n",
    "\n",
    "    #     create_batch_job(\n",
    "    #         command=command, task=\"csv_processing\", region=region, background_tasks=background_tasks\n",
    "    #     )\n",
    "\n",
    "    steps = [SimpleCLICommand(command=\"s3_pull {datablob_id}\")]\n",
    "    executor = AirflowExecutor.create_executor(\n",
    "        steps, cloud_provider=datablob.cloud_provider, region=datablob.region\n",
    "    )\n",
    "    dag_file_path, run_id = executor.execute(\n",
    "        description=\"s3 pull\",\n",
    "        tags=\"s3_pull\",\n",
    "        datablob_id=datablob.id,\n",
    "    )\n",
    "\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/from_s3\", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore\n",
    ")\n",
    "def from_s3_route(\n",
    "    *,\n",
    "    from_s3_request: FromS3Request,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from csv/parquet files in s3 bucket\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.from_s3(  # type: ignore\n",
    "        from_s3_request=from_s3_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "        background_tasks=background_tasks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748cfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 's3_pull-6', '--conf', '{\"datablob_id\": 6}', '--run-id', 'airt-service__2022-11-07T09:10:22.391682'], returncode=0, stdout='[\\x1b[34m2022-11-07 09:10:23,658\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun s3_pull-6 @ 2022-11-07T09:10:23+00:00: airt-service__2022-11-07T09:10:22.391682, state:queued, queued_at: 2022-11-07 09:10:23.722886+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 's3_pull-6', 'run_id': 'airt-service__2022-11-07T09:10:22.391682', 'state': 'running', 'execution_date': '2022-11-07T09:10:23+00:00', 'start_date': '2022-11-07T09:10:24.079718+00:00', 'end_date': ''}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=6, uuid=UUID('02d0ec7f-2f3e-4d69-9021-5ed335a046a9'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-3', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 6), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=4, created=datetime.datetime(2022, 11, 7, 9, 10, 6), name='my_s3_datablob_tag', uuid=UUID('93076773-196f-4144-aa16-66d18889460e'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = f\"s3://test-airt-service/account_312571_events\"\n",
    "    from_s3_request = FromS3Request(\n",
    "        uri=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=None,\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        tag=\"my_s3_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        actual = from_s3_route(\n",
    "            from_s3_request=from_s3_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(actual)\n",
    "\n",
    "    assert actual.type == \"s3\"\n",
    "    assert actual.source == uri, actual.source\n",
    "\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.uuid == actual.uuid)).one()\n",
    "\n",
    "    # bg_task = b.tasks[-1]\n",
    "    # display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "    # assert bg_task.func == execute_cli\n",
    "    # assert bg_task.kwargs[\"command\"] == f\"s3_pull {actual.id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ad66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Unknown region - region-doesnt-exists; Available regions are ap-northeast-1, ap...l-1, eu-central-1, eu-north-1, eu-west-1, eu-west-2, eu-west-3, sa-east-1, us-east-1, us-east-2, us-west-1, us-west-2') tblen=4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = f\"s3://test-airt-service/account_312571_events\"\n",
    "    from_s3_request = FromS3Request(\n",
    "        uri=uri,\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"region-doesnt-exists\",\n",
    "        tag=\"my_s3_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            actual = from_s3_route(\n",
    "                from_s3_request=from_s3_request,\n",
    "                user=user,\n",
    "                session=session,\n",
    "                background_tasks=b,\n",
    "            )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AzureBlobStorageRequest(BaseModel):\n",
    "    \"\"\"Base Request object for from_azure_blob_storage and to_azure_blob_storage routes\"\"\"\n",
    "\n",
    "    uri: str\n",
    "    credential: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c94d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FromAzureBlobStorageRequest(AzureBlobStorageRequest):\n",
    "    \"\"\"Request object for the from_azure_blob_storage route\n",
    "\n",
    "    Args:\n",
    "        uri: Azure blob storage uri of the folder where parquet files are stored\n",
    "        credential: Credential for the blob storage container\n",
    "        cloud_provider: Cloud provider to save files\n",
    "        region: Region of the cloud provider\n",
    "        tag: Tag to add to the datablob\n",
    "    \"\"\"\n",
    "\n",
    "    cloud_provider: CloudProvider = \"azure\"  # type: ignore\n",
    "    #     region: Optional[str] = None\n",
    "    region: str\n",
    "    tag: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dd3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def from_azure_blob_storage(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    from_azure_blob_storage_request: FromAzureBlobStorageRequest,\n",
    "    user: User,\n",
    "    session: Session,\n",
    "    background_tasks: BackgroundTasks,\n",
    "    no_retries: int = 3,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from given azure blob storage\n",
    "\n",
    "    Args:\n",
    "        from_azure_blob_storage_request: The from_azure_blob_storage request\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "        background_tasks: BackgroundTasks object\n",
    "        no_retries: Number of times to retry before raising an exception\n",
    "\n",
    "    Returns:\n",
    "        A new datablob created from azure blob storage\n",
    "    \"\"\"\n",
    "\n",
    "    uri = create_db_uri_for_azure_blob_storage_datablob(\n",
    "        uri=from_azure_blob_storage_request.uri,\n",
    "        credential=from_azure_blob_storage_request.credential,\n",
    "    )\n",
    "\n",
    "    cloud_provider = from_azure_blob_storage_request.cloud_provider\n",
    "    region = from_azure_blob_storage_request.region\n",
    "    # ToDo: get region\n",
    "    #     if region is None:\n",
    "    #         s3_client = boto3.client(\n",
    "    #             \"s3\",\n",
    "    #             aws_access_key_id=from_s3_request.access_key,\n",
    "    #             aws_secret_access_key=from_s3_request.secret_key,\n",
    "    #         )\n",
    "    #         bucket_name, folder = get_s3_bucket_name_and_folder_from_uri(from_s3_request.uri)\n",
    "    #         region = s3_client.get_bucket_location(Bucket=bucket_name)[\"LocationConstraint\"]\n",
    "\n",
    "    verify_aws_region(region) if cloud_provider == \"aws\" else verify_azure_region(\n",
    "        region\n",
    "    )\n",
    "    source = from_azure_blob_storage_request.uri\n",
    "\n",
    "    datablob = DataBlob._create(  # type: ignore\n",
    "        type=\"azure_blob_storage\",\n",
    "        uri=uri,\n",
    "        source=source,\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        total_steps=1,\n",
    "        user_tag=from_azure_blob_storage_request.tag,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    command = f\"azure_blob_storage_pull {datablob.id}\"\n",
    "    create_batch_job(\n",
    "        command=command,\n",
    "        task=\"csv_processing\",\n",
    "        cloud_provider=cloud_provider,\n",
    "        region=region,\n",
    "        background_tasks=background_tasks,\n",
    "    )\n",
    "\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/from_azure_blob_storage\", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore\n",
    ")\n",
    "def from_azure_blob_storage_route(\n",
    "    *,\n",
    "    from_azure_blob_storage_request: FromAzureBlobStorageRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from csv/parquet files in s3 bucket\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.from_azure_blob_storage(  # type: ignore\n",
    "        from_azure_blob_storage_request=from_azure_blob_storage_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "        background_tasks=background_tasks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae16a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] airt_service.batch_job: create_batch_job(): command='azure_blob_storage_pull 7', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='azure_blob_storage_pull 7', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=7, uuid=UUID('e10db2f9-c20d-4bd6-8704-76fc0c848218'), type='azure_blob_storage', uri='https://****************************************@testairtservice.blob.core.windows.net/test-container/account_312571_events', source='https://testairtservice.blob.core.windows.net/test-container/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.azure: 'azure'>, region='westeurope', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 27), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=5, created=datetime.datetime(2022, 11, 7, 9, 10, 27), name='my_azure_blob_storage_datablob_tag', uuid=UUID('ab876872-4eaf-455e-95ea-db7027f82b0b'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7fbd812eef70>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"bg_task.kwargs={'command': 'azure_blob_storage_pull 7'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"https://testairtservice.blob.core.windows.net/test-container/account_312571_events\"\n",
    "\n",
    "    storage_client = StorageManagementClient(\n",
    "        DefaultAzureCredential(), os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    "    )\n",
    "    keys = storage_client.storage_accounts.list_keys(\n",
    "        \"test-airt-service\", \"testairtservice\"\n",
    "    )\n",
    "    credential = keys.keys[0].value\n",
    "\n",
    "    from_azure_blob_storage_request = FromAzureBlobStorageRequest(\n",
    "        uri=uri,\n",
    "        credential=credential,\n",
    "        cloud_provider=\"azure\",\n",
    "        region=\"westeurope\",\n",
    "        tag=\"my_azure_blob_storage_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        actual = from_azure_blob_storage_route(\n",
    "            from_azure_blob_storage_request=from_azure_blob_storage_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(actual)\n",
    "\n",
    "    assert actual.type == \"azure_blob_storage\"\n",
    "    assert actual.source == uri, actual.source\n",
    "\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.uuid == actual.uuid)).one()\n",
    "\n",
    "    bg_task = b.tasks[-1]\n",
    "    display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "    assert bg_task.func == execute_cli\n",
    "    assert bg_task.kwargs[\"command\"] == f\"azure_blob_storage_pull {actual.id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec1019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='Unknown region - region-does-not-exists; Available regions are australiacentral...dnorth, switzerlandwest, uaecentral, uaenorth, uksouth, ukwest, westcentralus, westeurope, westindia, westus, westus2') tblen=4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"https://testairtservice.blob.core.windows.net/test-container/account_312571_events\"\n",
    "\n",
    "    storage_client = StorageManagementClient(\n",
    "        DefaultAzureCredential(), os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    "    )\n",
    "    keys = storage_client.storage_accounts.list_keys(\n",
    "        \"test-airt-service\", \"testairtservice\"\n",
    "    )\n",
    "    credential = keys.keys[0].value\n",
    "\n",
    "    from_azure_blob_storage_request = FromAzureBlobStorageRequest(\n",
    "        uri=uri,\n",
    "        credential=credential,\n",
    "        cloud_provider=\"azure\",\n",
    "        region=\"region-does-not-exists\",\n",
    "        tag=\"my_azure_blob_storage_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            actual = from_azure_blob_storage_route(\n",
    "                from_azure_blob_storage_request=from_azure_blob_storage_request,\n",
    "                user=user,\n",
    "                session=session,\n",
    "                background_tasks=b,\n",
    "            )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900faa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class DBRequest(BaseModel):\n",
    "    \"\"\"Base request object for from_db and to_db routes\"\"\"\n",
    "\n",
    "    host: str\n",
    "    port: int\n",
    "    username: str\n",
    "    password: str\n",
    "    database: str\n",
    "    table: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabb6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FromDBRequest(DBRequest):\n",
    "    \"\"\"Request object for the /datablob/db route\n",
    "\n",
    "    Args:\n",
    "        host: Remote database host name\n",
    "        port: DB port\n",
    "        username: Username to access the db\n",
    "        password: Password to access the db\n",
    "        database: Database to use\n",
    "        table: Table to import data from\n",
    "        cloud_provider: Cloud provider to save files\n",
    "        region: Region of the cloud provider\n",
    "        tag: Tag to add to the datablob\n",
    "    \"\"\"\n",
    "\n",
    "    cloud_provider: CloudProvider = \"aws\"  # type: ignore\n",
    "    region: str = \"eu-west-1\"\n",
    "    tag: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def from_rdbms(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    from_db_request: FromDBRequest,\n",
    "    database_server: str,\n",
    "    user: User,\n",
    "    session: Session,\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from a RDBMS\n",
    "\n",
    "    Args:\n",
    "        from_db_request: The from_db request\n",
    "        database_server: Database engine name\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "        background_tasks: BackgroundTasks object\n",
    "\n",
    "    Returns:\n",
    "        A new datablob created from a RDBMS\n",
    "    \"\"\"\n",
    "    host = from_db_request.host\n",
    "    port = from_db_request.port\n",
    "    table = from_db_request.table\n",
    "    database = from_db_request.database\n",
    "\n",
    "    uri = create_db_uri_for_db_datablob(\n",
    "        username=from_db_request.username,\n",
    "        password=from_db_request.password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        table=table,\n",
    "        database=database,\n",
    "        database_server=database_server,\n",
    "    )\n",
    "\n",
    "    source = f\"{database_server}://{host}:{port}/{database}/{table}\"\n",
    "    verify_aws_region(\n",
    "        from_db_request.region\n",
    "    ) if from_db_request.cloud_provider == \"aws\" else verify_azure_region(\n",
    "        from_db_request.region\n",
    "    )\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        datablob = DataBlob._create(  # type: ignore\n",
    "            type=\"db\",\n",
    "            uri=uri,\n",
    "            source=source,\n",
    "            cloud_provider=from_db_request.cloud_provider,\n",
    "            region=from_db_request.region,\n",
    "            total_steps=1,\n",
    "            user_tag=from_db_request.tag,\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "    if database_server in [\"mysql\", \"postgresql\"]:\n",
    "        command = f\"db_pull {datablob.id}\"\n",
    "    else:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_501_NOT_IMPLEMENTED,\n",
    "            detail=f\"{ERRORS['PULL_NOT_AVAILABLE']} for database server {database_server}\",\n",
    "        )\n",
    "\n",
    "    create_batch_job(\n",
    "        command=command,\n",
    "        task=\"csv_processing\",\n",
    "        cloud_provider=from_db_request.cloud_provider,\n",
    "        region=from_db_request.region,\n",
    "        background_tasks=background_tasks,\n",
    "    )\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/from_mysql\", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore\n",
    ")\n",
    "def from_mysql_route(\n",
    "    *,\n",
    "    from_db_request: FromDBRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from a database\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.from_rdbms(  # type: ignore\n",
    "        from_db_request=from_db_request,\n",
    "        database_server=\"mysql\",\n",
    "        user=user,\n",
    "        session=session,\n",
    "        background_tasks=background_tasks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c3390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='db_pull 8', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='db_pull 8', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=8, uuid=UUID('67a32ba1-37aa-425b-b887-b0b02ab9e432'), type='db', uri='mysql://****************************************@db.example.com:3306/database_to_import/events', source='mysql://db.example.com:3306/database_to_import/events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 29), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=6, created=datetime.datetime(2022, 11, 7, 9, 10, 29), name='my_db_datablob_tag', uuid=UUID('8995cbb2-c7e7-4e03-9dd9-2d6756aaf8ca'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual.source='mysql://db.example.com:3306/database_to_import/events'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7fbd812eef70>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"bg_task.kwargs={'command': 'db_pull 8'}\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    database_server = \"mysql\"\n",
    "    host = \"db.example.com\"\n",
    "    port = 3306\n",
    "    database = \"database_to_import\"\n",
    "    table = \"events\"\n",
    "\n",
    "    from_db_request = FromDBRequest(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        username=\"username\",\n",
    "        password=\"password\",\n",
    "        database=database,\n",
    "        table=table,\n",
    "        tag=\"my_db_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        actual = from_mysql_route(\n",
    "            from_db_request=from_db_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(actual)\n",
    "\n",
    "    assert actual.type == \"db\"\n",
    "\n",
    "    display(f\"{actual.source=}\")\n",
    "    assert (\n",
    "        actual.source == f\"{database_server}://{host}:{port}/{database}/{table}\"\n",
    "    ), actual.source\n",
    "\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.uuid == actual.uuid)).one()\n",
    "\n",
    "    bg_task = b.tasks[-1]\n",
    "    display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "    assert bg_task.func == execute_cli\n",
    "    assert bg_task.kwargs[\"command\"] == f\"db_pull {actual.id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e0385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] __main__: DataError('(MySQLdb.DataError) (1406, \"Data too long for column \\'uri\\' at row 1\")')\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/cursors.py\", line 206, in execute\n",
      "    res = self._query(query)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/cursors.py\", line 319, in _query\n",
      "    db.query(q)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/connections.py\", line 254, in query\n",
      "    _mysql.connection.query(self, query)\n",
      "MySQLdb.DataError: (1406, \"Data too long for column 'uri' at row 1\")\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-53ff729c4413>\", line 52, in _create\n",
      "    datablob.remove_tag_from_previous_datablobs(  # type: ignore\n",
      "  File \"<ipython-input-9-ae65663c9f14>\", line 12, in remove_tag_from_previous_datablobs\n",
      "    tag_to_remove = Tag.get_by_name(name=tag_name, session=session)  # type: ignore\n",
      "  File \"/tf/airt-service/airt_service/db/models.py\", line 1235, in get_by_name\n",
      "    tag = session.exec(select(Tag).where(Tag.name == name)).one()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlmodel/orm/session.py\", line 60, in exec\n",
      "    results = super().execute(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 1660, in execute\n",
      "    ) = compile_state_cls.orm_pre_session_exec(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/context.py\", line 312, in orm_pre_session_exec\n",
      "    session._autoflush()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 2257, in _autoflush\n",
      "    util.raise_(e, with_traceback=sys.exc_info()[2])\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 208, in raise_\n",
      "    raise exception\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 2246, in _autoflush\n",
      "    self.flush()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 3386, in flush\n",
      "    self._flush(objects)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 3526, in _flush\n",
      "    transaction.rollback(_capture_exception=True)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py\", line 70, in __exit__\n",
      "    compat.raise_(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 208, in raise_\n",
      "    raise exception\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py\", line 3486, in _flush\n",
      "    flush_context.execute()\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py\", line 456, in execute\n",
      "    rec.execute(self)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/unitofwork.py\", line 630, in execute\n",
      "    util.preloaded.orm_persistence.save_obj(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py\", line 245, in save_obj\n",
      "    _emit_insert_statements(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/orm/persistence.py\", line 1238, in _emit_insert_statements\n",
      "    result = connection._execute_20(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1705, in _execute_20\n",
      "    return meth(self, args_10style, kwargs_10style, execution_options)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\", line 333, in _execute_on_connection\n",
      "    return connection._execute_clauseelement(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1572, in _execute_clauseelement\n",
      "    ret = self._execute_context(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1943, in _execute_context\n",
      "    self._handle_dbapi_exception(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 2124, in _handle_dbapi_exception\n",
      "    util.raise_(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py\", line 208, in raise_\n",
      "    raise exception\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py\", line 1900, in _execute_context\n",
      "    self.dialect.do_execute(\n",
      "  File \"/root/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n",
      "    cursor.execute(statement, parameters)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/cursors.py\", line 206, in execute\n",
      "    res = self._query(query)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/cursors.py\", line 319, in _query\n",
      "    db.query(q)\n",
      "  File \"/root/.local/lib/python3.8/site-packages/MySQLdb/connections.py\", line 254, in query\n",
      "    _mysql.connection.query(self, query)\n",
      "sqlalchemy.exc.DataError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)\n",
      "(MySQLdb.DataError) (1406, \"Data too long for column 'uri' at row 1\")\n",
      "[SQL: INSERT INTO datablob (cloud_provider, type, total_steps, source, completed_steps, folder_size, error, region, disabled, created, pulled_on, uuid, uri, path, user_id) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)]\n",
      "[parameters: ('aws', 'db', 1, 'mysql://db.example.com:3306/database_to_import/events', 0, None, None, 'eu-west-1', 0, datetime.datetime(2022, 11, 7, 9, 10, 28, 786621), None, '38e7e50504384036bfb7e28473d040ad', 'mysql://username:ZAP+%251%21s%252%21s%253%21s%254%21s%255%21s%256%21s%257%21s%258%21s%259%21s%2510%21s%2511%21s%2512%21s%2513%21s%2514%21s%2515%21s%2 ... (120 characters truncated) ... %21n%2530%21n%2531%21n%2532%21n%2533%21n%2534%21n%2535%21n%2536%21n%2537%21n%2538%21n%2539%21n%2540%21n@db.example.com:3306/database_to_import/events', None, 6)]\n",
      "(Background on this error at: https://sqlalche.me/e/14/9h9h)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='(MySQLdb.DataError) (1406, \"Data too long for column \\'uri\\' at row 1\")') tblen=4>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test failure scenario where lengthy string is a db password\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    database_server = \"mysql\"\n",
    "    host = \"db.example.com\"\n",
    "    port = 3306\n",
    "    database = \"database_to_import\"\n",
    "    table = \"events\"\n",
    "\n",
    "    from_db_request = FromDBRequest(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        username=\"username\",\n",
    "        password=\"ZAP %1!s%2!s%3!s%4!s%5!s%6!s%7!s%8!s%9!s%10!s%11!s%12!s%13!s%14!s%15!s%16!s%17!s%18!s%19!s%20!s%21!n%22!n%23!n%24!n%25!n%26!n%27!n%28!n%29!n%30!n%31!n%32!n%33!n%34!n%35!n%36!n%37!n%38!n%39!n%40!n\",\n",
    "        database=database,\n",
    "        table=table,\n",
    "        tag=\"my_db_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "            actual = from_mysql_route(\n",
    "                from_db_request=from_db_request,\n",
    "                user=user,\n",
    "                session=session,\n",
    "                background_tasks=b,\n",
    "            )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class ClickHouseRequest(DBRequest):\n",
    "    \"\"\"Base request object for from_clickhouse and to_clickhouse routes\"\"\"\n",
    "\n",
    "    protocol: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73965e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FromClickHouseRequest(ClickHouseRequest):\n",
    "    \"\"\"Request object for the /datablob/from_clickhouse route\n",
    "\n",
    "    Args:\n",
    "        host: Hostname where db is hosted\n",
    "        port: DB port\n",
    "        username: Username to access the db\n",
    "        password: Password to access the db\n",
    "        database: Database to use\n",
    "        table: Table to import/export data\n",
    "        protocol: Protocol to use (native/http)\n",
    "        index_column: Column to use to partition rows and to use as index\n",
    "        timestamp_column: Timestamp column\n",
    "        filters: Additional column filters as a dictionary\n",
    "        cloud_provider: Cloud provider to save files\n",
    "        region: Region of the cloud provider\n",
    "        tag: Tag to add to the datablob\n",
    "    \"\"\"\n",
    "\n",
    "    index_column: str\n",
    "    timestamp_column: str\n",
    "    filters: Optional[Dict[str, Any]] = None\n",
    "    cloud_provider: CloudProvider = \"aws\"  # type: ignore\n",
    "    region: str = \"eu-west-1\"\n",
    "    tag: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e8ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def from_clickhouse(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    from_clickhouse_request: FromClickHouseRequest,\n",
    "    user: User,\n",
    "    session: Session,\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from a clickhouse database\n",
    "\n",
    "    Args:\n",
    "        from_clickhouse_request: The from_clickhouse request\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "        background_tasks: BackgroundTasks object\n",
    "\n",
    "    Returns:\n",
    "        A new datablob created from a clickhouse database\n",
    "    \"\"\"\n",
    "\n",
    "    host = from_clickhouse_request.host\n",
    "    port = from_clickhouse_request.port\n",
    "    table = from_clickhouse_request.table\n",
    "    database = from_clickhouse_request.database\n",
    "    protocol = from_clickhouse_request.protocol\n",
    "\n",
    "    uri = create_db_uri_for_clickhouse_datablob(\n",
    "        username=from_clickhouse_request.username,\n",
    "        password=from_clickhouse_request.password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        table=table,\n",
    "        database=database,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "\n",
    "    source = f\"clickhouse+{protocol}://{host}:{port}/{database}/{table}\"\n",
    "    verify_aws_region(\n",
    "        from_clickhouse_request.region\n",
    "    ) if from_clickhouse_request.cloud_provider == \"aws\" else verify_azure_region(\n",
    "        from_clickhouse_request.region\n",
    "    )\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        datablob = DataBlob._create(  # type: ignore\n",
    "            type=\"db\",\n",
    "            uri=uri,\n",
    "            source=source,\n",
    "            cloud_provider=from_clickhouse_request.cloud_provider,\n",
    "            region=from_clickhouse_request.region,\n",
    "            total_steps=1,\n",
    "            user_tag=from_clickhouse_request.tag,\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "    command = f\"clickhouse_pull {datablob.id} {from_clickhouse_request.index_column} {from_clickhouse_request.timestamp_column}\"\n",
    "    if from_clickhouse_request.filters:\n",
    "        command = (\n",
    "            command\n",
    "            + f\" --filters_json {shlex.quote(json.dumps(from_clickhouse_request.filters))}\"\n",
    "        )\n",
    "\n",
    "    create_batch_job(\n",
    "        command=command,\n",
    "        task=\"csv_processing\",\n",
    "        cloud_provider=from_clickhouse_request.cloud_provider,\n",
    "        region=from_clickhouse_request.region,\n",
    "        background_tasks=background_tasks,\n",
    "    )\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/from_clickhouse\", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore\n",
    ")\n",
    "def from_clickhouse_route(\n",
    "    *,\n",
    "    from_clickhouse_request: FromClickHouseRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataBlob:\n",
    "    \"\"\"Create a datablob from a database\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.from_clickhouse(  # type: ignore\n",
    "        from_clickhouse_request=from_clickhouse_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "        background_tasks=background_tasks,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a08c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='clickhouse_pull 9 PersonId OccurredTimeTicks --filters_json \\'{\"AccountId\": 312571}\\'', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='clickhouse_pull 9 PersonId OccurredTimeTicks --filters_json \\'{\"AccountId\": 312571}\\'', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=9, uuid=UUID('6548dfad-9588-43ed-a789-264d02171de0'), type='db', uri='clickhouse+native://****************************************@db.example.com:3306/database_to_import/events', source='clickhouse+native://db.example.com:3306/database_to_import/events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 29), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=7, created=datetime.datetime(2022, 11, 7, 9, 10, 29), name='my_clickhouse_datablob_tag', uuid=UUID('c362c5dd-f437-456c-8135-2563b49bfa70'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual.source='clickhouse+native://db.example.com:3306/database_to_import/events'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7fbd812eef70>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.kwargs={\\'command\\': \\'clickhouse_pull 9 PersonId OccurredTimeTicks --filters_json \\\\\\'{\"AccountId\": 312571}\\\\\\'\\'}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    filters = {\"AccountId\": 312571}\n",
    "\n",
    "    host = \"db.example.com\"\n",
    "    port = 3306\n",
    "    database = \"database_to_import\"\n",
    "    table = \"events\"\n",
    "    protocol = \"native\"\n",
    "\n",
    "    from_clickhouse_request = FromClickHouseRequest(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        username=\"username\",\n",
    "        password=\"password\",\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "        index_column=\"PersonId\",\n",
    "        timestamp_column=\"OccurredTimeTicks\",\n",
    "        filters=filters,\n",
    "        tag=\"my_clickhouse_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        actual = from_clickhouse_route(\n",
    "            from_clickhouse_request=from_clickhouse_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(actual)\n",
    "\n",
    "    assert actual.type == \"db\"\n",
    "\n",
    "    display(f\"{actual.source=}\")\n",
    "    assert (\n",
    "        actual.source == f\"clickhouse+{protocol}://{host}:{port}/{database}/{table}\"\n",
    "    ), actual.source\n",
    "\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.uuid == actual.uuid)).one()\n",
    "\n",
    "    bg_task = b.tasks[-1]\n",
    "    display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "    assert bg_task.func == execute_cli\n",
    "    assert (\n",
    "        bg_task.kwargs[\"command\"]\n",
    "        == f\"clickhouse_pull {actual.id} PersonId OccurredTimeTicks --filters_json '{json.dumps(filters)}'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f75821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FromLocalRequest(BaseModel):\n",
    "    \"\"\"Request object for from_local route\n",
    "\n",
    "    Args:\n",
    "        path: Local path of the datablob\n",
    "        cloud_provider: Cloud provider to save files\n",
    "        region: Region of the cloud provider\n",
    "        tag: Tag to add to the datablob\n",
    "    \"\"\"\n",
    "\n",
    "    path: str\n",
    "    cloud_provider: CloudProvider = \"aws\"  # type: ignore\n",
    "    region: str = \"eu-west-1\"\n",
    "    tag: Optional[str] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True\n",
    "\n",
    "\n",
    "class FromLocalResponse(BaseModel):\n",
    "    \"\"\"Response object for the /datablob/from_local route\n",
    "\n",
    "    Args:\n",
    "        uuid: Datablob uuid\n",
    "        type: Type of the datablob\n",
    "        presigned: Presigned s3 url(valid for 24 hours) and other params to upload CSV file\n",
    "    \"\"\"\n",
    "\n",
    "    uuid: uuid_pkg.UUID\n",
    "    type: str\n",
    "    presigned: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def from_local(\n",
    "    cls: DataBlob,\n",
    "    *,\n",
    "    path: str,\n",
    "    cloud_provider: str,\n",
    "    region: str,\n",
    "    user_tag: Optional[str] = None,\n",
    "    user: User,\n",
    "    session: Session,\n",
    ") -> FromLocalResponse:\n",
    "    \"\"\"Create a datablob from local file(s)\n",
    "\n",
    "    Args:\n",
    "        path: The relative or absolute path to a local file or to a directory containing the files.\n",
    "        user_tag: A string to tag the datablob\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        A new datablob created from local file(s)\n",
    "    \"\"\"\n",
    "    if cloud_provider == \"azure\":\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"AZURE_NOT_SUPPORTED\"],\n",
    "        )\n",
    "    verify_aws_region(region)\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        datablob = DataBlob._create(  # type: ignore\n",
    "            type=\"local\",\n",
    "            uri=None,\n",
    "            source=path,\n",
    "            cloud_provider=cloud_provider,\n",
    "            region=region,\n",
    "            total_steps=1,\n",
    "            user_tag=user_tag,\n",
    "            user=user,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "    destination_bucket, s3_path = create_s3_datablob_path(\n",
    "        user_id=user.id, datablob_id=datablob.id, region=region  # type: ignore\n",
    "    )\n",
    "    uri = create_db_uri_for_local_datablob(bucket=destination_bucket, s3_path=s3_path)\n",
    "    with commit_or_rollback(session):\n",
    "        datablob.uri = uri\n",
    "        session.add(datablob)\n",
    "\n",
    "    presigned = boto3.client(\n",
    "        \"s3\", region_name=region, config=Config(signature_version=\"s3v4\")\n",
    "    ).generate_presigned_post(\n",
    "        Bucket=destination_bucket.name,\n",
    "        Key=s3_path + \"/\" + \"${filename}\",\n",
    "        Fields=None,\n",
    "        Conditions=[[\"starts-with\", \"$key\", s3_path]],\n",
    "        ExpiresIn=60 * 60 * 24,\n",
    "    )\n",
    "    from_local_response = FromLocalResponse(\n",
    "        uuid=datablob.uuid, type=datablob.type, presigned=presigned\n",
    "    )\n",
    "    logger.info(f\"DataBlob.from_local(): {from_local_response.__repr__()}\")\n",
    "    return from_local_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/from_local/start\",\n",
    "    response_model=FromLocalResponse,\n",
    "    responses=create_datablob_responses,  # type: ignore\n",
    ")\n",
    "def from_local_start_route(\n",
    "    from_local_request: FromLocalRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> FromLocalResponse:\n",
    "    \"\"\"Get presigned s3 url to upload local CSV/Parquet files and create datablob from it\"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.from_local(  # type: ignore\n",
    "        path=from_local_request.path,\n",
    "        cloud_provider=from_local_request.cloud_provider,\n",
    "        region=from_local_request.region,\n",
    "        user_tag=from_local_request.tag,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31698e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] __main__: DataBlob.from_local(): FromLocalResponse(uuid=UUID('a17e6c65-4c91-4e24-a89f-dd1fc23a7e23'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'x-amz-algorithm': 'AWS4-HMAC-SHA256', 'x-amz-credential': '********************/20221107/eu-west-1/s3/aws4_request', 'x-amz-date': '20221107T091029Z', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'x-amz-signature': '****************************'}})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FromLocalResponse(uuid=UUID('a17e6c65-4c91-4e24-a89f-dd1fc23a7e23'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'x-amz-algorithm': 'AWS4-HMAC-SHA256', 'x-amz-credential': '********************/20221107/eu-west-1/s3/aws4_request', 'x-amz-date': '20221107T091029Z', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'x-amz-signature': '****************************'}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-4.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-3.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-2.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-0.csv'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-1.csv')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-0.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2/csv/file-4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_b8j5dnb2\n"
     ]
    }
   ],
   "source": [
    "test_path = \"tmp/test-folder/\"\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_local_request = FromLocalRequest(path=test_path, tag=\"my_csv_datablob_tag\")\n",
    "    actual = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual.uuid\n",
    "    assert actual.type == \"local\"\n",
    "    assert isinstance(actual.presigned, dict)\n",
    "\n",
    "    try:\n",
    "        datablob_csv = session.exec(\n",
    "            select(DataBlob).where(DataBlob.uuid == actual.uuid)\n",
    "        ).one()\n",
    "        datablob_csv.source == test_path\n",
    "    except NoResultFound:\n",
    "        assert False\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        ddf = dd.read_parquet(test_s3_path.as_path())\n",
    "        ddf.to_csv(test_s3_path.as_path() / \"csv\" / \"file-*.csv\", index=False)\n",
    "        display(list((test_s3_path.as_path() / \"csv\").glob(\"*\")))\n",
    "        sleep(10)\n",
    "\n",
    "        for csv_to_upload in sorted((test_s3_path.as_path() / \"csv\").glob(\"*.csv\")):\n",
    "            display(f\"Uploading {csv_to_upload}\")\n",
    "            upload_to_s3_with_retry(\n",
    "                csv_to_upload, actual.presigned[\"url\"], actual.presigned[\"fields\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0839af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "get_datablob_responses = {\n",
    "    400: {\"model\": HTTPError, \"description\": ERRORS[\"INCORRECT_DATABLOB_ID\"]},\n",
    "    422: {\"model\": HTTPError, \"description\": \"DataBlob error\"},\n",
    "}\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def get(cls: DataBlob, uuid: str, user: User, session: Session) -> DataBlob:\n",
    "    \"\"\"Get datablob based on uuid\n",
    "\n",
    "    Args:\n",
    "        uuid: Datablob uuid\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        The datablob object for given datablob uuid\n",
    "\n",
    "    Raises:\n",
    "        HTTPException: if datablob id is incorrect or if datablob is deleted\n",
    "    \"\"\"\n",
    "    try:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.uuid == uuid, DataBlob.user == user)\n",
    "        ).one()\n",
    "    except NoResultFound:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"INCORRECT_DATABLOB_ID\"],\n",
    "        )\n",
    "\n",
    "    if datablob.disabled:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_400_BAD_REQUEST,\n",
    "            detail=ERRORS[\"DATABLOB_IS_DELETED\"],\n",
    "        )\n",
    "\n",
    "    if datablob.error is not None:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=datablob.error\n",
    "        )\n",
    "\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64587a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='The datablob uuid is incorrect. Please try again.') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='The datablob uuid is incorrect. Please try again.') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    expected = user.datablobs[0]\n",
    "\n",
    "    actual = DataBlob.get(uuid=expected.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual == expected\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataBlob.get(\n",
    "            uuid=\"00000000-0000-0000-0000-000000000000\", user=user, session=session\n",
    "        )\n",
    "    display(e)\n",
    "\n",
    "    user_kumaran = session.exec(select(User).where(User.username == \"kumaran\")).one()\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataBlob.get(uuid=expected.uuid, user=user_kumaran, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d691f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=400, detail='The datablob has already been deleted.') tblen=2>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    datablob_disabled = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=\"s3://\", access_key=\"access\", secret_key=\"secret\"\n",
    "        ),\n",
    "        source=\"s3://\",\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        disabled=True,\n",
    "    )\n",
    "    session.add(datablob_disabled)\n",
    "    session.commit()\n",
    "    session.refresh(datablob_disabled)\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        DataBlob.get(uuid=datablob_disabled.uuid, user=user, session=session)\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e14e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def is_ready(self: DataBlob):\n",
    "    \"\"\"Check if the datablob's completed steps equal to total steps, else raise HTTPException\"\"\"\n",
    "    if self.completed_steps != self.total_steps:\n",
    "        if self.path:\n",
    "            bucket, s3_path = get_s3_bucket_and_path_from_uri(self.path)  # type: ignore\n",
    "        else:\n",
    "            bucket, s3_path = create_s3_datablob_path(user_id=self.user.id, datablob_id=self.id, region=self.region)  # type: ignore\n",
    "\n",
    "        if len(list(bucket.objects.filter(Prefix=s3_path + \"/\"))) == 0:\n",
    "            raise HTTPException(\n",
    "                status_code=status.HTTP_400_BAD_REQUEST,\n",
    "                detail=ERRORS[\"DATABLOB_CSV_FILES_NOT_AVAILABLE\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be9605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class FileType(str, Enum):\n",
    "    csv = \"csv\"\n",
    "    parquet = \"parquet\"\n",
    "\n",
    "\n",
    "class ToDataSourceRequest(BaseModel):\n",
    "    \"\"\"Request object for the /datablob/{datablob_id}/to_datasource route\n",
    "\n",
    "    Args:\n",
    "\n",
    "        file_type: type of files in datablob; currently csv and parquet are supported\n",
    "        deduplicate_data: If set to True (default value False), then duplicate rows are removed while processing\n",
    "        index_column: Name of the column used to index and partition the data into partitions\n",
    "        sort_by: Name of the column or list of columns  used to sort data within the same index value\n",
    "        blocksize: Size of partition\n",
    "        kwargs: Keyword arguments which are passed to the **dask.dataframe.read_csv()** function,\n",
    "            typically params for underlining **pd.read_csv()** from Pandas.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type: FileType\n",
    "    deduplicate_data: bool = False\n",
    "    index_column: str\n",
    "    sort_by: Union[str, List[str]]\n",
    "    blocksize: str = \"256MB\"\n",
    "    kwargs: Optional[Dict[str, Any]] = None\n",
    "\n",
    "    class Config:\n",
    "        use_enum_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1093fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def to_datasource(\n",
    "    self: DataBlob,\n",
    "    to_datasource_request: ToDataSourceRequest,\n",
    "    user: User,\n",
    "    session: Session,\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataSource:\n",
    "    \"\"\"Process the CSV/Parquet datablob files and return a datasource object\n",
    "\n",
    "    Args:\n",
    "        to_datasource_request: The to_datasource_request object\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "        background_tasks: BackgroundTasks object\n",
    "    \"\"\"\n",
    "\n",
    "    self.is_ready()  # type: ignore\n",
    "    datasource = DataSource._create(datablob=self, user=user, session=session)  # type: ignore\n",
    "\n",
    "    if to_datasource_request.file_type == \"csv\":\n",
    "        process_command = \"process_csv\"\n",
    "    elif to_datasource_request.file_type == \"parquet\":\n",
    "        process_command = \"process_parquet\"\n",
    "\n",
    "    sort_by = (\n",
    "        [to_datasource_request.sort_by]\n",
    "        if isinstance(to_datasource_request.sort_by, str)\n",
    "        else to_datasource_request.sort_by\n",
    "    )\n",
    "\n",
    "    command = f\"{process_command} {self.id} {datasource.id} {shlex.quote(to_datasource_request.index_column)} {shlex.quote(json.dumps(sort_by))} --blocksize {to_datasource_request.blocksize}\"\n",
    "    if to_datasource_request.kwargs is not None:\n",
    "        command = (\n",
    "            command\n",
    "            + f\" --kwargs_json {shlex.quote(json.dumps(to_datasource_request.kwargs))}\"\n",
    "        )\n",
    "    if to_datasource_request.deduplicate_data:\n",
    "        command = command + \" --deduplicate_data\"\n",
    "\n",
    "    create_batch_job(\n",
    "        command=command,\n",
    "        task=\"csv_processing\",\n",
    "        cloud_provider=self.cloud_provider,\n",
    "        region=self.region,\n",
    "        background_tasks=background_tasks,\n",
    "    )\n",
    "\n",
    "    return datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\n",
    "    \"/{datablob_uuid}/to_datasource\",\n",
    "    status_code=status.HTTP_202_ACCEPTED,\n",
    "    response_model=DataSourceRead,\n",
    "    responses=get_datablob_responses,  # type: ignore\n",
    ")\n",
    "def to_datasource_route(\n",
    "    *,\n",
    "    datablob_uuid: str,\n",
    "    to_datasource_request: ToDataSourceRequest,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    "    background_tasks: BackgroundTasks,\n",
    ") -> DataSource:\n",
    "    \"\"\"Pull uploaded CSV/Parquet datablob, process it and store in s3 client storage bucket as parquet\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datablob.to_datasource(\n",
    "        to_datasource_request, user, session, background_tasks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f426cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='process_csv 10 5 PersonId \\'[\"OccurredTime\"]\\' --blocksize 256MB --kwargs_json \\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\' --deduplicate_data', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='process_csv 10 5 PersonId \\'[\"OccurredTime\"]\\' --blocksize 256MB --kwargs_json \\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\' --deduplicate_data', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=5, uuid=UUID('4c06889f-a615-4bc5-affa-a5f91ca29c8e'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 59), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=8, created=datetime.datetime(2022, 11, 7, 9, 10, 29), name='my_csv_datablob_tag', uuid=UUID('c6f8bc58-8142-429e-a29f-25b6120fff1d'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7fbd812eef70>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.kwargs={\\'command\\': \\'process_csv 10 5 PersonId \\\\\\'[\"OccurredTime\"]\\\\\\' --blocksize 256MB --kwargs_json \\\\\\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\\\\\' --deduplicate_data\\'}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "\n",
    "with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "    file_type = \"csv\"\n",
    "    deduplicate_data = True\n",
    "    index_column = \"PersonId\"\n",
    "    sort_by = \"OccurredTime\"\n",
    "    blocksize = \"256MB\"\n",
    "    kwargs = dict(\n",
    "        usecols=[0, 1, 2, 3, 4],\n",
    "        parse_dates=[\"OccurredTime\"],\n",
    "    )\n",
    "\n",
    "    with get_session_with_context() as session:\n",
    "        user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "        to_datasource_request = ToDataSourceRequest(\n",
    "            file_type=file_type,\n",
    "            deduplicate_data=deduplicate_data,\n",
    "            index_column=index_column,\n",
    "            sort_by=sort_by,\n",
    "            blocksize=blocksize,\n",
    "            kwargs=kwargs,\n",
    "        )\n",
    "        b = BackgroundTasks()\n",
    "        actual = to_datasource_route(\n",
    "            datablob_uuid=datablob_csv.uuid,\n",
    "            to_datasource_request=to_datasource_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "        display(actual)\n",
    "        assert isinstance(actual, DataSource)\n",
    "        bg_task = b.tasks[-1]\n",
    "        display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "        assert bg_task.func == execute_cli\n",
    "        assert (\n",
    "            bg_task.kwargs[\"command\"]\n",
    "            == f\"process_csv {datablob_csv.id} {actual.id} {index_column} '{json.dumps([sort_by])}' --blocksize {blocksize} --kwargs_json '{json.dumps(kwargs)}' --deduplicate_data\"\n",
    "        ), bg_task.kwargs[\"command\"]\n",
    "#     assert actual.id == datablob_csv.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff12428c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: DataBlob.from_local(): FromLocalResponse(uuid=UUID('5e6fb117-e9ad-450a-b1dd-5bf6e272f789'), type='local', presigned={'url': 'https://kumaran-airt-service-eu-west-1.s3.amazonaws.com/', 'fields': {'key': '****************************************', 'x-amz-algorithm': 'AWS4-HMAC-SHA256', 'x-amz-credential': '********************/20221107/eu-west-1/s3/aws4_request', 'x-amz-date': '20221107T091100Z', 'policy': '************************************************************************************************************************************************************************************************************************************************************', 'x-amz-signature': '****************************'}})\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/_common_metadata'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.3.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.0.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.1.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.4.parquet'),\n",
       " Path('/tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.2.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/_common_metadata'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/_metadata'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.0.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.1.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.2.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.3.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Uploading /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0/part.4.parquet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_3otoccd0\n",
      "[INFO] airt_service.batch_job: create_batch_job(): command='process_parquet 12 7 PersonId \\'[\"OccurredTime\"]\\' --blocksize 256MB --kwargs_json \\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\' --deduplicate_data', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='process_parquet 12 7 PersonId \\'[\"OccurredTime\"]\\' --blocksize 256MB --kwargs_json \\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\' --deduplicate_data', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=7, uuid=UUID('bb1928aa-9213-4af9-86c1-3a8f64bebb58'), hash=None, total_steps=1, completed_steps=0, folder_size=None, no_of_rows=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 11, 28), user_id=6, pulled_on=None, tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25')), Tag(id=9, created=datetime.datetime(2022, 11, 7, 9, 10, 59), name='my_parquet_datablob_tag', uuid=UUID('2d04b349-5f76-4988-bc98-47241abc0022'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.func=<function execute_cli at 0x7fbd812eef70>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.args=()'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'bg_task.kwargs={\\'command\\': \\'process_parquet 12 7 PersonId \\\\\\'[\"OccurredTime\"]\\\\\\' --blocksize 256MB --kwargs_json \\\\\\'{\"usecols\": [0, 1, 2, 3, 4], \"parse_dates\": [\"OccurredTime\"]}\\\\\\' --deduplicate_data\\'}'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_path = \"tmp/test-folder/\"\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_local_request = FromLocalRequest(path=test_path, tag=\"my_parquet_datablob_tag\")\n",
    "    actual = from_local_start_route(\n",
    "        from_local_request=from_local_request,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        datablob_parquet = session.exec(\n",
    "            select(DataBlob).where(DataBlob.uuid == actual.uuid)\n",
    "        ).one()\n",
    "        datablob_parquet.source == test_path\n",
    "    except NoResultFound:\n",
    "        assert False\n",
    "\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://test-airt-service/account_312571_events\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    ) as test_s3_path:\n",
    "        display(list(test_s3_path.as_path().glob(\"*\")))\n",
    "        sleep(10)\n",
    "\n",
    "        for parquet_to_upload in sorted(test_s3_path.as_path().glob(\"*\")):\n",
    "            display(f\"Uploading {parquet_to_upload}\")\n",
    "            upload_to_s3_with_retry(\n",
    "                parquet_to_upload, actual.presigned[\"url\"], actual.presigned[\"fields\"]\n",
    "            )\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        file_type = \"parquet\"\n",
    "        deduplicate_data = True\n",
    "        index_column = \"PersonId\"\n",
    "        sort_by = \"OccurredTime\"\n",
    "        blocksize = \"256MB\"\n",
    "        kwargs = dict(\n",
    "            usecols=[0, 1, 2, 3, 4],\n",
    "            parse_dates=[\"OccurredTime\"],\n",
    "        )\n",
    "        to_datasource_request = ToDataSourceRequest(\n",
    "            file_type=file_type,\n",
    "            deduplicate_data=deduplicate_data,\n",
    "            index_column=index_column,\n",
    "            sort_by=sort_by,\n",
    "            blocksize=blocksize,\n",
    "            kwargs=kwargs,\n",
    "        )\n",
    "        b = BackgroundTasks()\n",
    "        actual = to_datasource_route(\n",
    "            datablob_uuid=datablob_parquet.uuid,\n",
    "            to_datasource_request=to_datasource_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "        display(actual)\n",
    "        assert isinstance(actual, DataSource)\n",
    "        bg_task = b.tasks[-1]\n",
    "        display(f\"{bg_task.func=}\", f\"{bg_task.args=}\", f\"{bg_task.kwargs=}\")\n",
    "        assert bg_task.func == execute_cli\n",
    "        assert (\n",
    "            bg_task.kwargs[\"command\"]\n",
    "            == f\"process_parquet {datablob_parquet.id} {actual.id} {index_column} '{json.dumps([sort_by])}' --blocksize {blocksize} --kwargs_json '{json.dumps(kwargs)}' --deduplicate_data\"\n",
    "        ), bg_task.kwargs[\"command\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed95d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.get(\n",
    "    \"/{datablob_uuid}\", response_model=DataBlobRead, responses=get_datablob_responses  # type: ignore\n",
    ")\n",
    "def get_details_of_datablob(\n",
    "    datablob_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataBlob:\n",
    "    \"\"\"Get details of the datablob\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore\n",
    "    return datablob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52aa15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    expected = user.datablobs[0]\n",
    "\n",
    "    actual = get_details_of_datablob(\n",
    "        datablob_uuid=expected.uuid, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "    assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada680b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ExceptionInfo HTTPException(status_code=422, detail='test error') tblen=3>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DataBlob errored out while importing\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    datablob_errored = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=\"wrong_uri\",\n",
    "            access_key=\"wrong_access_key\",\n",
    "            secret_key=\"wrong_secret_key\",\n",
    "        ),\n",
    "        source=\"wrong_uri\",\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "        error=\"test error\",\n",
    "    )\n",
    "    session.add(datablob_errored)\n",
    "    session.commit()\n",
    "    session.refresh(datablob_errored)\n",
    "\n",
    "    with pytest.raises(HTTPException) as e:\n",
    "        get_details_of_datablob(\n",
    "            datablob_uuid=datablob_errored.uuid, user=user, session=session\n",
    "        )\n",
    "    display(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def delete(self: DataBlob, user: User, session: Session):\n",
    "    \"\"\"Delete a datablob\n",
    "\n",
    "    Args:\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "    delete_data_object_files_in_cloud(data_object=self)\n",
    "\n",
    "    self.disabled = True\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(self)\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.delete(\n",
    "    \"/{datablob_uuid}\",\n",
    "    response_model=DataBlobRead,\n",
    "    responses=get_datablob_responses,  # type: ignore\n",
    ")\n",
    "def delete_datablob(\n",
    "    datablob_uuid: str,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataBlob:\n",
    "    \"\"\"Delete datablob\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datablob.delete(user, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['/root/airflow_venv/bin/airflow', 'dags', 'trigger', 's3_pull-14', '--conf', '{\"datablob_id\": 14}', '--run-id', 'airt-service__2022-11-07T09:11:37.727383'], returncode=0, stdout='[\\x1b[34m2022-11-07 09:11:38,748\\x1b[0m] {\\x1b[34m__init__.py:\\x1b[0m42} INFO\\x1b[0m - Loaded API auth backend: airflow.api.auth.backend.session\\x1b[0m\\nCreated <DagRun s3_pull-14 @ 2022-11-07T09:11:38+00:00: airt-service__2022-11-07T09:11:37.727383, state:queued, queued_at: 2022-11-07 09:11:38.811204+00:00. externally triggered: True>\\n', stderr='/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n  option = self._get_environment_variables(deprecated_key, deprecated_section, key, section)\\n/root/airflow_venv/lib/python3.8/site-packages/airflow/configuration.py:545 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.\\n')\n",
      "[{'dag_id': 's3_pull-14', 'run_id': 'airt-service__2022-11-07T09:11:37.727383', 'state': 'running', 'execution_date': '2022-11-07T09:11:38+00:00', 'start_date': '2022-11-07T09:11:39.079337+00:00', 'end_date': ''}]\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-3/6/datablob/14\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-36datablob14_cached_m2ourj22\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-3/6/datablob/14 locally in /tmp/s3kumaran-airt-service-eu-west-36datablob14_cached_m2ourj22\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_a27kyen9\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_a27kyen9\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_a27kyen9\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_a27kyen9\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-36datablob14_cached_m2ourj22 to s3://kumaran-airt-service-eu-west-3/6/datablob/14\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-36datablob14_cached_m2ourj22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=14, uuid=UUID('2ae92bb7-ae8d-4198-960f-58f0dbde4899'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=1, folder_size=11219613, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-3', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-3/6/datablob/14', created=datetime.datetime(2022, 11, 7, 9, 11, 29), user_id=6, pulled_on=datetime.datetime(2022, 11, 7, 9, 11, 47), tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=14, uuid=UUID('2ae92bb7-ae8d-4198-960f-58f0dbde4899'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=1, folder_size=11219613, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-3', error=None, disabled=True, path='s3://kumaran-airt-service-eu-west-3/6/datablob/14', created=datetime.datetime(2022, 11, 7, 9, 11, 29), user_id=6, pulled_on=datetime.datetime(2022, 11, 7, 9, 11, 47), tags=[Tag(id=2, created=datetime.datetime(2022, 11, 7, 9, 9, 59), name='latest', uuid=UUID('487ec313-1f51-498e-ad17-c29a9a10aa25'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    from_s3_request = FromS3Request(\n",
    "        uri=\"s3://test-airt-service/account_312571_events\",\n",
    "        access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    )\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        datablob = from_s3_route(\n",
    "            from_s3_request=from_s3_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=BackgroundTasks(),\n",
    "        )\n",
    "\n",
    "    s3_pull(datablob_id=datablob.id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob.id)).one()\n",
    "    display(datablob)\n",
    "    actual = delete_datablob(datablob_uuid=datablob.uuid, user=user, session=session)\n",
    "    display(actual)\n",
    "    assert actual.disabled == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d888af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch(cls_method=True)\n",
    "def get_all(\n",
    "    cls: DataBlob,\n",
    "    disabled: bool,\n",
    "    completed: bool,\n",
    "    offset: int,\n",
    "    limit: int,\n",
    "    user: User,\n",
    "    session: Session,\n",
    ") -> List[DataBlob]:\n",
    "    \"\"\"Get all datablobs created by the user\n",
    "\n",
    "    Args:\n",
    "        disabled: Whether to get disabled datablobs\n",
    "        completed: Whether to include only datablobs which are successfully pulled from its source\n",
    "        offset: Offset results by given integer\n",
    "        limit: Limit results by given integer\n",
    "        user: User object\n",
    "        session: Sqlmodel session\n",
    "\n",
    "    Returns:\n",
    "        A list of datablob objects\n",
    "    \"\"\"\n",
    "    statement = select(DataBlob).where(DataBlob.user == user)\n",
    "    statement = statement.where(DataBlob.disabled == disabled)\n",
    "    if completed:\n",
    "        statement = statement.where(DataBlob.completed_steps == DataBlob.total_steps)\n",
    "    # get all data sources from db\n",
    "    return session.exec(statement.offset(offset).limit(limit)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.get(\"/\", response_model=List[DataBlobRead])\n",
    "def get_all_datablobs(\n",
    "    disabled: bool = False,\n",
    "    completed: bool = False,\n",
    "    offset: int = 0,\n",
    "    limit: int = Query(default=100, lte=100),\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> List[DataBlob]:\n",
    "    \"\"\"\n",
    "    Get all datablobs created by user\n",
    "    \"\"\"\n",
    "    user = session.merge(user)\n",
    "    return DataBlob.get_all(  # type: ignore\n",
    "        disabled=disabled,\n",
    "        completed=completed,\n",
    "        offset=offset,\n",
    "        limit=limit,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca0979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    actual = get_all_datablobs(\n",
    "        disabled=False, completed=False, offset=0, limit=1, user=user, session=session\n",
    "    )\n",
    "    display(actual)\n",
    "\n",
    "    assert len(actual) == 1\n",
    "    assert isinstance(actual[0], DataBlob)\n",
    "    assert actual[0] == user.datablobs[0], f\"{actual[0]} != {user.datablobs[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ced41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(actual)=10'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'len(actual)=0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual = get_all_datablobs(\n",
    "    disabled=False, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datablob in actual:\n",
    "    assert not datablob.disabled\n",
    "assert actual[0] == user.datablobs[0]\n",
    "actual = get_all_datablobs(\n",
    "    disabled=True, completed=False, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datablob in actual:\n",
    "    assert datablob.disabled\n",
    "\n",
    "actual = get_all_datablobs(\n",
    "    disabled=False, completed=True, offset=0, limit=10, user=user, session=session\n",
    ")\n",
    "display(f\"{len(actual)=}\")\n",
    "for datablob in actual:\n",
    "    assert datablob.completed_steps == datablob.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "\n",
    "@patch\n",
    "def tag(self: DataBlob, tag_name: str, session: Session):\n",
    "    \"\"\"Tag an existing datablob\n",
    "\n",
    "    Args:\n",
    "        tag_name: A string to tag the datablob\n",
    "        session: Sqlmodel session\n",
    "    \"\"\"\n",
    "    user_tag = Tag.get_by_name(name=tag_name, session=session)  # type: ignore\n",
    "\n",
    "    self.remove_tag_from_previous_datablobs(tag_name=user_tag.name, session=session)  # type: ignore\n",
    "    self.tags.append(user_tag)\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(self)\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@datablob_router.post(\"/{datablob_uuid}/tag\", response_model=DataBlobRead)\n",
    "def tag_datablob(\n",
    "    datablob_uuid: str,\n",
    "    tag_to_create: TagCreate,\n",
    "    user: User = Depends(get_current_active_user),\n",
    "    session: Session = Depends(get_session),\n",
    ") -> DataBlob:\n",
    "    \"\"\"Add tag to datablob\"\"\"\n",
    "    user = session.merge(user)\n",
    "    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore\n",
    "\n",
    "    return datablob.tag(tag_name=tag_to_create.name, session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81e3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBlob(id=3, uuid=UUID('5f83b4d0-4f0c-46a6-84f6-a79523e77d55'), type='s3', uri='s3://****************************************@bucket', source='s3://bucket', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path=None, created=datetime.datetime(2022, 11, 7, 9, 10, 5), user_id=6, pulled_on=None, tags=[Tag(id=10, created=datetime.datetime(2022, 11, 7, 9, 12), name='new_tag', uuid=UUID('0c1b52f8-166d-4df2-a954-237d2b5eb067'))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    datablob = user.datablobs[0]\n",
    "\n",
    "    tag_name = \"new_tag\"\n",
    "    tag_to_create = TagCreate(name=tag_name)\n",
    "    actual = tag_datablob(\n",
    "        datablob_uuid=datablob.uuid,\n",
    "        tag_to_create=tag_to_create,\n",
    "        user=user,\n",
    "        session=session,\n",
    "    )\n",
    "    display(actual)\n",
    "    tag_found = False\n",
    "    for tag in actual.tags:\n",
    "        if tag.name == tag_name:\n",
    "            tag_found = True\n",
    "            break\n",
    "    assert tag_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed6460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
