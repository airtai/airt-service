{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to interact with s3 datablob\n",
    "output-file: datablob_s3.html\n",
    "title: DataBlob S3\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7316aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7300a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import *\n",
    "\n",
    "from airt.helpers import get_s3_bucket_name_and_folder_from_uri\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath\n",
    "from fastcore.script import call_parse\n",
    "from fastcore.utils import *\n",
    "from sqlmodel import select\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt_service.aws.utils import create_s3_datablob_path\n",
    "from airt_service.azure.utils import create_azure_blob_storage_datablob_path\n",
    "from airt_service.constants import METADATA_FOLDER_PATH\n",
    "from airt_service.data.utils import (\n",
    "    calculate_data_object_folder_size_and_path,\n",
    "    calculate_data_object_pulled_on,\n",
    "    get_s3_connection_params_from_db_uri,\n",
    ")\n",
    "from airt_service.db.models import DataBlob, PredictionPush, get_session_with_context\n",
    "from airt_service.helpers import truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pytest\n",
    "from fastapi import BackgroundTasks\n",
    "\n",
    "from airt_service.aws.utils import create_s3_prediction_path\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.db.models import (\n",
    "    DataSource,\n",
    "    User,\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    ")\n",
    "from airt_service.helpers import commit_or_rollback, set_env_variable_context\n",
    "from airt_service.model.train import TrainRequest, predict_model, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4718464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uskrsfrset'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def copy_between_s3(\n",
    "    source_remote_url: str,\n",
    "    destination_remote_url: str,\n",
    "    source_access_key: Optional[str] = None,\n",
    "    source_secret_key: Optional[str] = None,\n",
    "    destination_access_key: Optional[str] = None,\n",
    "    destination_secret_key: Optional[str] = None,\n",
    "    datablob: Optional[DataBlob] = None,\n",
    "    skip_metadata_dir: Optional[bool] = False,\n",
    ") -> None:\n",
    "    \"\"\"Copy files from source S3 path and to destination S3 path\n",
    "\n",
    "    By default, all files are copied to the destination_remote_url. In case\n",
    "    the **skip_metadata_dir** flag is set to **True**, then the **.metadata_by_airt**\n",
    "    folder will not be copied to the destination_remote_url.\n",
    "\n",
    "    Args:\n",
    "        source_remote_url: S3 uri where files to copy are located\n",
    "        destination_remote_url: S3 uri to copy files\n",
    "        source_access_key: Source s3 bucket access key\n",
    "        source_secret_key: Source s3 bucket secret key\n",
    "        destination_access_key: Destination s3 bucket access key\n",
    "        destination_secret_key: Destination s3 bucket secret key\n",
    "        datablob: Optional datablob object to calculate pulled_on field\n",
    "        skip_metadata_dir: If set to **True** then the **.metadata_by_airt** folder\n",
    "            will not be copied to the destination_remote_url.\n",
    "    \"\"\"\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=destination_remote_url,\n",
    "        pull_on_enter=False,\n",
    "        push_on_exit=True,\n",
    "        exist_ok=True,\n",
    "        parents=True,\n",
    "        access_key=destination_access_key,\n",
    "        secret_key=destination_secret_key,\n",
    "    ) as destionation_s3_path:\n",
    "        sync_path = destionation_s3_path.as_path()\n",
    "        with RemotePath.from_url(\n",
    "            remote_url=source_remote_url,\n",
    "            pull_on_enter=True,\n",
    "            push_on_exit=False,\n",
    "            exist_ok=True,\n",
    "            parents=False,\n",
    "            access_key=source_access_key,\n",
    "            secret_key=source_secret_key,\n",
    "        ) as source_s3_path:\n",
    "            if datablob is not None:\n",
    "                calculate_data_object_pulled_on(datablob)\n",
    "\n",
    "            source_files = source_s3_path.as_path().iterdir()\n",
    "\n",
    "            if skip_metadata_dir:\n",
    "                source_files = [\n",
    "                    f for f in source_files if METADATA_FOLDER_PATH not in str(f)\n",
    "                ]\n",
    "\n",
    "            for f in source_files:\n",
    "                shutil.move(str(f), sync_path)\n",
    "\n",
    "        if len(list(sync_path.glob(\"*\"))) == 0:\n",
    "            raise ValueError(\n",
    "                f\"URI {source_remote_url} is invalid or no files available\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/datablob/7\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_9lt5joja\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/datablob/7 locally in /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_9lt5joja\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_9lt5joja to s3://kumaran-airt-service-eu-west-1/82/datablob/7\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_9lt5joja\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"source_remote_url='s3://kumaran-airt-service-eu-west-1/82/datablob/7'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"destination_remote_url='s3://kumaran-airt-service-eu-west-3/82/datablob/8'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-3/82/datablob/8\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_nk8597cm\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-3/82/datablob/8 locally in /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_nk8597cm\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/datablob/7\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_1lrl5t0a\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/datablob/7 locally in /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_1lrl5t0a\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/82/datablob/7 to /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_1lrl5t0a\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_1lrl5t0a\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_nk8597cm to s3://kumaran-airt-service-eu-west-3/82/datablob/8\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_nk8597cm\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-3/82/datablob/8\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_32js2_p9\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-3/82/datablob/8 locally in /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_32js2_p9\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-3/82/datablob/8 to /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_32js2_p9\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-382datablob8_cached_32js2_p9\n"
     ]
    }
   ],
   "source": [
    "# Test case for skip_metadata_dir=True\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=\"\",\n",
    "        source=\"\",\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    # Creating source bucket\n",
    "    bucket, s3_path = create_s3_datablob_path(\n",
    "        user_id=user.id, datablob_id=datablob.id, region=datablob.region\n",
    "    )\n",
    "    source_remote_url = f\"s3://{bucket.name}/{s3_path}\"\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=source_remote_url,\n",
    "        pull_on_enter=False,\n",
    "        push_on_exit=True,\n",
    "        exist_ok=True,\n",
    "        parents=True,\n",
    "    ) as cache_path:\n",
    "        processed_cache_path = cache_path.as_path()\n",
    "        (processed_cache_path / \"file-1.parquet\").touch()\n",
    "\n",
    "        metadata_folder_path = processed_cache_path / METADATA_FOLDER_PATH\n",
    "        metadata_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        (metadata_folder_path / \"metadata-1.parquet\").touch()\n",
    "        (metadata_folder_path / \"metadata-2.parquet\").touch()\n",
    "\n",
    "    # Creating destination bucket\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=\"\",\n",
    "        source=\"\",\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-3\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    destination_bucket, destination_s3_path = create_s3_datablob_path(\n",
    "        user_id=user.id, datablob_id=datablob.id, region=datablob.region\n",
    "    )\n",
    "\n",
    "    destination_remote_url = f\"s3://{destination_bucket.name}/{destination_s3_path}\"\n",
    "\n",
    "    display(f\"{source_remote_url=}\")\n",
    "    display(f\"{destination_remote_url=}\")\n",
    "\n",
    "    copy_between_s3(\n",
    "        source_remote_url=source_remote_url,\n",
    "        destination_remote_url=destination_remote_url,\n",
    "        skip_metadata_dir=True,\n",
    "    )\n",
    "\n",
    "    # Validating the contents of the destination bucket\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=destination_remote_url,\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as cache_path:\n",
    "        files = list(cache_path.as_path().rglob(\"*.*\"))\n",
    "        assert len(files) == 1, len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23662029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"source_remote_url='s3://kumaran-airt-service-eu-west-1/82/datablob/7'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"destination_remote_url='s3://kumaran-airt-service-eu-west-2/82/datablob/9'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-2/82/datablob/9\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_qqyipeti\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-2/82/datablob/9 locally in /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_qqyipeti\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/datablob/7\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_odkzp4ea\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/datablob/7 locally in /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_odkzp4ea\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/82/datablob/7 to /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_odkzp4ea\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182datablob7_cached_odkzp4ea\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_qqyipeti to s3://kumaran-airt-service-eu-west-2/82/datablob/9\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_qqyipeti\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-2/82/datablob/9\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_mpsumm9k\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-2/82/datablob/9 locally in /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_mpsumm9k\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-2/82/datablob/9 to /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_mpsumm9k\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-282datablob9_cached_mpsumm9k\n"
     ]
    }
   ],
   "source": [
    "# Test case for skip_metadata_dir=False\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=\"\",\n",
    "        source=\"\",\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-2\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    destination_bucket, destination_s3_path = create_s3_datablob_path(\n",
    "        user_id=user.id, datablob_id=datablob.id, region=datablob.region\n",
    "    )\n",
    "\n",
    "    destination_remote_url = f\"s3://{destination_bucket.name}/{destination_s3_path}\"\n",
    "\n",
    "    display(f\"{source_remote_url=}\")\n",
    "    display(f\"{destination_remote_url=}\")\n",
    "\n",
    "    copy_between_s3(\n",
    "        source_remote_url=source_remote_url,\n",
    "        destination_remote_url=destination_remote_url,\n",
    "        skip_metadata_dir=False,\n",
    "    )\n",
    "\n",
    "    # Validating the contents of the destination bucket\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=destination_remote_url,\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as cache_path:\n",
    "        files = list(cache_path.as_path().rglob(\"*.*\"))\n",
    "        assert len(files) == 4, len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse  # type: ignore\n",
    "def s3_pull(datablob_id: int) -> None:\n",
    "    \"\"\"Pull the data from s3 and updates progress in db\n",
    "\n",
    "    Args:\n",
    "        datablob_id: Id of datablob in db\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```s3_pull 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.id == datablob_id)\n",
    "        ).one()\n",
    "\n",
    "        datablob.error = None\n",
    "        datablob.completed_steps = 0\n",
    "        datablob.folder_size = None\n",
    "        datablob.path = None\n",
    "\n",
    "        (\n",
    "            uri,\n",
    "            source_access_key,\n",
    "            source_secret_key,\n",
    "        ) = get_s3_connection_params_from_db_uri(db_uri=datablob.uri)\n",
    "\n",
    "        try:\n",
    "            source_bucket, folder = get_s3_bucket_name_and_folder_from_uri(uri=uri)\n",
    "            source_remote_url = f\"s3://{source_bucket}/{folder}\"\n",
    "\n",
    "            if datablob.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datablob.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=destination_remote_url,\n",
    "                pull_on_enter=False,\n",
    "                push_on_exit=True,\n",
    "                exist_ok=True,\n",
    "                parents=True,\n",
    "            ) as destionation_remote_path:\n",
    "                sync_path = destionation_remote_path.as_path()\n",
    "                with RemotePath.from_url(\n",
    "                    remote_url=source_remote_url,\n",
    "                    pull_on_enter=True,\n",
    "                    push_on_exit=False,\n",
    "                    exist_ok=True,\n",
    "                    parents=False,\n",
    "                    access_key=source_access_key,\n",
    "                    secret_key=source_secret_key,\n",
    "                ) as source_s3_path:\n",
    "                    calculate_data_object_pulled_on(datablob)\n",
    "\n",
    "                    source_files = source_s3_path.as_path().iterdir()\n",
    "                    for f in source_files:\n",
    "                        shutil.move(str(f), sync_path)\n",
    "\n",
    "                if len(list(sync_path.glob(\"*\"))) == 0:\n",
    "                    raise ValueError(\n",
    "                        f\"URI {source_remote_url} is invalid or no files available\"\n",
    "                    )\n",
    "\n",
    "            # Calculate folder size in S3/Azure blob storage\n",
    "            calculate_data_object_folder_size_and_path(datablob)\n",
    "        except Exception as e:\n",
    "            datablob.error = truncate(str(e))\n",
    "\n",
    "        session.add(datablob)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d0317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-3/82/datablob/10\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-382datablob10_cached_tcopleiq\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-3/82/datablob/10 locally in /tmp/s3kumaran-airt-service-eu-west-382datablob10_cached_tcopleiq\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_r7a_djad\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_r7a_djad\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_r7a_djad\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_r7a_djad\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-382datablob10_cached_tcopleiq to s3://kumaran-airt-service-eu-west-3/82/datablob/10\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-382datablob10_cached_tcopleiq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=10, uuid=UUID('bbf6dad0-ea3f-4733-ab94-caec4fc5a465'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=1, folder_size=11219613, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-3', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-3/82/datablob/10', created=datetime.datetime(2022, 10, 20, 6, 41, 5), user_id=82, pulled_on=datetime.datetime(2022, 10, 20, 6, 41, 11), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-3\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path\n",
    "\n",
    "    s3_pull(datablob_id=datablob.id)\n",
    "    user_id = user.id\n",
    "    datablob_id = datablob.id\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob_id)).one()\n",
    "    display(datablob)\n",
    "    assert datablob.folder_size == 11219613, datablob.folder_size\n",
    "    assert (\n",
    "        datablob.path\n",
    "        == f\"s3://{os.environ['STORAGE_BUCKET_PREFIX']}-eu-west-3/{user_id}/datablob/{datablob.id}\"\n",
    "    ), datablob.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad25858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/82/datablob/12\n",
      "[INFO] airt.remote_path: AzureBlobPath._create_cache_path(): created cache path: /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope82datablob12_cached_w144flnx\n",
      "[INFO] airt.remote_path: AzureBlobPath.__init__(): created object for accessing https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/82/datablob/12 locally in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope82datablob12_cached_w144flnx\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_69xv4pf8\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_69xv4pf8\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_69xv4pf8\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_69xv4pf8\n",
      "[INFO] airt.remote_path: AzureBlobPath.__exit__(): pushing data from /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope82datablob12_cached_w144flnx to https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/82/datablob/12\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] airt.remote_path: AzureBlobPath._clean_up(): removing local cache path /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope82datablob12_cached_w144flnx\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=12, uuid=UUID('33e979e6-60b5-41e6-abb1-6adb1b553dea'), type='s3', uri='s3://****************************************@test-airt-service/account_312571_events', source='s3://test-airt-service/account_312571_events', total_steps=1, completed_steps=1, folder_size=11219613, cloud_provider=<CloudProvider.azure: 'azure'>, region='westeurope', error=None, disabled=False, path='https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/82/datablob/12', created=datetime.datetime(2022, 10, 20, 6, 41, 26), user_id=82, pulled_on=datetime.datetime(2022, 10, 20, 6, 41, 37), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    region = \"westeurope\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"azure\",\n",
    "        region=region,\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path\n",
    "\n",
    "    s3_pull(datablob_id=datablob.id)\n",
    "    user_id = user.id\n",
    "    datablob_id = datablob.id\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob_id)).one()\n",
    "    display(datablob)\n",
    "    assert datablob.folder_size == 11219613, datablob.folder_size\n",
    "    assert f\"{region}/{user_id}/datablob/{datablob_id}\" in datablob.path, datablob.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/datablob/14\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182datablob14_cached_lpa48fyz\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/datablob/14 locally in /tmp/s3kumaran-airt-service-eu-west-182datablob14_cached_lpa48fyz\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/folder_does_not_exists\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-servicefolder_does_not_exists_cached_b9utql4i\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/folder_does_not_exists locally in /tmp/s3test-airt-servicefolder_does_not_exists_cached_b9utql4i\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/folder_does_not_exists to /tmp/s3test-airt-servicefolder_does_not_exists_cached_b9utql4i\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-servicefolder_does_not_exists_cached_b9utql4i\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-182datablob14_cached_lpa48fyz to s3://kumaran-airt-service-eu-west-1/82/datablob/14\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182datablob14_cached_lpa48fyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=14, uuid=UUID('1fb3b435-82cd-4bd8-8d8d-5e9b3e2c76d6'), type='s3', uri='s3://****************************************@test-airt-service/folder_does_not_exists', source='s3://test-airt-service/folder_does_not_exists', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error='URI s3://test-airt-service/folder_does_not_exists is invalid or no files available', disabled=False, path=None, created=datetime.datetime(2022, 10, 20, 6, 42, 15), user_id=82, pulled_on=datetime.datetime(2022, 10, 20, 6, 42, 17), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/folder_does_not_exists\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path\n",
    "\n",
    "    s3_pull(datablob_id=datablob.id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob.id)).one()\n",
    "    display(datablob)\n",
    "    assert f\"URI {uri} is invalid or no files available\" in datablob.error\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d520db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse  # type: ignore\n",
    "def s3_push(prediction_push_id: int) -> None:\n",
    "    \"\"\"Push the data from s3 and update its progress in db\n",
    "\n",
    "    Args:\n",
    "        prediction_push_id: Id of prediction_push\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```s3_push 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        prediction_push = session.exec(\n",
    "            select(PredictionPush).where(PredictionPush.id == prediction_push_id)\n",
    "        ).one()\n",
    "\n",
    "        prediction_push.error = None\n",
    "        prediction_push.completed_steps = 0\n",
    "\n",
    "        (\n",
    "            uri,\n",
    "            destination_access_key,\n",
    "            destination_secret_key,\n",
    "        ) = get_s3_connection_params_from_db_uri(db_uri=prediction_push.uri)\n",
    "\n",
    "        try:\n",
    "            (\n",
    "                destination_bucket,\n",
    "                destination_s3_path,\n",
    "            ) = get_s3_bucket_name_and_folder_from_uri(uri=uri)\n",
    "            source_remote_url = prediction_push.prediction.path\n",
    "            destination_remote_url = f\"s3://{destination_bucket}/{destination_s3_path}\"\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=destination_remote_url,\n",
    "                pull_on_enter=False,\n",
    "                push_on_exit=True,\n",
    "                exist_ok=True,\n",
    "                parents=True,\n",
    "                access_key=destination_access_key,\n",
    "                secret_key=destination_secret_key,\n",
    "            ) as destionation_s3_path:\n",
    "                sync_path = destionation_s3_path.as_path()\n",
    "                with RemotePath.from_url(\n",
    "                    remote_url=source_remote_url,\n",
    "                    pull_on_enter=True,\n",
    "                    push_on_exit=False,\n",
    "                    exist_ok=True,\n",
    "                    parents=False,\n",
    "                ) as source_remote_path:\n",
    "                    source_files = source_remote_path.as_path().iterdir()\n",
    "                    for f in source_files:\n",
    "                        shutil.move(str(f), sync_path)\n",
    "\n",
    "                if len(list(sync_path.glob(\"*\"))) == 0:\n",
    "                    raise ValueError(\n",
    "                        f\"URI {source_remote_url} is invalid or no files available\"\n",
    "                    )\n",
    "\n",
    "            prediction_push.completed_steps = 1\n",
    "        except Exception as e:\n",
    "            prediction_push.error = truncate(str(e))\n",
    "\n",
    "        session.add(prediction_push)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2d598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"datablob=DataBlob(id=14, uuid=UUID('1fb3b435-82cd-4bd8-8d8d-5e9b3e2c76d6'), type='s3', uri='s3://****************************************@test-airt-service/folder_does_not_exists', source='s3://test-airt-service/folder_does_not_exists', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error='URI s3://test-airt-service/folder_does_not_exists is invalid or no files available', disabled=False, path=None, created=datetime.datetime(2022, 10, 20, 6, 42, 15), user_id=82, pulled_on=datetime.datetime(2022, 10, 20, 6, 42, 17), tags=[])\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 9', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 9', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(total_steps=3, created=datetime.datetime(2022, 10, 20, 6, 42, 18), error=None, uuid=UUID('bd92f179-0baa-4304-b6d4-a2fe545e4456'), disabled=False, datasource_id=6, id=9, model_id=8, path=None, cloud_provider=<CloudProvider.aws: 'aws'>, completed_steps=0, region='eu-west-1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/prediction/9\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_h5e39q4f\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/prediction/9 locally in /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_h5e39q4f\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_h7avz68b\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_h7avz68b\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_h7avz68b\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_h7avz68b\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_h5e39q4f to s3://kumaran-airt-service-eu-west-1/82/prediction/9\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_h5e39q4f\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=5, uuid=UUID('3b08e353-8917-4669-9920-8091afca616d'), uri='s3://****************************************@kumaran-airt-service-eu-west-1/82/prediction/9', total_steps=1, completed_steps=0, error=None, created=datetime.datetime(2022, 10, 20, 6, 42, 32), prediction_id=9, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/prediction/9\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached__c7pghx_\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/prediction/9 locally in /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached__c7pghx_\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/82/prediction/9\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_b15zl0mp\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/82/prediction/9 locally in /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_b15zl0mp\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/82/prediction/9 to /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_b15zl0mp\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached_b15zl0mp\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached__c7pghx_ to s3://kumaran-airt-service-eu-west-1/82/prediction/9\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-182prediction9_cached__c7pghx_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=5, uuid=UUID('3b08e353-8917-4669-9920-8091afca616d'), uri='s3://****************************************@kumaran-airt-service-eu-west-1/82/prediction/9', total_steps=1, completed_steps=1, error=None, created=datetime.datetime(2022, 10, 20, 6, 42, 32), prediction_id=9, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        display(f\"{datablob=}\")\n",
    "        datasource = DataSource(\n",
    "            datablob_id=datablob.id,\n",
    "            cloud_provider=datablob.cloud_provider,\n",
    "            region=datablob.region,\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "\n",
    "    train_request = TrainRequest(\n",
    "        data_uuid=datasource.uuid,\n",
    "        client_column=\"AccountId\",\n",
    "        target_column=\"DefinitionId\",\n",
    "        target=\"load*\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "    )\n",
    "\n",
    "    model = train_model(train_request=train_request, user=user, session=session)\n",
    "\n",
    "    b = BackgroundTasks()\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        prediction = predict_model(\n",
    "            model_uuid=model.uuid, user=user, session=session, background_tasks=b\n",
    "        )\n",
    "    display(prediction)\n",
    "\n",
    "    bucket, s3_path = create_s3_prediction_path(\n",
    "        user_id=user.id, prediction_id=prediction.id, region=prediction.region\n",
    "    )\n",
    "    copy_between_s3(\n",
    "        source_remote_url=r\"s3://test-airt-service/account_312571_events\",\n",
    "        destination_remote_url=f\"s3://{bucket.name}/{s3_path}\",\n",
    "    )\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        prediction.path = f\"s3://{bucket.name}/{s3_path}\"\n",
    "        session.add(prediction)\n",
    "\n",
    "    prediction_push = PredictionPush(\n",
    "        total_steps=1,\n",
    "        prediction_id=prediction.id,\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=f\"s3://{bucket.name}/{s3_path}\",\n",
    "            access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "    )\n",
    "    session.add(prediction_push)\n",
    "    session.commit()\n",
    "    display(prediction_push)\n",
    "\n",
    "    assert prediction_push.completed_steps == 0\n",
    "\n",
    "    s3_push(prediction_push_id=prediction_push.id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    prediction_push = session.exec(\n",
    "        select(PredictionPush).where(PredictionPush.id == prediction_push.id)\n",
    "    ).one()\n",
    "    display(prediction_push)\n",
    "    assert prediction_push.completed_steps == prediction_push.total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05290f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
