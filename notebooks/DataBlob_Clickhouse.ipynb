{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fcfeb9f3",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to interact with clickhouse database\n",
    "output-file: datablob_clickhouse.html\n",
    "title: DataBlob Clickhouse\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7316aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca2950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 11:57:14.279447: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[WARNING] airt.testing.activate_by_import: Failed to set gpu memory limit for tf; This could happen because of no gpu availability\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7300a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "from urllib.parse import quote_plus as urlquote\n",
    "from urllib.parse import unquote_plus as urlunquote\n",
    "\n",
    "import pandas as pd\n",
    "from fastcore.script import call_parse, Param\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "from sqlalchemy import create_engine, select, column, Table, MetaData, and_\n",
    "\n",
    "# from sqlmodel import create_engine, select, column, Table, MetaData, and_\n",
    "from sqlalchemy.engine import Connection\n",
    "from sqlalchemy.sql.expression import func\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt.engine.engine import get_default_engine, using_cluster\n",
    "from airt.helpers import ensure\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.azure.utils import create_azure_blob_storage_datablob_path\n",
    "from airt_service.aws.utils import create_s3_datablob_path\n",
    "from airt_service.data.utils import (\n",
    "    calculate_data_object_folder_size_and_path,\n",
    "    calculate_data_object_pulled_on,\n",
    ")\n",
    "from airt_service.db.models import get_session_with_context, DataBlob, PredictionPush\n",
    "from airt_service.helpers import (\n",
    "    truncate,\n",
    "    validate_user_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import timedelta\n",
    "from os import environ\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pytest\n",
    "from fastapi import BackgroundTasks\n",
    "\n",
    "from airt_service.aws.utils import create_s3_prediction_path\n",
    "from airt_service.data.s3 import copy_between_s3\n",
    "from airt_service.db.models import (\n",
    "    create_user_for_testing,\n",
    "    get_session,\n",
    "    DataSource,\n",
    "    User,\n",
    ")\n",
    "from airt_service.helpers import (\n",
    "    commit_or_rollback,\n",
    "    set_env_variable_context,\n",
    ")\n",
    "from airt_service.model.train import TrainRequest, train_model, predict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea2e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eldovsahbm'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing(subscription_type=\"small\")\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0559726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _create_clickhouse_connection_string(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    ") -> str:\n",
    "    # Double quoting is needed to fix a problem with special character '?' in password\n",
    "    quoted_password = urlquote(urlquote(password))\n",
    "    conn_str = (\n",
    "        f\"clickhouse+{protocol}://{username}:{quoted_password}@{host}:{port}/{database}\"\n",
    "    )\n",
    "\n",
    "    return conn_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e936378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=8123,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"http\",\n",
    ")\n",
    "assert actual == \"clickhouse+http://default:123456@localhost:8123/infobip\"\n",
    "\n",
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123456\",\n",
    "    host=\"localhost\",\n",
    "    port=9000,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"native\",\n",
    ")\n",
    "assert actual == \"clickhouse+native://default:123456@localhost:9000/infobip\"\n",
    "\n",
    "actual = _create_clickhouse_connection_string(\n",
    "    username=\"default\",\n",
    "    password=\"123?456@\",\n",
    "    host=\"localhost\",\n",
    "    port=9000,\n",
    "    database=\"infobip\",\n",
    "    #     table=\"events\",\n",
    "    protocol=\"native\",\n",
    ")\n",
    "assert (\n",
    "    actual == \"clickhouse+native://default:123%253F456%2540@localhost:9000/infobip\"\n",
    "), actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdab67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def create_db_uri_for_clickhouse_datablob(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    table: str,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    ") -> str:\n",
    "    \"\"\"Create uri for clickhouse datablob based on connection params\n",
    "\n",
    "    Args:\n",
    "        username: Username of clickhouse database\n",
    "        password: Password of clickhouse database\n",
    "        host: Host of clickhouse database\n",
    "        port: Port of clickhouse database\n",
    "        table: Table of clickhouse database\n",
    "        database: Database to use\n",
    "        protocol: Protocol to connect to clickhouse (native/http)\n",
    "\n",
    "    Returns:\n",
    "        An uri for the clickhouse datablob\n",
    "    \"\"\"\n",
    "    clickhouse_uri = _create_clickhouse_connection_string(\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "    clickhouse_uri = f\"{clickhouse_uri}/{table}\"\n",
    "    return clickhouse_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5a042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"actual_db_uri='clickhouse+native://****************************************@localhost:9000/infobip/events'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "db_test_cases = [\n",
    "    dict(\n",
    "        username=\"default\",\n",
    "        password=\"123456\",\n",
    "        host=\"localhost\",\n",
    "        port=9000,\n",
    "        database=\"infobip\",\n",
    "        table=\"events\",\n",
    "        protocol=\"native\",\n",
    "        db_uri=\"clickhouse+native://default:123456@localhost:9000/infobip/events\",\n",
    "    )\n",
    "]\n",
    "\n",
    "for test_case in db_test_cases:\n",
    "    actual_db_uri = create_db_uri_for_clickhouse_datablob(\n",
    "        username=test_case[\"username\"],\n",
    "        password=test_case[\"password\"],\n",
    "        host=test_case[\"host\"],\n",
    "        port=test_case[\"port\"],\n",
    "        table=test_case[\"table\"],\n",
    "        database=test_case[\"database\"],\n",
    "        protocol=test_case[\"protocol\"],\n",
    "    )\n",
    "    display(f\"{actual_db_uri=}\")\n",
    "    assert actual_db_uri == test_case[\"db_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6733f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _get_clickhouse_connection_params_from_db_uri(\n",
    "    db_uri: str,\n",
    ") -> Tuple[str, str, str, int, str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Function to get clickhouse connection params from db_uri of the db datablob\n",
    "\n",
    "    Args:\n",
    "        db_uri: DB uri of db datablob\n",
    "    Returns:\n",
    "        The username, password, host, port, table, database, protocol, database_server of the db datablob as a tuple\n",
    "    \"\"\"\n",
    "    result = re.search(\"(.*)\\+(.*):\\/\\/(.*):(.*)@(.*):(.*)\\/(.*)\\/(.*)\", db_uri)\n",
    "    database_server = result.group(1)  # type: ignore\n",
    "    protocol = result.group(2)  # type: ignore\n",
    "    username = result.group(3)  # type: ignore\n",
    "    password = urlunquote(urlunquote(result.group(4)))  # type: ignore\n",
    "    host = result.group(5)  # type: ignore\n",
    "    port = int(result.group(6))  # type: ignore\n",
    "    database = result.group(7)  # type: ignore\n",
    "    table = result.group(8)  # type: ignore\n",
    "    return username, password, host, port, table, database, protocol, database_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a6e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"actual_username='default'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_password = '****************************************'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_host='localhost'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'actual_port=9000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_table='events'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_database='infobip'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_protocol='native'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"actual_database_server='clickhouse'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for test_case in db_test_cases:\n",
    "    (\n",
    "        actual_username,\n",
    "        actual_password,\n",
    "        actual_host,\n",
    "        actual_port,\n",
    "        actual_table,\n",
    "        actual_database,\n",
    "        actual_protocol,\n",
    "        actual_database_server,\n",
    "    ) = _get_clickhouse_connection_params_from_db_uri(db_uri=test_case[\"db_uri\"])\n",
    "    display(\n",
    "        f\"{actual_username=}\",\n",
    "        f\"{actual_password=}\",\n",
    "        f\"{actual_host=}\",\n",
    "        f\"{actual_port=}\",\n",
    "        f\"{actual_table=}\",\n",
    "        f\"{actual_database=}\",\n",
    "        f\"{actual_protocol=}\",\n",
    "        f\"{actual_database_server=}\",\n",
    "    )\n",
    "\n",
    "    assert actual_username == test_case[\"username\"]\n",
    "    assert actual_password == test_case[\"password\"]\n",
    "    assert actual_host == test_case[\"host\"]\n",
    "    assert actual_port == test_case[\"port\"]\n",
    "    assert actual_table == test_case[\"table\"]\n",
    "    assert actual_database == test_case[\"database\"]\n",
    "    assert actual_protocol == test_case[\"protocol\"]\n",
    "    assert actual_database_server == \"clickhouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clickhouse_params_from_env_vars():\n",
    "    return dict(\n",
    "        username=environ[\"CLICKHOUSE_USERNAME\"],\n",
    "        password=environ[\"CLICKHOUSE_PASSWORD\"],\n",
    "        host=environ[\"CLICKHOUSE_HOST\"],\n",
    "        database=environ[\"CLICKHOUSE_DATABASE\"],\n",
    "        port=int(environ[\"CLICKHOUSE_PORT\"]),\n",
    "        protocol=environ[\"CLICKHOUSE_PROTOCOL\"],\n",
    "        table=environ[\"CLICKHOUSE_EVENTS_TABLE\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@contextmanager  # type: ignore\n",
    "def get_clickhouse_connection(  # type: ignore\n",
    "    *,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    "    #     verbose: bool = False,\n",
    ") -> Connection:\n",
    "    if protocol != \"native\":\n",
    "        raise ValueError()\n",
    "    conn_str = _create_clickhouse_connection_string(\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "\n",
    "    db_engine = create_engine(conn_str)\n",
    "    # args, kwargs = db_engine.dialect.create_connect_args(db_engine.url)\n",
    "    with db_engine.connect() as connection:\n",
    "        logger.info(f\"Connected to database using {db_engine}\")\n",
    "        yield connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84f3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infobip</td>\n",
       "      <td>airt_training_3m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>infobip</td>\n",
       "      <td>events_distributed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>infobip</td>\n",
       "      <td>events_distributed_new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>infobip</td>\n",
       "      <td>pera_analytics_events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>infobip</td>\n",
       "      <td>pera_analytics_events_with_external</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>system</td>\n",
       "      <td>trace_log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>system</td>\n",
       "      <td>user_directories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>system</td>\n",
       "      <td>users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>system</td>\n",
       "      <td>zeros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>system</td>\n",
       "      <td>zeros_mt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   database                                 name\n",
       "0   infobip                     airt_training_3m\n",
       "1   infobip                   events_distributed\n",
       "2   infobip               events_distributed_new\n",
       "3   infobip                pera_analytics_events\n",
       "4   infobip  pera_analytics_events_with_external\n",
       "..      ...                                  ...\n",
       "69   system                            trace_log\n",
       "70   system                     user_directories\n",
       "71   system                                users\n",
       "72   system                                zeros\n",
       "73   system                             zeros_mt\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rename events to events_distributed\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "with get_clickhouse_connection(\n",
    "    **db_params,\n",
    ") as connection:\n",
    "    assert type(connection) == Connection\n",
    "\n",
    "    query = f\"SELECT database, name from system.tables\"\n",
    "    df = pd.read_sql(sql=query, con=connection)\n",
    "    display(df)\n",
    "\n",
    "    database = db_params[\"database\"]\n",
    "    xs = df.loc[(df.database == db_params[\"database\"]) & (df.name == \"events\")]\n",
    "    if xs.shape[0] > 0:\n",
    "        query = f\"RENAME TABLE {database}.events TO {database}.events_distributed\"\n",
    "        ys = pd.read_sql(sql=query, con=connection)\n",
    "        display(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6565245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def get_max_timestamp(\n",
    "    timestamp_column: str,\n",
    "    connection: Connection,\n",
    "    table,\n",
    "    verbose: bool = False,\n",
    ") -> int:\n",
    "    engine = connection.engine\n",
    "\n",
    "    # create a Session\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "\n",
    "    metadata = MetaData(bind=None)\n",
    "    sql_table = Table(table, metadata, autoload=True, autoload_with=engine)\n",
    "\n",
    "    query = func.max(sql_table.columns[timestamp_column])\n",
    "    #     logger.info(f\"query='{query}'\")\n",
    "\n",
    "    result = session.query(query).scalar()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4367af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'actual=1624612267272'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ok'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expected = 1624612267272\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "with get_clickhouse_connection(\n",
    "    **db_params,\n",
    ") as connection:\n",
    "    actual = get_max_timestamp(\n",
    "        timestamp_column=\"OccurredTimeTicks\",\n",
    "        connection=connection,\n",
    "        table=db_params[\"table\"],\n",
    "        verbose=True,\n",
    "    )\n",
    "    display(f\"{actual=}\")\n",
    "    assert actual == expected\n",
    "\n",
    "display(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea62d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _construct_filter_query(filters: Optional[Dict[str, str]] = None) -> str:\n",
    "    filter_query = \"\"\n",
    "    if filters:\n",
    "        for column, value in filters.items():\n",
    "            filter_query = filter_query + f\" AND {column}={value}\"\n",
    "    return filter_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212937f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' AND AccountId=312571'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"filters\": {\"AccountId\": 312571},\n",
    "        \"expected\": \" AND AccountId=312571\",\n",
    "    },\n",
    "    {\n",
    "        \"filters\": {},\n",
    "        \"expected\": \"\",\n",
    "    },\n",
    "    {\n",
    "        \"filters\": None,\n",
    "        \"expected\": \"\",\n",
    "    },\n",
    "]\n",
    "for case in test_cases:\n",
    "    actual = _construct_filter_query(case[\"filters\"])\n",
    "    display(actual)\n",
    "    assert actual == case[\"expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3724a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _get_value_counts_for_index_column(\n",
    "    index_column: str,\n",
    "    timestamp_column: str,\n",
    "    filters: Optional[Dict[str, str]] = None,\n",
    "    *,\n",
    "    connection: Connection,\n",
    "    table: str,\n",
    "    max_timestamp: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Queries the database and returns a number of events for each person_id\"\"\"\n",
    "    query = f\"SELECT {index_column}, COUNT(*) AS `count` FROM {table} where {timestamp_column}<={max_timestamp}\"\n",
    "    query = query + _construct_filter_query(filters)\n",
    "    query = query + f\" GROUP BY {index_column} ORDER BY {index_column}\"\n",
    "\n",
    "    logger.info(\n",
    "        f\"Querying database to get unique person_ids and its number of events - {query=}\"\n",
    "    )\n",
    "    df = pd.read_sql(sql=query, con=connection)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Querying database to get unique person_ids and its number of events - query='SELECT PersonId, COUNT(*) AS `count` FROM airt_training_3m where OccurredTimeTicks<=1624612267272 AND AccountId=312571 GROUP BY PersonId ORDER BY PersonId'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonId</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49890</th>\n",
       "      <td>99993</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49891</th>\n",
       "      <td>99994</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49892</th>\n",
       "      <td>99995</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49893</th>\n",
       "      <td>99996</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49894</th>\n",
       "      <td>100000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49895 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PersonId  count\n",
       "0             2     14\n",
       "1             4     10\n",
       "2             8     10\n",
       "3             9     10\n",
       "4            10     10\n",
       "...         ...    ...\n",
       "49890     99993     10\n",
       "49891     99994     10\n",
       "49892     99995     10\n",
       "49893     99996     10\n",
       "49894    100000     10\n",
       "\n",
       "[49895 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "account_id = 312571\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "with get_clickhouse_connection(\n",
    "    **db_params,\n",
    ") as connection:\n",
    "    max_timestamp = get_max_timestamp(\n",
    "        timestamp_column=\"OccurredTimeTicks\",\n",
    "        connection=connection,\n",
    "        table=db_params[\"table\"],\n",
    "        verbose=True,\n",
    "    )\n",
    "    index_value_counts = _get_value_counts_for_index_column(\n",
    "        index_column=\"PersonId\",\n",
    "        timestamp_column=\"OccurredTimeTicks\",\n",
    "        filters={\"AccountId\": account_id},\n",
    "        connection=connection,\n",
    "        table=db_params[\"table\"],\n",
    "        max_timestamp=max_timestamp,\n",
    "    )\n",
    "display(index_value_counts)\n",
    "assert isinstance(index_value_counts, pd.DataFrame)\n",
    "assert len(index_value_counts.columns.to_list()) == 2\n",
    "assert \"PersonId\" in index_value_counts.columns\n",
    "assert \"count\" in index_value_counts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664cc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def partition_index_value_counts_into_chunks(\n",
    "    index_column: str,\n",
    "    index_value_counts: pd.DataFrame,\n",
    "    db_download_size: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Partition index value counts into chunks with size less than db_download_size, unless a single index has more than db_download_size events\"\"\"\n",
    "    logger.info(\"Partitioning index ids into chunks...\")\n",
    "    partitions: Dict[str, List[int]] = {\n",
    "        \"index_id_start\": [],\n",
    "        \"index_id_end\": [],\n",
    "        \"count\": [],\n",
    "    }\n",
    "\n",
    "    for index, row in index_value_counts.iterrows():\n",
    "        index_id = row[index_column]\n",
    "        count = row[\"count\"]\n",
    "        if not partitions[\"count\"]:\n",
    "            partitions[\"index_id_start\"].append(index_id)\n",
    "            partitions[\"count\"].append(0)\n",
    "\n",
    "        if (partitions[\"count\"][-1] + count) <= db_download_size:\n",
    "            partitions[\"count\"][-1] = partitions[\"count\"][-1] + count\n",
    "        else:\n",
    "            partitions[\"index_id_end\"].append(index_id)\n",
    "            partitions[\"index_id_start\"].append(index_id)\n",
    "            partitions[\"count\"].append(count)\n",
    "    partitions[\"index_id_end\"].append(index_id + 1)\n",
    "    logger.info(\"Partitioning finished\")\n",
    "    return pd.DataFrame(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6fbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PersonId</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PersonId    count\n",
       "0         1  1000000\n",
       "1         2   100000\n",
       "2         3   150000\n",
       "3         4  2000000\n",
       "4         7      150\n",
       "5         9      264"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Partitioning index ids into chunks...\n",
      "[INFO] __main__: Partitioning finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_id_start</th>\n",
       "      <th>index_id_end</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index_id_start  index_id_end    count\n",
       "0               1             2  1000000\n",
       "1               2             4   250000\n",
       "2               4             7  2000000\n",
       "3               7            10      414"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_value_counts = pd.DataFrame(\n",
    "    {\n",
    "        \"PersonId\": [1, 2, 3, 4, 7, 9],\n",
    "        \"count\": [1_000_000, 100_000, 150_000, 2_000_000, 150, 264],\n",
    "    }\n",
    ")\n",
    "display(index_value_counts)\n",
    "expected = pd.DataFrame(\n",
    "    {\n",
    "        \"index_id_start\": [1, 2, 4, 7],\n",
    "        \"index_id_end\": [2, 4, 7, 10],\n",
    "        \"count\": [1_000_000, 250_000, 2_000_000, 414],\n",
    "    }\n",
    ")\n",
    "actual = partition_index_value_counts_into_chunks(\n",
    "    index_column=\"PersonId\",\n",
    "    index_value_counts=index_value_counts,\n",
    "    db_download_size=1_000_000,\n",
    ")\n",
    "display(actual)\n",
    "pd.testing.assert_frame_equal(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75335a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def _download_from_clickhouse(\n",
    "    *,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    database: str,\n",
    "    protocol: str,\n",
    "    table: str,\n",
    "    chunksize: Optional[int] = 1_000_000,\n",
    "    index_column: str,\n",
    "    timestamp_column: str,\n",
    "    filters: Optional[Dict[str, str]] = None,\n",
    "    output_path: Path,\n",
    "    db_download_size=50_000_000,\n",
    "):\n",
    "    \"\"\"Downloads data from database and stores it as parquet files in output path\n",
    "\n",
    "    Args:\n",
    "        host: Host of db\n",
    "        port: Port of db\n",
    "        username: Username of db\n",
    "        password: Password of db\n",
    "        database: Database to use in db\n",
    "        database_server: Server/engine of db\n",
    "        table: Table to use in db\n",
    "        chunksize: Chunksize to download as\n",
    "        index_column: Column to use to partition rows and to use as index\n",
    "        timestamp_column: Timestamp column\n",
    "        filters: Additional column filters\n",
    "        output_path: Path to store parquet files\n",
    "        db_download_size: Number of rows to include in single partition\n",
    "    \"\"\"\n",
    "\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "\n",
    "        max_timestamp = get_max_timestamp(\n",
    "            timestamp_column=timestamp_column,\n",
    "            connection=connection,\n",
    "            table=table,\n",
    "        )\n",
    "        index_value_counts = _get_value_counts_for_index_column(\n",
    "            index_column=index_column,\n",
    "            timestamp_column=timestamp_column,\n",
    "            filters=filters,\n",
    "            connection=connection,\n",
    "            table=table,\n",
    "            max_timestamp=max_timestamp,\n",
    "        )\n",
    "        partitions = partition_index_value_counts_into_chunks(\n",
    "            index_column=index_column,\n",
    "            index_value_counts=index_value_counts,\n",
    "            db_download_size=db_download_size,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"{partitions.shape[0]} chunk(s) of ~{db_download_size} rows each found\"\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as td:\n",
    "            d = Path(td)\n",
    "            i = 0\n",
    "            for index, chunk in partitions.iterrows():\n",
    "                index_id_start = chunk[\"index_id_start\"]\n",
    "                index_id_end = chunk[\"index_id_end\"]\n",
    "\n",
    "                # User input is validated for SQL code injection\n",
    "                validate_user_inputs([table, timestamp_column, index_column])\n",
    "\n",
    "                query = f\"SELECT * FROM {table} FINAL WHERE {timestamp_column}<={max_timestamp} AND {index_column}>={index_id_start} AND {index_column}<{index_id_end}\"  # nosec B608\n",
    "                query = query + _construct_filter_query(filters)\n",
    "                query = query + f\" ORDER BY {index_column}, {timestamp_column}\"\n",
    "                logger.info(f\"{query=}\")\n",
    "\n",
    "                for df in pd.read_sql(sql=query, con=connection, chunksize=chunksize):\n",
    "                    fname = d / f\"clickhouse_data_{i:09d}.parquet\"\n",
    "                    logger.info(\n",
    "                        f\"Writing data retrieved from the database to temporary file {fname}\"\n",
    "                    )\n",
    "                    df.to_parquet(fname, engine=\"pyarrow\")  # type: ignore\n",
    "                    i = i + 1\n",
    "\n",
    "            engine = get_default_engine()\n",
    "            logger.info(\n",
    "                f\"Rewriting temporary parquet files from {d / f'clickhouse_data_*.parquet'} to output directory {output_path}\"\n",
    "            )\n",
    "            ddf = engine.dd.read_parquet(\n",
    "                d,\n",
    "                blocksize=None,\n",
    "            )\n",
    "            ddf.to_parquet(output_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea79a8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:39355' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Querying database to get unique person_ids and its number of events - query='SELECT PersonId, COUNT(*) AS `count` FROM airt_training_3m where OccurredTimeTicks<=1624612267272 AND AccountId=312571 GROUP BY PersonId ORDER BY PersonId'\n",
      "[INFO] __main__: Partitioning index ids into chunks...\n",
      "[INFO] __main__: Partitioning finished\n",
      "[INFO] __main__: 5 chunk(s) of ~100000 rows each found\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=2 AND PersonId<19983 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000000.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000001.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000002.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000003.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000004.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000005.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000006.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000007.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000008.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000009.parquet\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=19983 AND PersonId<39991 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000010.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000011.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000012.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000013.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000014.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000015.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000016.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000017.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000018.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000019.parquet\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=39991 AND PersonId<59977 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000020.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000021.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000022.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000023.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000024.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000025.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000026.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000027.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000028.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000029.parquet\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=59977 AND PersonId<80363 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000030.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000031.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000032.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000033.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000034.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000035.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000036.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000037.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000038.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000039.parquet\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=80363 AND PersonId<100001 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000040.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000041.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000042.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000043.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000044.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000045.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000046.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000047.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000048.parquet\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmp6vao67c7/clickhouse_data_000000049.parquet\n",
      "[INFO] __main__: Rewriting temporary parquet files from /tmp/tmp6vao67c7/clickhouse_data_*.parquet to output directory /tmp/test_clickhouse_download_4msrklxl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Path('/tmp/test_clickhouse_download_4msrklxl/part.25.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.10.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.36.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.16.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.7.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.47.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.30.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.43.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.15.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.28.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.29.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.9.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.6.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.17.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.11.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.37.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.35.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.14.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.46.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.18.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.8.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.42.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.48.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.44.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.34.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.12.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.23.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.41.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.40.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.33.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.45.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.26.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.3.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.22.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.19.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.13.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.0.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.31.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.1.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.27.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.5.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.38.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.39.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.4.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.32.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.24.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.20.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.21.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.2.parquet'),\n",
       " Path('/tmp/test_clickhouse_download_4msrklxl/part.49.parquet')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "      <th>PersonId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccountId DefinitionId        OccurredTime  OccurredTimeTicks  PersonId\n",
       "0     312571   loadTests2 2019-12-31 21:30:02      1577836802678         2\n",
       "1     312571   loadTests3 2020-01-03 23:53:22      1578104602678         2\n",
       "2     312571   loadTests1 2020-01-07 02:16:42      1578372402678         2\n",
       "3     312571   loadTests2 2020-01-10 04:40:02      1578640202678         2\n",
       "4     312571   loadTests3 2020-01-13 07:03:22      1578908002678         2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n"
     ]
    }
   ],
   "source": [
    "account_id = 312571\n",
    "\n",
    "with using_cluster(\"cpu\") as engine:\n",
    "    with tempfile.TemporaryDirectory(prefix=\"test_clickhouse_download_\") as d:\n",
    "        d = Path(d)\n",
    "        db_params = get_clickhouse_params_from_env_vars()\n",
    "        _download_from_clickhouse(\n",
    "            **db_params,\n",
    "            chunksize=10_000,\n",
    "            index_column=\"PersonId\",\n",
    "            timestamp_column=\"OccurredTimeTicks\",\n",
    "            filters={\"AccountId\": account_id},\n",
    "            output_path=d,\n",
    "            db_download_size=100_000,\n",
    "        )\n",
    "        len(d.ls())\n",
    "        display(list(d.glob(\"*\")))\n",
    "        ddf = engine.dd.read_parquet(d)\n",
    "        display(ddf.head())\n",
    "\n",
    "        files = sorted(d.glob(\"*.parquet\"))\n",
    "        assert len(files) == math.ceil(498961 / 10_000)\n",
    "        assert ddf.npartitions == len(files)\n",
    "        assert ddf.shape[0].compute() == 498961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d897405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def clickhouse_pull(\n",
    "    datablob_id: Param(\"id of datablob in db\", int),  # type: ignore\n",
    "    index_column: Param(\"column to use to partition rows and to use as index\", str),  # type: ignore\n",
    "    timestamp_column: Param(\"timestamp column\", str),  # type: ignore\n",
    "    filters_json: Param(  # type: ignore\n",
    "        \"additional column filters as json string key, value pairs\", str\n",
    "    ) = \"{}\",\n",
    "):\n",
    "    \"\"\"Pull datablob from a clickhouse database and update progress in the internal database\n",
    "\n",
    "    Args:\n",
    "        datablob_id: Id of datablob in db\n",
    "        index_column: Column to use to partition the rows and to use as the index\n",
    "        timestamp_column: Timestamp column name\n",
    "        filters_json: Additional column filters as json string\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```clickhouse_pull 1 PersonId OccurredTimeTicks {\"AccountId\":312571}\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.id == datablob_id)\n",
    "        ).one()[0]\n",
    "\n",
    "        datablob.error = None\n",
    "        datablob.completed_steps = 0\n",
    "        datablob.folder_size = None\n",
    "        datablob.path = None\n",
    "\n",
    "        (\n",
    "            username,\n",
    "            password,\n",
    "            host,\n",
    "            port,\n",
    "            table,\n",
    "            database,\n",
    "            protocol,\n",
    "            database_server,\n",
    "        ) = _get_clickhouse_connection_params_from_db_uri(datablob.uri)\n",
    "\n",
    "        try:\n",
    "            if datablob.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datablob.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_datablob_path(\n",
    "                    user_id=datablob.user.id,\n",
    "                    datablob_id=datablob.id,\n",
    "                    region=datablob.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=destination_remote_url,\n",
    "                pull_on_enter=False,\n",
    "                push_on_exit=True,\n",
    "                exist_ok=True,\n",
    "                parents=True,\n",
    "            ) as destionation_s3_path:\n",
    "                with using_cluster(\"cpu\") as engine:\n",
    "                    filters = json.loads(filters_json)\n",
    "                    _download_from_clickhouse(\n",
    "                        host=host,\n",
    "                        port=port,\n",
    "                        username=username,\n",
    "                        password=password,\n",
    "                        database=database,\n",
    "                        table=table,\n",
    "                        protocol=protocol,\n",
    "                        index_column=index_column,\n",
    "                        timestamp_column=timestamp_column,\n",
    "                        filters=filters,\n",
    "                        output_path=destionation_s3_path.as_path(),\n",
    "                    )\n",
    "                calculate_data_object_pulled_on(datablob)\n",
    "\n",
    "                if len(list(destionation_s3_path.as_path().glob(\"*\"))) == 0:\n",
    "                    raise ValueError(f\"no files to download, table is empty\")\n",
    "\n",
    "            # Calculate folder size in S3\n",
    "            calculate_data_object_folder_size_and_path(datablob)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while pulling from clickhouse - {str(e)}\")\n",
    "            datablob.error = truncate(str(e))\n",
    "        session.add(datablob)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001aeb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/4/datablob/1\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_i2082et0\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/4/datablob/1 locally in /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_i2082et0\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:41229' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Querying database to get unique person_ids and its number of events - query='SELECT PersonId, COUNT(*) AS `count` FROM airt_training_3m where OccurredTimeTicks<=1624612267272 AND AccountId=312571 GROUP BY PersonId ORDER BY PersonId'\n",
      "[INFO] __main__: Partitioning index ids into chunks...\n",
      "[INFO] __main__: Partitioning finished\n",
      "[INFO] __main__: 1 chunk(s) of ~50000000 rows each found\n",
      "[INFO] __main__: query='SELECT * FROM airt_training_3m FINAL WHERE OccurredTimeTicks<=1624612267272 AND PersonId>=2 AND PersonId<100001 AND AccountId=312571 ORDER BY PersonId, OccurredTimeTicks'\n",
      "[INFO] __main__: Writing data retrieved from the database to temporary file /tmp/tmptud7c9by/clickhouse_data_000000000.parquet\n",
      "[INFO] __main__: Rewriting temporary parquet files from /tmp/tmptud7c9by/clickhouse_data_*.parquet to output directory /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_i2082et0\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_i2082et0 to s3://kumaran-airt-service-eu-west-1/4/datablob/1\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-14datablob1_cached_i2082et0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=1, uuid=UUID('d5c19f33-00b4-413c-a1d7-544f68bd7dfe'), type='db', uri='clickhouse+native://****************************************@35.158.134.25:9000/infobip/airt_training_3m', source='clickhouse+native://35.158.134.25:9000/infobip/airt_training_3m', total_steps=1, completed_steps=1, folder_size=8896699, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/4/datablob/1', created=datetime.datetime(2023, 1, 9, 11, 57, 52), user_id=4, pulled_on=datetime.datetime(2023, 1, 9, 11, 58, 7), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()[0]\n",
    "    db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "    source = f'clickhouse+{db_params[\"protocol\"]}://{db_params[\"host\"]}:{db_params[\"port\"]}/{db_params[\"database\"]}/{db_params[\"table\"]}'\n",
    "\n",
    "    datablob = DataBlob(\n",
    "        type=\"db\",\n",
    "        uri=create_db_uri_for_clickhouse_datablob(**db_params),\n",
    "        source=source,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    assert not datablob.folder_size\n",
    "    assert not datablob.path\n",
    "\n",
    "    account_id = 312571\n",
    "    clickhouse_pull(\n",
    "        datablob_id=datablob.id,\n",
    "        index_column=\"PersonId\",\n",
    "        timestamp_column=\"OccurredTimeTicks\",\n",
    "        filters_json=json.dumps({\"AccountId\": account_id}),\n",
    "    )\n",
    "    datablob_id = datablob.id\n",
    "    user_id = user.id\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datablob = session.exec(select(DataBlob).where(DataBlob.id == datablob_id)).one()[0]\n",
    "    display(datablob)\n",
    "    assert datablob.folder_size == 8896699, datablob.folder_size\n",
    "    assert (\n",
    "        datablob.path\n",
    "        == f\"s3://{environ['STORAGE_BUCKET_PREFIX']}-eu-west-1/{user_id}/datablob/{datablob.id}\"\n",
    "    ), datablob.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d578c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>22.484040</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>2011-04-11 19:15:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>41.351746</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>2006-06-30 11:11:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>1.729123</td>\n",
       "      <td>mouse</td>\n",
       "      <td>True</td>\n",
       "      <td>1994-12-28 00:43:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3.560204</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>1993-02-04 10:29:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>29.590526</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>1993-01-17 11:54:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>12</td>\n",
       "      <td>95.680367</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>1994-09-17 19:50:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>90</td>\n",
       "      <td>98.290805</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-10-21 02:50:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>57</td>\n",
       "      <td>8.772941</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-10-07 03:01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6</td>\n",
       "      <td>47.075761</td>\n",
       "      <td>mouse</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-06-25 21:08:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>45</td>\n",
       "      <td>16.566561</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-01-15 03:39:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a          b      c      d                   e\n",
       "i                                                   \n",
       "0    39  22.484040    dog   True 2011-04-11 19:15:29\n",
       "1    84  41.351746    cat  False 2006-06-30 11:11:31\n",
       "2    89   1.729123  mouse   True 1994-12-28 00:43:53\n",
       "3     5   3.560204  horse  False 1993-02-04 10:29:01\n",
       "4    64  29.590526    dog   True 1993-01-17 11:54:39\n",
       "..   ..        ...    ...    ...                 ...\n",
       "995  12  95.680367  horse  False 1994-09-17 19:50:02\n",
       "996  90  98.290805    dog   True 2009-10-21 02:50:48\n",
       "997  57   8.772941    cat  False 2010-10-07 03:01:31\n",
       "998   6  47.075761  mouse   True 2021-06-25 21:08:21\n",
       "999  45  16.566561  horse  False 2010-01-15 03:39:09\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use in following function's test case\n",
    "from datetime import datetime\n",
    "\n",
    "size = 1000\n",
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        a=np.random.randint(100, size=size),\n",
    "        b=np.random.rand(size) * 100,\n",
    "        c=[\"dog\", \"cat\", \"mouse\", \"horse\"] * (size // 4),\n",
    "        d=[True, False] * (size // 2),\n",
    "        e=np.random.randint(\n",
    "            low=int(datetime.now().timestamp()) - 1_000_000_000,\n",
    "            high=int(datetime.now().timestamp()),\n",
    "            size=size,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "df[\"e\"] = df[\"e\"].apply(datetime.fromtimestamp)\n",
    "df.index.name = \"i\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91944833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _sql_type(xs: pd.Series) -> str:\n",
    "    dtype = str(xs.dtype)\n",
    "    if dtype.startswith(\"int\"):\n",
    "        dtype = f\"Int{dtype[3:]}\"\n",
    "    elif dtype.startswith(\"float\"):\n",
    "        dtype = f\"Float{dtype[5:]}\"\n",
    "    elif is_datetime64_any_dtype(xs):\n",
    "        dtype = \"DateTime64\"\n",
    "    elif dtype == \"object\":\n",
    "        dtype = \"String\"\n",
    "    elif dtype == \"bool\":\n",
    "        dtype = \"UInt8\"\n",
    "    else:\n",
    "        raise ValueError(dtype)\n",
    "    return dtype\n",
    "\n",
    "\n",
    "def _sql_types(df: pd.DataFrame) -> str:\n",
    "    ensure(df.index.name is not None)\n",
    "    return \", \".join(\n",
    "        [f\"{df.index.name} {_sql_type(df.index.to_series())}\"]\n",
    "        + [f\"{c} {_sql_type(df[c])}\" for c in df]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd37788",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    _sql_types(df) == \"i Int64, a Int64, b Float64, c String, d UInt8, e DateTime64\"\n",
    "), _sql_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75e0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _insert_table_query(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    *,\n",
    "    if_not_exists: bool = True,\n",
    "    engine: str = \"ReplacingMergeTree\",\n",
    ") -> str:\n",
    "    if if_not_exists:\n",
    "        if_not_exists_str = \"IF NOT EXISTS \"\n",
    "    else:\n",
    "        if_not_exists_str = \"\"\n",
    "\n",
    "    return f\"CREATE TABLE {if_not_exists_str}{table_name} ({_sql_types(df)}) ENGINE = {engine} ORDER BY {df.index.name};\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"predictions\"\n",
    "if_not_exists = True\n",
    "\n",
    "expected = \"CREATE TABLE IF NOT EXISTS predictions (i Int64, a Int64, b Float64, c String, d UInt8, e DateTime64) ENGINE = ReplacingMergeTree ORDER BY i;\"\n",
    "assert _insert_table_query(df, table_name) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49821ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _insert_table(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    *,\n",
    "    if_not_exists: bool = True,\n",
    "    engine: str = \"ReplacingMergeTree\",\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    "):\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "        if not type(connection) == Connection:\n",
    "            raise ValueError(f\"{type(connection)=} != Connection\")\n",
    "\n",
    "        query = _insert_table_query(\n",
    "            df, table_name, if_not_exists=if_not_exists, engine=engine\n",
    "        )\n",
    "        logger.info(f\"Inserting table with query={query}\")\n",
    "\n",
    "        return connection.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7429967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _drop_table(\n",
    "    table_name: str,\n",
    "    *,\n",
    "    if_exists: bool = True,\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    "):\n",
    "\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "        if not type(connection) == Connection:\n",
    "            raise ValueError(f\"{type(connection)=} != Connection\")\n",
    "\n",
    "        if if_exists:\n",
    "            if_exists_str = \"IF EXISTS \"\n",
    "        else:\n",
    "            if_exists_str = \"\"\n",
    "\n",
    "        # User input is validated for SQL code injection\n",
    "        validate_user_inputs([table_name])\n",
    "\n",
    "        # nosemgrep: python.sqlalchemy.security.sqlalchemy-execute-raw-query.sqlalchemy-execute-raw-query\n",
    "        query = f\"DROP TABLE {if_exists_str}{table_name};\"\n",
    "        logger.info(f\"Dropping table with query={query}\")\n",
    "\n",
    "        # nosemgrep: python.lang.security.audit.formatted-sql-query.formatted-sql-query\n",
    "        return connection.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dda2644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    }
   ],
   "source": [
    "table_name = \"; DROP TABLE test ;--\"\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "with pytest.raises(ValueError) as e:\n",
    "    _drop_table(table_name, **db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f12dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Inserting table with query=CREATE TABLE IF NOT EXISTS tmp_table (i Int64, a Int64, b Float64, c String, d UInt8, e DateTime64) ENGINE = ReplacingMergeTree ORDER BY i;\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumaran/.local/lib/python3.9/site-packages/clickhouse_sqlalchemy/drivers/base.py:273: SAWarning: Did not recognize type 'DateTime64(3)' of column 'e'\n",
      "  warn(\"Did not recognize type '%s' of column '%s'\" %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [i, a, b, c, d, e]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Dropping table with query=DROP TABLE IF EXISTS tmp_table;\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    }
   ],
   "source": [
    "table_name = \"tmp_table\"\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "retval = _insert_table(df, table_name, **db_params)\n",
    "assert retval.fetchall() == []\n",
    "\n",
    "with get_clickhouse_connection(**db_params) as connection:\n",
    "\n",
    "    engine = connection.engine\n",
    "\n",
    "    metadata = MetaData(bind=None)\n",
    "    table = Table(table_name, metadata, autoload=True, autoload_with=engine)\n",
    "    query = select([table.columns[c] for c in df.reset_index().columns])\n",
    "    actual = pd.read_sql(sql=query, con=connection)\n",
    "    pd.testing.assert_index_equal(actual.columns, df.reset_index().columns)\n",
    "    display(actual)\n",
    "\n",
    "retval = _drop_table(table_name, **db_params)\n",
    "assert retval.fetchall() == []\n",
    "\n",
    "with get_clickhouse_connection(**db_params) as connection:\n",
    "\n",
    "    engine = connection.engine\n",
    "\n",
    "    metadata = MetaData(bind=None)\n",
    "    with pytest.raises(Exception):\n",
    "        table = Table(table_name, metadata, autoload=True, autoload_with=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def _insert_data(\n",
    "    df: pd.DataFrame,\n",
    "    table_name: str,\n",
    "    *,\n",
    "    if_exists: str = \"append\",\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    "):\n",
    "    _insert_table(\n",
    "        df,\n",
    "        table_name,\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    )\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "        if not type(connection) == Connection:\n",
    "            raise ValueError(f\"{type(connection)=} != Connection\")\n",
    "\n",
    "        logger.info(f\"Inserting data to table '{table_name}'\")\n",
    "        df.to_sql(table_name, connection, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5252183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Dropping table with query=DROP TABLE IF EXISTS tmp_table;\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Inserting table with query=CREATE TABLE IF NOT EXISTS tmp_table (i Int64, a Int64, b Float64, c String, d UInt8, e DateTime64) ENGINE = ReplacingMergeTree ORDER BY i;\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Inserting data to table 'tmp_table'\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumaran/.local/lib/python3.9/site-packages/clickhouse_sqlalchemy/drivers/base.py:273: SAWarning: Did not recognize type 'DateTime64(3)' of column 'e'\n",
      "  warn(\"Did not recognize type '%s' of column '%s'\" %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>22.484040</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>2011-04-11 19:15:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84</td>\n",
       "      <td>41.351746</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>2006-06-30 11:11:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>1.729123</td>\n",
       "      <td>mouse</td>\n",
       "      <td>True</td>\n",
       "      <td>1994-12-28 00:43:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>3.560204</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>1993-02-04 10:29:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>29.590526</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>1993-01-17 11:54:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>12</td>\n",
       "      <td>95.680367</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>1994-09-17 19:50:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>90</td>\n",
       "      <td>98.290805</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>2009-10-21 02:50:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>57</td>\n",
       "      <td>8.772941</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-10-07 03:01:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6</td>\n",
       "      <td>47.075761</td>\n",
       "      <td>mouse</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-06-25 21:08:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>45</td>\n",
       "      <td>16.566561</td>\n",
       "      <td>horse</td>\n",
       "      <td>False</td>\n",
       "      <td>2010-01-15 03:39:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a          b      c      d                   e\n",
       "i                                                   \n",
       "0    39  22.484040    dog   True 2011-04-11 19:15:29\n",
       "1    84  41.351746    cat  False 2006-06-30 11:11:31\n",
       "2    89   1.729123  mouse   True 1994-12-28 00:43:53\n",
       "3     5   3.560204  horse  False 1993-02-04 10:29:01\n",
       "4    64  29.590526    dog   True 1993-01-17 11:54:39\n",
       "..   ..        ...    ...    ...                 ...\n",
       "995  12  95.680367  horse  False 1994-09-17 19:50:02\n",
       "996  90  98.290805    dog   True 2009-10-21 02:50:48\n",
       "997  57   8.772941    cat  False 2010-10-07 03:01:31\n",
       "998   6  47.075761  mouse   True 2021-06-25 21:08:21\n",
       "999  45  16.566561  horse  False 2010-01-15 03:39:09\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Dropping table with query=DROP TABLE IF EXISTS tmp_table;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = \"tmp_table\"\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "_drop_table(table_name, **db_params)\n",
    "\n",
    "# _insert_table(df, table_name, **db_params)\n",
    "\n",
    "_insert_data(df, table_name, **db_params)\n",
    "\n",
    "with get_clickhouse_connection(\n",
    "    **db_params,\n",
    ") as connection:\n",
    "\n",
    "    engine = connection.engine\n",
    "\n",
    "    metadata = MetaData(bind=None)\n",
    "    table = Table(table_name, metadata, autoload=True, autoload_with=engine)\n",
    "    query = select([table.columns[c] for c in df.reset_index().columns])\n",
    "    actual = pd.read_sql(sql=query, con=connection).set_index(\"i\").astype({\"d\": \"bool\"})\n",
    "\n",
    "    # todo: doesn't work with http driver\n",
    "    #     actual[\"e\"] = pd.to_datetime(actual[\"e\"])\n",
    "\n",
    "    display(actual)\n",
    "\n",
    "    pd.testing.assert_frame_equal(actual, df)\n",
    "\n",
    "_drop_table(table_name, **db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61421a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def clickhouse_push(prediction_push_id: int):  # type: ignore\n",
    "    \"\"\"Push the data to a clickhouse database\n",
    "\n",
    "    Args:\n",
    "        prediction_push_id: Id of prediction_push\n",
    "\n",
    "    Example:\n",
    "        The following code executes a CLI command:\n",
    "        ```clickhouse_push 1\n",
    "        ```\n",
    "    \"\"\"\n",
    "    with get_session_with_context() as session:\n",
    "        prediction_push = session.exec(\n",
    "            select(PredictionPush).where(PredictionPush.id == prediction_push_id)\n",
    "        ).one()[0]\n",
    "\n",
    "        prediction_push.error = None\n",
    "        prediction_push.completed_steps = 0\n",
    "\n",
    "        (\n",
    "            username,\n",
    "            password,\n",
    "            host,\n",
    "            port,\n",
    "            table,\n",
    "            database,\n",
    "            protocol,\n",
    "            database_server,\n",
    "        ) = _get_clickhouse_connection_params_from_db_uri(db_uri=prediction_push.uri)\n",
    "\n",
    "        try:\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=prediction_push.prediction.path,\n",
    "                pull_on_enter=True,\n",
    "                push_on_exit=False,\n",
    "                exist_ok=True,\n",
    "                parents=False,\n",
    "            ) as s3_path:\n",
    "                df = pd.read_parquet(s3_path.as_path())\n",
    "                _insert_table(\n",
    "                    df,\n",
    "                    table,\n",
    "                    username=username,\n",
    "                    password=password,\n",
    "                    host=host,\n",
    "                    port=port,\n",
    "                    database=database,\n",
    "                    table=table,\n",
    "                    protocol=protocol,\n",
    "                )\n",
    "            prediction_push.completed_steps = 1\n",
    "        except Exception as e:\n",
    "            prediction_push.error = truncate(str(e))\n",
    "\n",
    "        session.add(prediction_push)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c114896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt_service.batch_job: create_batch_job(): command='predict 1', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='predict 1', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(cloud_provider=<CloudProvider.aws: 'aws'>, created=datetime.datetime(2023, 1, 9, 11, 58, 32), uuid=UUID('621dead2-d539-46ee-ba52-157919b9cd6c'), datasource_id=1, error=None, disabled=False, model_id=1, path=None, total_steps=3, id=1, completed_steps=0, region='eu-west-1')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/4/prediction/1\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached_3laaaq2j\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/4/prediction/1 locally in /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached_3laaaq2j\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached_3laaaq2j to s3://kumaran-airt-service-eu-west-1/4/prediction/1\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached_3laaaq2j\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=1, uuid=UUID('f6898dee-3b0b-4967-aa56-ab6329a98bb0'), uri='clickhouse+native://****************************************@35.158.134.25:9000/infobip/test_clickhouse_push_prediction', total_steps=1, completed_steps=0, error=None, created=datetime.datetime(2023, 1, 9, 11, 58, 36), prediction_id=1, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()[0]\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        datasource = DataSource(\n",
    "            datablob_id=datablob.id,\n",
    "            cloud_provider=datablob.cloud_provider,\n",
    "            region=datablob.region,\n",
    "            total_steps=1,\n",
    "            user=user,\n",
    "        )\n",
    "\n",
    "    train_request = TrainRequest(\n",
    "        data_uuid=datasource.uuid,\n",
    "        client_column=\"AccountId\",\n",
    "        target_column=\"DefinitionId\",\n",
    "        target=\"load*\",\n",
    "        predict_after=timedelta(seconds=20 * 24 * 60 * 60),\n",
    "    )\n",
    "\n",
    "    model = train_model(train_request=train_request, user=user, session=session)\n",
    "    b = BackgroundTasks()\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        prediction = predict_model(\n",
    "            model_uuid=model.uuid, user=user, session=session, background_tasks=b\n",
    "        )\n",
    "    display(prediction)\n",
    "\n",
    "    bucket, s3_path = create_s3_prediction_path(\n",
    "        user_id=user.id, prediction_id=prediction.id, region=prediction.region\n",
    "    )\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://{bucket.name}/{s3_path}\",\n",
    "        pull_on_enter=False,\n",
    "        push_on_exit=True,\n",
    "        exist_ok=True,\n",
    "        parents=True,\n",
    "    ) as destionation_s3_path:\n",
    "        dd.from_pandas(df, npartitions=2).to_parquet(destionation_s3_path.as_path())\n",
    "\n",
    "    with commit_or_rollback(session):\n",
    "        prediction.path = f\"s3://{bucket.name}/{s3_path}\"\n",
    "        session.add(prediction)\n",
    "\n",
    "    table_name = \"test_clickhouse_push_prediction\"\n",
    "    db_params = get_clickhouse_params_from_env_vars()\n",
    "    prediction_push = PredictionPush(\n",
    "        total_steps=1,\n",
    "        prediction_id=prediction.id,\n",
    "        uri=create_db_uri_for_clickhouse_datablob(\n",
    "            table=table_name,\n",
    "            **{k: v for k, v in db_params.items() if k not in [\"table\"]},\n",
    "        ),\n",
    "    )\n",
    "    session.add(prediction_push)\n",
    "    session.commit()\n",
    "    \n",
    "    display(prediction_push)\n",
    "    assert prediction_push.completed_steps == 0\n",
    "    \n",
    "    prediction_push_id = prediction_push.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae106c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Dropping table with query=DROP TABLE IF EXISTS test_clickhouse_push_prediction;\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/4/prediction/1\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached__w31smh8\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/4/prediction/1 locally in /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached__w31smh8\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/4/prediction/1 to /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached__w31smh8\n",
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Inserting table with query=CREATE TABLE IF NOT EXISTS test_clickhouse_push_prediction (i Int64, a Int64, b Float64, c String, d UInt8, e DateTime64) ENGINE = ReplacingMergeTree ORDER BY i;\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-14prediction1_cached__w31smh8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionPush(id=1, uuid=UUID('f6898dee-3b0b-4967-aa56-ab6329a98bb0'), uri='clickhouse+native://****************************************@35.158.134.25:9000/infobip/test_clickhouse_push_prediction', total_steps=1, completed_steps=1, error=None, created=datetime.datetime(2023, 1, 9, 11, 58, 36), prediction_id=1, )"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kumaran/.local/lib/python3.9/site-packages/clickhouse_sqlalchemy/drivers/base.py:273: SAWarning: Did not recognize type 'DateTime64(3)' of column 'e'\n",
      "  warn(\"Did not recognize type '%s' of column '%s'\" %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [a, b, c, d, e]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Dropping table with query=DROP TABLE IF EXISTS test_clickhouse_push_prediction;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "_drop_table(table_name, **db_params)\n",
    "\n",
    "clickhouse_push(prediction_push_id=prediction_push_id)\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    prediction_push = session.exec(\n",
    "        select(PredictionPush).where(PredictionPush.id == prediction_push_id)\n",
    "    ).one()[0]\n",
    "    display(prediction_push)\n",
    "    assert prediction_push.completed_steps == prediction_push.total_steps\n",
    "\n",
    "    with get_clickhouse_connection(\n",
    "        **db_params,\n",
    "    ) as connection:\n",
    "\n",
    "        engine = connection.engine\n",
    "\n",
    "        metadata = MetaData(bind=None)\n",
    "        table = Table(table_name, metadata, autoload=True, autoload_with=engine)\n",
    "        query = select([table.columns[c] for c in df.reset_index().columns])\n",
    "        actual = (\n",
    "            pd.read_sql(sql=query, con=connection).set_index(\"i\").astype({\"d\": \"bool\"})\n",
    "        )\n",
    "\n",
    "        display(actual)\n",
    "\n",
    "    _drop_table(table_name, **db_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8510da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def get_count(\n",
    "    username: str,\n",
    "    password: str,\n",
    "    host: str,\n",
    "    port: int,\n",
    "    database: str,\n",
    "    table: str,\n",
    "    protocol: str,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Function to get count of all rows from given table\n",
    "\n",
    "    Args:\n",
    "        username: Username of clickhouse database\n",
    "        password: Password of clickhouse database\n",
    "        host: Host of clickhouse database\n",
    "        port: Port of clickhouse database\n",
    "        table: Table of clickhouse database\n",
    "        database: Database to use\n",
    "        protocol: Protocol to connect to clickhouse (native/http)\n",
    "\n",
    "    Returns:\n",
    "        Count of all rows for given table\n",
    "    \"\"\"\n",
    "    with get_clickhouse_connection(  # type: ignore\n",
    "        username=username,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        table=table,\n",
    "        protocol=protocol,\n",
    "    ) as connection:\n",
    "        if not type(connection) == Connection:\n",
    "            raise ValueError(f\"{type(connection)=} != Connection\")\n",
    "\n",
    "        # nosemgrep: python.sqlalchemy.security.sqlalchemy-execute-raw-query.sqlalchemy-execute-raw-query\n",
    "        query = f\"SELECT count() FROM {database}.{table}\" # nosec B608\n",
    "        logger.info(f\"Getting count with query={query}\")\n",
    "\n",
    "        # nosemgrep: python.lang.security.audit.formatted-sql-query.formatted-sql-query\n",
    "        result = connection.execute(query)\n",
    "        return result.fetchall()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd07d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Connected to database using Engine(clickhouse+native://****************************************@35.158.134.25:9000/infobip)\n",
      "[INFO] __main__: Getting count with query=SELECT count() FROM infobip.airt_training_3m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "158572720"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_params = get_clickhouse_params_from_env_vars()\n",
    "\n",
    "retval = get_count(**db_params)\n",
    "retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae699b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
