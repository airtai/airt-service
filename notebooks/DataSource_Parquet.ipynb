{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Functions to process parquet files\n",
    "output-file: datasource_parquet.html\n",
    "title: DataSource Parquet\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.testing.activate_by_import: Testing environment activated.\n",
      "[INFO] numexpr.utils: Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[INFO] numexpr.utils: NumExpr defaulting to 8 threads.\n",
      "[INFO] airt.keras.helpers: Using a single GPU #0 with memory_limit 1024 MB\n"
     ]
    }
   ],
   "source": [
    "from airt.testing import activate_by_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.data.importers: Module loaded:\n",
      "[INFO] airt.data.importers:  - using pandas     : 1.5.1\n",
      "[INFO] airt.data.importers:  - using dask       : 2022.10.0\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "\n",
    "import json\n",
    "from typing import *\n",
    "\n",
    "import airt_service.sanitizer\n",
    "from airt.data.importers import import_parquet\n",
    "from airt.logger import get_logger\n",
    "from airt.remote_path import RemotePath\n",
    "from airt_service.aws.utils import create_s3_datasource_path\n",
    "from airt_service.azure.utils import create_azure_blob_storage_datasource_path\n",
    "from airt_service.data.datasource import DataSource\n",
    "from airt_service.data.utils import (\n",
    "    calculate_azure_data_object_folder_size_and_path,\n",
    "    calculate_data_object_folder_size_and_path,\n",
    "    calculate_data_object_pulled_on,\n",
    ")\n",
    "from airt_service.db.models import DataBlob, get_session_with_context\n",
    "from airt_service.helpers import truncate\n",
    "from fastcore.script import Param, call_parse\n",
    "from fastcore.utils import *\n",
    "from sqlmodel import select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.executor.subcommand: Module loaded.\n"
     ]
    }
   ],
   "source": [
    "from os import environ\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import requests\n",
    "from airt_service.constants import DS_HEAD_FILE_NAME, METADATA_FOLDER_PATH\n",
    "from airt_service.data.azure_blob_storage import azure_blob_storage_pull\n",
    "from airt_service.data.datablob import (\n",
    "    FromAzureBlobStorageRequest,\n",
    "    from_azure_blob_storage_route,\n",
    ")\n",
    "from airt_service.data.s3 import s3_pull\n",
    "from airt_service.data.utils import create_db_uri_for_s3_datablob\n",
    "from airt_service.db.models import User, create_user_for_testing, get_session\n",
    "from airt_service.helpers import commit_or_rollback, set_env_variable_context\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.storage import StorageManagementClient\n",
    "from fastapi import BackgroundTasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xityalounu'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_username = create_user_for_testing()\n",
    "display(test_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def process_parquet(\n",
    "    datablob_id: Param(\"datablob_id\", int),  # type: ignore\n",
    "    datasource_id: Param(\"datasource_id\", int),  # type: ignore\n",
    "    *,\n",
    "    deduplicate_data: Param(\"deduplicate_data\", bool) = False,  # type: ignore\n",
    "    index_column: Param(\"index_column\", str),  # type: ignore\n",
    "    sort_by: Param(\"sort_by\", str),  # type: ignore\n",
    "    blocksize: Param(\"blocksize\", str) = \"256MB\",  # type: ignore\n",
    "    kwargs_json: Param(\"kwargs_json\", str) = \"{}\",  # type: ignore\n",
    "):\n",
    "    \"\"\"Download the user uploaded parquet file from S3, processes it and upload the processed parquet files to S3\n",
    "\n",
    "    Args:\n",
    "        datablob_id: Id of the datablob\n",
    "        datasource_id: Id of the datasource\n",
    "        deduplicate_data: If set to True, then duplicate rows are removed while uploading.\n",
    "        index_column: Name of the column used to index and partition the data into partitions\n",
    "        sort_by: Name of the column used to sort data within the same index value\n",
    "        blocksize: Size of partition\n",
    "        kwargs_json: Parameters as json string which are passed to the **dask.dataframe.read_csv()** function,\n",
    "            typically params for underlining **pd.read_csv()** from Pandas.\n",
    "    \"\"\"\n",
    "    logger.info(\n",
    "        f\"process_parquet({datablob_id=}, {datasource_id=}): processing user uploaded parquet files for {datablob_id=} and uploading parquet back to S3 for {datasource_id=}\"\n",
    "    )\n",
    "    with get_session_with_context() as session:\n",
    "        datablob = session.exec(\n",
    "            select(DataBlob).where(DataBlob.id == datablob_id)\n",
    "        ).one()\n",
    "        datasource = session.exec(\n",
    "            select(DataSource).where(DataSource.id == datasource_id)\n",
    "        ).one()\n",
    "\n",
    "        # Following is needed if datablob was created from user uploaded parquet files\n",
    "        calculate_data_object_folder_size_and_path(datablob)\n",
    "\n",
    "        datasource.error = None\n",
    "        datasource.completed_steps = 0\n",
    "        datasource.folder_size = None\n",
    "        datasource.no_of_rows = None\n",
    "        datasource.path = None\n",
    "        datasource.hash = None\n",
    "\n",
    "        try:\n",
    "            source_path = datablob.path\n",
    "            if datasource.cloud_provider == \"aws\":\n",
    "                destination_bucket, s3_path = create_s3_datasource_path(\n",
    "                    user_id=datasource.user.id,\n",
    "                    datasource_id=datasource.id,\n",
    "                    region=datasource.region,\n",
    "                )\n",
    "                destination_remote_url = f\"s3://{destination_bucket.name}/{s3_path}\"\n",
    "            elif datasource.cloud_provider == \"azure\":\n",
    "                (\n",
    "                    destination_container_client,\n",
    "                    destination_azure_blob_storage_path,\n",
    "                ) = create_azure_blob_storage_datasource_path(\n",
    "                    user_id=datasource.user.id,\n",
    "                    datasource_id=datasource.id,\n",
    "                    region=datasource.region,\n",
    "                )\n",
    "                destination_remote_url = f\"{destination_container_client.url}/{destination_azure_blob_storage_path}\"\n",
    "            logger.info(\n",
    "                f\"process_parquet({datablob_id=}, {datasource_id=}): step 1/4: downloading user uploaded file from bucket {source_path}\"\n",
    "            )\n",
    "\n",
    "            with RemotePath.from_url(\n",
    "                remote_url=source_path,\n",
    "                pull_on_enter=True,\n",
    "                push_on_exit=False,\n",
    "                exist_ok=True,\n",
    "                parents=False,\n",
    "            ) as source_s3_path:\n",
    "                calculate_data_object_pulled_on(datasource)\n",
    "                user_uploaded_parquet_files = list(\n",
    "                    source_s3_path.as_path().glob(\"*.parquet\")\n",
    "                )\n",
    "                if len(user_uploaded_parquet_files) == 0:\n",
    "                    raise ValueError(\"parquet files not found\")\n",
    "\n",
    "                logger.info(\n",
    "                    f\"process_parquet({datablob_id=}, {datasource_id=}): step 2/4: processing parquet files\"\n",
    "                )\n",
    "\n",
    "                with RemotePath.from_url(\n",
    "                    remote_url=destination_remote_url,\n",
    "                    pull_on_enter=False,\n",
    "                    push_on_exit=True,\n",
    "                    exist_ok=True,\n",
    "                    parents=True,\n",
    "                ) as destination_path:\n",
    "                    processed_path = destination_path.as_path()\n",
    "                    kwargs = json.loads(kwargs_json)\n",
    "                    try:\n",
    "                        sort_by = json.loads(sort_by)\n",
    "                    except json.JSONDecodeError:\n",
    "                        pass\n",
    "                    import_parquet(\n",
    "                        input_path=source_s3_path.as_path(),\n",
    "                        output_path=processed_path,\n",
    "                        index_column=index_column,\n",
    "                        sort_by=sort_by,\n",
    "                        blocksize=blocksize,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "                    datasource.calculate_properties(processed_path)\n",
    "\n",
    "                    if not len(list(processed_path.glob(\"*.parquet\"))) > 0:\n",
    "                        raise ValueError(\n",
    "                            f\"processing failed; parquet files not available\"\n",
    "                        )\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"process_parquet({datablob_id=}, {datasource_id=}): step 3/4: uploading parquet files back to path {destination_path}\"\n",
    "                    )\n",
    "            logger.info(\n",
    "                f\"process_parquet({datablob_id=}, {datasource_id=}): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\"\n",
    "            )\n",
    "\n",
    "            calculate_data_object_folder_size_and_path(datasource)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"process_parquet({datasource_id=}): error: {str(e)}\")\n",
    "            datasource.error = truncate(str(e))\n",
    "        logger.info(f\"process_parquet({datablob_id=}, {datasource_id=}): completed\")\n",
    "        session.add(datablob)\n",
    "        session.add(datasource)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] botocore.credentials: Found credentials in environment variables.\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/131/datablob/50\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_j5v7or1k\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/131/datablob/50 locally in /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_j5v7or1k\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://test-airt-service/account_312571_events\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3test-airt-serviceaccount_312571_events_cached_iir5s2mh\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://test-airt-service/account_312571_events locally in /tmp/s3test-airt-serviceaccount_312571_events_cached_iir5s2mh\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://test-airt-service/account_312571_events to /tmp/s3test-airt-serviceaccount_312571_events_cached_iir5s2mh\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3test-airt-serviceaccount_312571_events_cached_iir5s2mh\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_j5v7or1k to s3://kumaran-airt-service-eu-west-1/131/datablob/50\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_j5v7or1k\n"
     ]
    }
   ],
   "source": [
    "# Create a datablob and upload multiple parquet files using presigned url\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"s3://test-airt-service/account_312571_events\"\n",
    "    datablob = DataBlob(\n",
    "        type=\"s3\",\n",
    "        uri=create_db_uri_for_s3_datablob(\n",
    "            uri=uri,\n",
    "            access_key=environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "            secret_key=environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        ),\n",
    "        source=uri,\n",
    "        cloud_provider=\"aws\",\n",
    "        region=\"eu-west-1\",\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    with commit_or_rollback(session):\n",
    "        session.add(datablob)\n",
    "\n",
    "    s3_pull(datablob_id=datablob.id)\n",
    "\n",
    "    datablob_id = (\n",
    "        session.exec(select(DataBlob).where(DataBlob.uuid == datablob.uuid)).one().id\n",
    "    )\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=datablob.cloud_provider,\n",
    "        region=datablob.region,\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    datasource_id = datasource.id\n",
    "    datablob_id = datablob.id\n",
    "    user_id = user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): processing user uploaded parquet files for datablob_id=50 and uploading parquet back to S3 for datasource_id=30\n",
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): step 1/4: downloading user uploaded file from bucket s3://kumaran-airt-service-eu-west-1/131/datablob/50\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/131/datablob/50\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_gbxl8lmn\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/131/datablob/50 locally in /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_gbxl8lmn\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/131/datablob/50 to /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_gbxl8lmn\n",
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): step 2/4: processing parquet files\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/131/datasource/30\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/131/datasource/30 locally in /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r\n",
      "[INFO] airt.data.importers: import_parquet(): importing parquet file(s) from /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_gbxl8lmn using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:45019' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:35771' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:45349' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): completed, the final data is stored in /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): step 3/4: uploading parquet files back to path S3Path(enter_count=1, remote_url=s3://kumaran-airt-service-eu-west-1/131/datasource/30, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r, access_key=None, secret_key=None)\n",
      "[INFO] airt.remote_path: S3Path.__exit__(): pushing data from /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r to s3://kumaran-airt-service-eu-west-1/131/datasource/30\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_xobg6a6r\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1131datablob50_cached_gbxl8lmn\n",
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] __main__: process_parquet(datablob_id=50, datasource_id=30): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=30, uuid=UUID('042590ba-afaf-4aff-8750-84de221e73dc'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.aws: 'aws'>, region='eu-west-1', error=None, disabled=False, path='s3://kumaran-airt-service-eu-west-1/131/datasource/30', created=datetime.datetime(2022, 10, 27, 8, 10, 36), user_id=131, pulled_on=datetime.datetime(2022, 10, 27, 8, 10, 46), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/131/datasource/30\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_oa1qbq69\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/131/datasource/30 locally in /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_oa1qbq69\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/131/datasource/30 to /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_oa1qbq69\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountId</th>\n",
       "      <th>DefinitionId</th>\n",
       "      <th>OccurredTime</th>\n",
       "      <th>OccurredTimeTicks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PersonId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2019-12-31 21:30:02</td>\n",
       "      <td>1577836802678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-03 23:53:22</td>\n",
       "      <td>1578104602678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-07 02:16:42</td>\n",
       "      <td>1578372402678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-10 04:40:02</td>\n",
       "      <td>1578640202678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-13 07:03:22</td>\n",
       "      <td>1578908002678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-16 09:26:42</td>\n",
       "      <td>1579175802678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-19 11:50:02</td>\n",
       "      <td>1579443602678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests3</td>\n",
       "      <td>2020-01-22 14:13:22</td>\n",
       "      <td>1579711402678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests1</td>\n",
       "      <td>2020-01-25 16:36:42</td>\n",
       "      <td>1579979202678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312571</td>\n",
       "      <td>loadTests2</td>\n",
       "      <td>2020-01-28 19:00:02</td>\n",
       "      <td>1580247002678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AccountId DefinitionId        OccurredTime  OccurredTimeTicks\n",
       "PersonId                                                               \n",
       "2            312571   loadTests2 2019-12-31 21:30:02      1577836802678\n",
       "2            312571   loadTests3 2020-01-03 23:53:22      1578104602678\n",
       "2            312571   loadTests1 2020-01-07 02:16:42      1578372402678\n",
       "2            312571   loadTests2 2020-01-10 04:40:02      1578640202678\n",
       "2            312571   loadTests3 2020-01-13 07:03:22      1578908002678\n",
       "2            312571   loadTests1 2020-01-16 09:26:42      1579175802678\n",
       "2            312571   loadTests2 2020-01-19 11:50:02      1579443602678\n",
       "2            312571   loadTests3 2020-01-22 14:13:22      1579711402678\n",
       "2            312571   loadTests1 2020-01-25 16:36:42      1579979202678\n",
       "2            312571   loadTests2 2020-01-28 19:00:02      1580247002678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url s3://kumaran-airt-service-eu-west-1/131/datasource/30\n",
      "[INFO] airt.remote_path: S3Path._create_cache_path(): created cache path: /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_om_fvjrc\n",
      "[INFO] airt.remote_path: S3Path.__init__(): created object for accessing s3://kumaran-airt-service-eu-west-1/131/datasource/30 locally in /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_om_fvjrc\n",
      "[INFO] airt.remote_path: S3Path.__enter__(): pulling data from s3://kumaran-airt-service-eu-west-1/131/datasource/30 to /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_om_fvjrc\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_om_fvjrc\n",
      "[INFO] airt.remote_path: S3Path._clean_up(): removing local cache path /tmp/s3kumaran-airt-service-eu-west-1131datasource30_cached_oa1qbq69\n"
     ]
    }
   ],
   "source": [
    "# Test process_parquet\n",
    "process_parquet(\n",
    "    datablob_id=datablob_id,\n",
    "    datasource_id=datasource_id,\n",
    "    deduplicate_data=True,\n",
    "    index_column=\"PersonId\",\n",
    "    sort_by=\"OccurredTime\",\n",
    "    blocksize=\"256MB\",\n",
    "    kwargs_json=json.dumps(\n",
    "        dict(\n",
    "            usecols=[0, 1, 2, 3, 4],\n",
    "            parse_dates=[\"OccurredTime\"],\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_id)\n",
    "    ).one()\n",
    "    display(datasource)\n",
    "    assert (\n",
    "        datasource.folder_size == 6619982\n",
    "    ), f\"{datasource=}, {datasource.folder_size=}\"\n",
    "    assert datasource.no_of_rows == 498961\n",
    "    assert (\n",
    "        datasource.path\n",
    "        == f\"s3://{environ['STORAGE_BUCKET_PREFIX']}-eu-west-1/{user_id}/datasource/{datasource.id}\"\n",
    "    ), datasource.path\n",
    "    assert datasource.hash == \"1dd8ee7a0f96a48110dec6e25891d18d\", datasource.hash\n",
    "\n",
    "    destination_bucket, parquet_s3_path = create_s3_datasource_path(\n",
    "        user_id=datasource.user.id,\n",
    "        datasource_id=datasource.id,\n",
    "        region=datasource.region,\n",
    "    )\n",
    "\n",
    "    # tests for datasource head and dtypes\n",
    "    with RemotePath.from_url(\n",
    "        remote_url=f\"s3://{destination_bucket.name}/{parquet_s3_path}\",\n",
    "        pull_on_enter=True,\n",
    "        push_on_exit=False,\n",
    "        exist_ok=True,\n",
    "        parents=False,\n",
    "    ) as s3_path:\n",
    "        ddf = dd.read_parquet(s3_path.as_path())\n",
    "        ddf_head = ddf.head(n=10)\n",
    "        display(ddf_head)\n",
    "\n",
    "        with RemotePath.from_url(\n",
    "            remote_url=f\"{datasource.path}\",\n",
    "            pull_on_enter=True,\n",
    "            push_on_exit=False,\n",
    "            exist_ok=True,\n",
    "            parents=False,\n",
    "        ) as test_s3_info_path:\n",
    "            processed_test_s3_info_path = test_s3_info_path.as_path()\n",
    "\n",
    "            head_df = pd.read_parquet(\n",
    "                processed_test_s3_info_path / METADATA_FOLDER_PATH / DS_HEAD_FILE_NAME\n",
    "            )\n",
    "            assert head_df.index.name == \"PersonId\"\n",
    "            assert head_df.shape == (10, 4)\n",
    "\n",
    "            dtypes_dict = head_df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "            assert dtypes_dict == {\n",
    "                \"AccountId\": \"int64\",\n",
    "                \"DefinitionId\": \"object\",\n",
    "                \"OccurredTime\": \"datetime64[ns]\",\n",
    "                \"OccurredTimeTicks\": \"int64\",\n",
    "            }, dtypes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] airt_service.batch_job: create_batch_job(): command='azure_blob_storage_pull 52', task='csv_processing'\n",
      "[INFO] airt_service.batch_job_components.base: Entering FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job: batch_ctx=FastAPIBatchJobContext(task=csv_processing)\n",
      "[INFO] airt_service.batch_job_components.fastapi: FastAPIBatchJobContext.create_job(self=FastAPIBatchJobContext(task=csv_processing), command='azure_blob_storage_pull 52', environment_vars={'AWS_ACCESS_KEY_ID': '********************', 'AWS_SECRET_ACCESS_KEY': '****************************************', 'AWS_DEFAULT_REGION': 'eu-west-1', 'AZURE_SUBSCRIPTION_ID': '************************************', 'AZURE_TENANT_ID': '************************************', 'AZURE_CLIENT_ID': '************************************', 'AZURE_CLIENT_SECRET': '****************************************', 'AZURE_STORAGE_ACCOUNT_PREFIX': 'kumsairtsdev', 'AZURE_RESOURCE_GROUP': 'kumaran-airt-service-dev', 'STORAGE_BUCKET_PREFIX': 'kumaran-airt-service', 'DB_USERNAME': 'root', 'DB_PASSWORD': '****************************************', 'DB_HOST': 'kumaran-mysql', 'DB_PORT': '3306', 'DB_DATABASE': 'airt_service', 'DB_DATABASE_SERVER': 'mysql'})\n",
      "[INFO] airt_service.batch_job_components.base: Exiting FastAPIBatchJobContext(task=csv_processing): exc_type=None, exc=None, None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataBlob(id=52, uuid=UUID('be427bf1-11c5-47c1-b725-99ac8456fe60'), type='azure_blob_storage', uri='https://****************************************@testairtservice.blob.core.windows.net/test-container/account_312571_events', source='https://testairtservice.blob.core.windows.net/test-container/account_312571_events', total_steps=1, completed_steps=0, folder_size=None, cloud_provider=<CloudProvider.azure: 'azure'>, region='westeurope', error=None, disabled=False, path=None, created=datetime.datetime(2022, 10, 27, 8, 11, 46), user_id=131, pulled_on=None, tags=[Tag(uuid=UUID('29dc9d93-1484-46f6-85dd-e5e8b9e57319'), name='latest', id=2, created=datetime.datetime(2022, 10, 27, 7, 59, 7)), Tag(uuid=UUID('3bd8829c-d4e4-4e27-af72-13a75e4df15f'), name='my_azure_blob_storage_datablob_tag', id=6, created=datetime.datetime(2022, 10, 27, 8, 0, 49))])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52\n",
      "[INFO] airt.remote_path: AzureBlobPath._create_cache_path(): created cache path: /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_ms1bfay0\n",
      "[INFO] airt.remote_path: AzureBlobPath.__init__(): created object for accessing https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52 locally in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_ms1bfay0\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url https://testairtservice.blob.core.windows.net/test-container/account_312571_events\n",
      "[INFO] airt.remote_path: AzureBlobPath._create_cache_path(): created cache path: /tmp/httpstestairtserviceblobcorewindowsnettest-containeraccount_312571_events_cached_pjjrvncg\n",
      "[INFO] airt.remote_path: AzureBlobPath.__init__(): created object for accessing https://testairtservice.blob.core.windows.net/test-container/account_312571_events locally in /tmp/httpstestairtserviceblobcorewindowsnettest-containeraccount_312571_events_cached_pjjrvncg\n",
      "[INFO] airt.remote_path: AzureBlobPath.__enter__(): pulling data from https://testairtservice.blob.core.windows.net/test-container/account_312571_events to /tmp/httpstestairtserviceblobcorewindowsnettest-containeraccount_312571_events_cached_pjjrvncg\n",
      "[INFO] airt.remote_path: AzureBlobPath._clean_up(): removing local cache path /tmp/httpstestairtserviceblobcorewindowsnettest-containeraccount_312571_events_cached_pjjrvncg\n",
      "[INFO] airt.remote_path: AzureBlobPath.__exit__(): pushing data from /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_ms1bfay0 to https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] airt.remote_path: AzureBlobPath._clean_up(): removing local cache path /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_ms1bfay0\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n"
     ]
    }
   ],
   "source": [
    "# Create azure datablob and datasource from it\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    user = session.exec(select(User).where(User.username == test_username)).one()\n",
    "    uri = \"https://testairtservice.blob.core.windows.net/test-container/account_312571_events\"\n",
    "\n",
    "    storage_client = StorageManagementClient(\n",
    "        DefaultAzureCredential(), environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
    "    )\n",
    "    keys = storage_client.storage_accounts.list_keys(\n",
    "        \"test-airt-service\", \"testairtservice\"\n",
    "    )\n",
    "    credential = keys.keys[0].value\n",
    "\n",
    "    from_azure_blob_storage_request = FromAzureBlobStorageRequest(\n",
    "        uri=uri,\n",
    "        credential=credential,\n",
    "        region=\"westeurope\",\n",
    "        tag=\"my_azure_blob_storage_datablob_tag\",\n",
    "    )\n",
    "    b = BackgroundTasks()\n",
    "\n",
    "    # Test using FastAPIBatchJobContext with set_env_variable_context\n",
    "    with set_env_variable_context(variable=\"JOB_EXECUTOR\", value=\"fastapi\"):\n",
    "        datablob = from_azure_blob_storage_route(\n",
    "            from_azure_blob_storage_request=from_azure_blob_storage_request,\n",
    "            user=user,\n",
    "            session=session,\n",
    "            background_tasks=b,\n",
    "        )\n",
    "    display(datablob)\n",
    "\n",
    "    azure_blob_storage_pull(datablob_id=datablob.id)\n",
    "\n",
    "    datablob_id = (\n",
    "        session.exec(select(DataBlob).where(DataBlob.uuid == datablob.uuid)).one().id\n",
    "    )\n",
    "    datasource = DataSource(\n",
    "        datablob_id=datablob_id,\n",
    "        cloud_provider=datablob.cloud_provider,\n",
    "        region=datablob.region,\n",
    "        total_steps=1,\n",
    "        user=user,\n",
    "    )\n",
    "    session.add(datasource)\n",
    "    session.commit()\n",
    "\n",
    "    datasource_id = datasource.id\n",
    "    datablob_id = datablob.id\n",
    "    user_id = user.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): processing user uploaded parquet files for datablob_id=52 and uploading parquet back to S3 for datasource_id=31\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): step 1/4: downloading user uploaded file from bucket https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52\n",
      "[INFO] airt.remote_path: AzureBlobPath._create_cache_path(): created cache path: /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_7scgll4r\n",
      "[INFO] airt.remote_path: AzureBlobPath.__init__(): created object for accessing https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52 locally in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_7scgll4r\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.remote_path: AzureBlobPath.__enter__(): pulling data from https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datablob/52 to /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_7scgll4r\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): step 2/4: processing parquet files\n",
      "[INFO] airt.remote_path: RemotePath.from_url(): creating remote path with the following url https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datasource/31\n",
      "[INFO] airt.remote_path: AzureBlobPath._create_cache_path(): created cache path: /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx\n",
      "[INFO] airt.remote_path: AzureBlobPath.__init__(): created object for accessing https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datasource/31 locally in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] airt.data.importers: import_parquet(): importing parquet file(s) from /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_7scgll4r using blocksize='256MB' and kwargs={'usecols': [0, 1, 2, 3, 4], 'parse_dates': ['OccurredTime']} and storing result in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:37443' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 1/5: importing data and storing it into partitioned Parquet files\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:35425' processes=4 threads=4, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 2/5: indexing data by PersonId.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] airt.dask_manager: Starting cluster...\n",
      "[INFO] airt.dask_manager: Cluster started: <Client: 'tcp://127.0.0.1:37937' processes=8 threads=8, memory=22.89 GiB>\n",
      "Cluster dashboard: http://127.0.0.1:8787/status\n",
      "[INFO] airt.data.importers: import_parquet(): step 3/5: deduplicating and sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): step 4/5: repartitioning data.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): step 5/5: sorting data by PersonId and OccurredTime.\n",
      "[INFO] airt.data.importers:  - number of rows: 498,961\n",
      "[INFO] airt.data.importers: import_parquet(): completed, the final data is stored in /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx as Parquet files with:\n",
      "[INFO] airt.data.importers:  - dtypes={'AccountId': dtype('int64'), 'DefinitionId': dtype('O'), 'OccurredTime': dtype('<M8[ns]'), 'OccurredTimeTicks': dtype('int64')}\n",
      "[INFO] airt.data.importers:  - npartitions=1\n",
      "[INFO] airt.data.importers:  - partition_sizes={0: 498961}\n",
      "[INFO] airt.dask_manager: Starting stopping cluster...\n",
      "[INFO] airt.dask_manager: Cluster stopped\n",
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): step 3/4: uploading parquet files back to path AzureBlobPath(enter_count=1, remote_url=https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datasource/31, pull_on_enter=False, push_on_exit=True, cache_path=/tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx, credential=<azure.identity._credentials.default.DefaultAzureCredential object>)\n",
      "[INFO] airt.remote_path: AzureBlobPath.__exit__(): pushing data from /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx to https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datasource/31\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] airt.remote_path: AzureBlobPath._clean_up(): removing local cache path /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datasource31_cached_s5yo9ygx\n",
      "[INFO] airt.remote_path: AzureBlobPath._clean_up(): removing local cache path /tmp/httpskumsairtsdevwesteuropeblobcorewindowsnetkumsairtsdevwesteurope131datablob52_cached_7scgll4r\n",
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): step 4/4: calculating datasource attributes - folder_size, no_of_rows, head, hash\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.default: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] azure.identity._credentials.environment: Environment is configured for ClientSecretCredential\n",
      "[INFO] azure.identity._credentials.managed_identity: ManagedIdentityCredential will use IMDS\n",
      "[INFO] azure.identity._credentials.chained: DefaultAzureCredential acquired a token from EnvironmentCredential\n",
      "[INFO] __main__: process_parquet(datablob_id=52, datasource_id=31): completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataSource(id=31, uuid=UUID('5634e432-0102-4fd8-94fe-d5996b62fd86'), hash='1dd8ee7a0f96a48110dec6e25891d18d', total_steps=1, completed_steps=1, folder_size=6619982, no_of_rows=498961, cloud_provider=<CloudProvider.azure: 'azure'>, region='westeurope', error=None, disabled=False, path='https://kumsairtsdevwesteurope.blob.core.windows.net/kumsairtsdevwesteurope/131/datasource/31', created=datetime.datetime(2022, 10, 27, 8, 12, 17), user_id=131, pulled_on=datetime.datetime(2022, 10, 27, 8, 12, 32), tags=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test process_parquet for azure datablob's datasource\n",
    "process_parquet(\n",
    "    datablob_id=datablob_id,\n",
    "    datasource_id=datasource_id,\n",
    "    deduplicate_data=True,\n",
    "    index_column=\"PersonId\",\n",
    "    sort_by=\"OccurredTime\",\n",
    "    blocksize=\"256MB\",\n",
    "    kwargs_json=json.dumps(\n",
    "        dict(\n",
    "            usecols=[0, 1, 2, 3, 4],\n",
    "            parse_dates=[\"OccurredTime\"],\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "with get_session_with_context() as session:\n",
    "    datasource = session.exec(\n",
    "        select(DataSource).where(DataSource.id == datasource_id)\n",
    "    ).one()\n",
    "    display(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
