# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/Training_Status_Process.ipynb.

# %% auto 0
__all__ = ['get_recent_event_for_user', 'get_count_from_training_data_ch_table', 'get_user', 'process_row', 'process_dataframes',
           'process_training_status']

# %% ../notebooks/Training_Status_Process.ipynb 2
import asyncio
import random
import traceback
from datetime import datetime, timedelta
from os import environ
from time import sleep
from typing import *

import numpy as np
import pandas as pd
from airt.logger import get_logger
from airt.patching import patch
from asyncer import asyncify, create_task_group
from fast_kafka_api.application import FastKafkaAPI
from fastapi import FastAPI
from sqlalchemy import create_engine as sqlalchemy_create_engine
from sqlalchemy.exc import NoResultFound
from sqlmodel import Session, func, select

import airt_service
from .data.clickhouse import get_count_for_account_ids
from airt_service.db.models import (
    TrainingStreamStatus,
    User,
    create_connection_string,
    get_db_params_from_env_vars,
    get_engine,
    get_session_with_context,
)
from .users import User

# %% ../notebooks/Training_Status_Process.ipynb 5
logger = get_logger(__name__)

# %% ../notebooks/Training_Status_Process.ipynb 6
@patch(cls_method=True)
def _create(
    cls: TrainingStreamStatus,
    *,
    account_id: int,
    application_id: Optional[str] = None,
    model_id: str,
    model_type: str,
    event: str,
    count: int,
    total: int,
    user: User,
) -> TrainingStreamStatus:
    """
    Method to create event

    Args:
        account_id: account id
        application_id: Id of the application in case there is more than one for the AccountId
        model_id: User supplied ID of the model trained
        model_type: Model type
        event: one of start, upload, end
        count: current count of rows in clickhouse db
        total: total no. of rows sent by user
        user: user object
        session: session object

    Returns:
        created object of type TrainingStreamStatus
    """
    with get_session_with_context() as session:
        training_event = TrainingStreamStatus(
            account_id=account_id,
            application_id=application_id,
            model_id=model_id,
            model_type=model_type,
            event=event,
            count=count,
            total=total,
            user_id=user.id,
        )
        session.add(training_event)
        session.commit()
        session.refresh(training_event)

    return training_event

# %% ../notebooks/Training_Status_Process.ipynb 9
def get_recent_event_for_user(user: User) -> pd.DataFrame:
    """
    Get recent event for user

    Args:
        user: user object to get recent events

    Returns:
        A list of recent events for given user
    """
    conn_str = create_connection_string(**get_db_params_from_env_vars())  # type: ignore
    sqlalchemy_engine = sqlalchemy_create_engine(conn_str)

    # Get all rows from table
    df = pd.read_sql_table(table_name="trainingstreamstatus", con=sqlalchemy_engine)

    sqlalchemy_engine.dispose()

    # Filter events for given user and group by account_id
    events_for_user = (
        df.loc[df["user_id"] == user.id]
        .sort_values(["created", "id"], ascending=[False, False])
        .groupby(
            by=["account_id", "application_id", "model_id"],
            as_index=False,
            dropna=False,
        )
        .first()
    )
    events_for_user.rename(
        columns={"count": "prev_count", "account_id": "AccountId"}, inplace=True
    )
    events_for_user.set_index("AccountId", inplace=True)

    # Leave 'end' events
    return events_for_user.loc[events_for_user["event"] != "end"].sort_values(
        ["AccountId"], ascending=[True]
    )

# %% ../notebooks/Training_Status_Process.ipynb 11
def get_count_from_training_data_ch_table(
    account_ids: List[Union[int, str]]
) -> pd.DataFrame:
    """
    Get count of all rows for given account ids from clickhouse table

    Args:
        account_ids: List of account_ids to get count

    Returns:
        Count for the given account id
    """
    return airt_service.data.clickhouse.get_count_for_account_ids(
        account_ids=account_ids,
        username=environ["KAFKA_CH_USERNAME"],
        password=environ["KAFKA_CH_PASSWORD"],
        host=environ["KAFKA_CH_HOST"],
        port=int(environ["KAFKA_CH_PORT"]),
        database=environ["KAFKA_CH_DATABASE"],
        table=environ["KAFKA_CH_TABLE"],
        protocol=environ["KAFKA_CH_PROTOCOL"],
    )

# %% ../notebooks/Training_Status_Process.ipynb 13
def get_user(username: str) -> User:
    """Get the user object for the given username

    Args:
        username: Username as a string

    Returns:
        The user object
    """
    with get_session_with_context() as session:
        user = session.exec(select(User).where(User.username == username)).one()

    return user

# %% ../notebooks/Training_Status_Process.ipynb 15
async def process_row(
    row: pd.Series,
    user: User,
    fast_kafka_api_app: FastKafkaAPI,
):
    """
    Process a single row, update mysql db and send status message to kafka

    Args:
        row: pandas row
        user: user object
    """
    if not row["action"]:
        return

    async_training_stream_status_create = asyncify(TrainingStreamStatus._create)

    account_id = row.name
    application_id = None if np.isnan(row["application_id"]) else row["application_id"]

    upload_event = await async_training_stream_status_create(  # type: ignore
        account_id=account_id,
        application_id=application_id,
        model_id=row["model_id"],
        model_type=row["model_type"],
        event=row["action"],
        count=row["curr_count"],
        total=row["total"],
        user=user,
    )
    await fast_kafka_api_app.to_infobip_training_data_status(
        account_id=account_id,
        application_id=application_id,
        model_id=row["model_id"],
        no_of_records=row["curr_count"],
        total_no_of_records=row["total"],
    )

# %% ../notebooks/Training_Status_Process.ipynb 17
async def process_dataframes(
    recent_events_df: pd.DataFrame,
    ch_df: pd.DataFrame,
    *,
    user: User,
    end_timedelta: int = 30,
    fast_kafka_api_app: FastKafkaAPI,
):
    """
    Process mysql, clickhouse dataframes and take action if needed

    Args:
        recent_events_df: recent events as pandas dataframe from mysql db
        ch_df: count from clickhouse table as dataframe
        user: user object
        end_timedelta: timedelta in seconds to use to determine whether upload is over or not
    """
    df = pd.merge(recent_events_df, ch_df, on="AccountId")
    xs = np.where(  # type: ignore
        df["curr_check_on"].subtract(df["created"])
        > pd.Timedelta(seconds=end_timedelta),
        "end",
        None,
    )
    df["action"] = np.where(
        df["curr_count"] != df["prev_count"],
        "upload",
        xs,
    )

    async with create_task_group() as task_group:
        for account_id, row in df.iterrows():
            task_group.soonify(process_row)(
                row=row, user=user, fast_kafka_api_app=fast_kafka_api_app
            )

# %% ../notebooks/Training_Status_Process.ipynb 20
async def process_training_status(username: str, fast_kafka_api_app: FastKafkaAPI):
    """
    An infinite loop to keep track of training_data uploads from user

    Args:
        username: username of user to track training data uploads
    """
    async_get_user = asyncify(get_user)
    async_get_recent_event_for_user = asyncify(get_recent_event_for_user)
    async_get_count_from_training_data_ch_table = asyncify(
        get_count_from_training_data_ch_table
    )

    while True:
        #         logger.info(f"Starting the process loop")
        try:
            user = await async_get_user(username)
            recent_events_df = await async_get_recent_event_for_user(user=user)
            if not recent_events_df.empty:
                ch_df = await async_get_count_from_training_data_ch_table(
                    account_ids=recent_events_df.index.tolist()
                )
                await process_dataframes(
                    recent_events_df=recent_events_df,
                    ch_df=ch_df,
                    user=user,  # type: ignore
                    fast_kafka_api_app=fast_kafka_api_app,
                )
        except Exception as e:
            logger.info(
                f"Error in process_training_status - {e}, {traceback.format_exc()}"
            )

        await asyncio.sleep(random.randint(5, 20))  # nosec B311
