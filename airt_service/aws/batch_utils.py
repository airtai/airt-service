# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/AWS_Batch_Job_Utils.ipynb.

# %% auto 0
__all__ = ['get_random_string', 'get_instance_info', 'get_availability_zones_and_subnets_in_region',
           'get_subnets_with_instance_availability', 'get_default_security_group_id', 'ComputeEnvironment', 'JobQueue',
           'get_max_vcpus_memory_for_container', 'JobDefinition', 'Job', 'aws_batch_create_job',
           'create_default_batch_environment_config', 'create_batch_environment',
           'create_testing_batch_environment_ctx']

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 3
import random
import shlex
import string
import yaml
from contextlib import contextmanager, ContextDecorator
from os import environ
from pathlib import Path
from time import sleep
from typing import *

import boto3
from fastcore.script import call_parse, Param
from fastcore.utils import patch

from airt_service.sanitizer import sanitized_print
from airt.helpers import ensure
from airt.logger import get_logger
from airt_service.aws.utils import get_available_aws_regions

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 5
logger = get_logger(__name__)

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 6
def get_random_string(length: int = 6) -> str:
    """Generate random string

    Args:
        length: Random string length to generate

    Returns:
        A random string of given length
    """
    return "".join(
        random.choice(string.ascii_uppercase + string.digits)  # nosec B311
        for _ in range(length)
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 8
def get_instance_info(instance_type: str, region: str) -> Tuple[int, int, int]:
    """Get the instance VCPU count, memory, GPU count for the given instance_type

    Args:
        instance_type: Instance type as a string

    Returns:
        The VCPU count, memory, and GPU count as a tuple
    """
    client = boto3.client("ec2", region_name=region)
    response = client.describe_instance_types(
        InstanceTypes=[instance_type],
    )
    if len(response["InstanceTypes"]) == 0:
        raise ValueError(f"{len(response['InstanceTypes'])=}")
    instance_details = response["InstanceTypes"][0]

    vcpus = instance_details["VCpuInfo"]["DefaultVCpus"]
    memory = instance_details["MemoryInfo"]["SizeInMiB"]
    gpu = (
        len(instance_details["GpuInfo"]["Gpus"]) if "GpuInfo" in instance_details else 0
    )
    return vcpus, memory, gpu

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 10
def get_availability_zones_and_subnets_in_region(region: str) -> Dict[str, str]:
    """Get subnet ids and its availability zones available in given region

    Args:
        region: region to get subnets
    Returns:
        A dict with avilability zones as key and subnet ids as value
    """
    client = boto3.client("ec2", region_name=region)
    subnet_response = client.describe_subnets()["Subnets"]

    subnets = {}
    for subnet in subnet_response:
        subnets[subnet["AvailabilityZone"]] = subnet["SubnetId"]

    return subnets

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 12
def get_subnets_with_instance_availability(
    region: str, instance_types: Optional[List[str]] = None
) -> List[str]:
    """Get subnets where given instance type are available for offering

    Args:
        region: region to get subnets
        instance_types: list of instance types
    Returns:
        A list of subnets where given instance types are available
    """
    if instance_types is None:
        instance_types = ["g4dn.xlarge", "r5.16xlarge"]
    zones_and_subnets = get_availability_zones_and_subnets_in_region(region)

    client = boto3.client("ec2", region_name=region)

    available_zones_for_instance = []
    for instance_type in instance_types:
        offerings = client.describe_instance_type_offerings(
            LocationType="availability-zone",
            Filters=[{"Name": "instance-type", "Values": [instance_type]}],
        )
        availability_zones = [
            instance_details["Location"]
            for instance_details in offerings["InstanceTypeOfferings"]
        ]
        available_zones_for_instance.append(set(availability_zones))

    common_availability_zones = set.intersection(*available_zones_for_instance)
    return [
        zones_and_subnets[availability_zone]
        for availability_zone in common_availability_zones
    ]

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 14
def get_default_security_group_id(region: str) -> str:
    """Get default security group id for given region

    Args:
        region: region to get default security group id
    Returngs:
        Default security group id
    """
    client = boto3.client("ec2", region_name=region)
    security_groups = client.describe_security_groups(GroupNames=["default"])
    return security_groups["SecurityGroups"][0]["GroupId"]

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 17
class ComputeEnvironment(ContextDecorator):
    """A class for creating and managing the compute environment"""

    def __init__(self, response: Dict[str, Any], region: str) -> None:
        """Constructs a new ComputeEnvironment instance

        Args:
            response: The compute environment describe response
        """
        self.response = response
        self.region = region

    @property
    def arn(self) -> str:
        """Get ARN of the compute environment

        Returns:
            The ARN of the compute environment
        """
        return self.response["computeEnvironmentArn"]

    @property
    def name(self) -> str:
        """Get name of the compute environment

        Returns:
            The name of the compute environment
        """
        return self.response["computeEnvironmentName"]

    @property
    def instance_type(self) -> str:
        """Get instance type of the compute environment

        Returns:
            The instance type of the compute environment
        """
        return self.response["computeResources"]["instanceTypes"][0]

    @classmethod
    def from_name_or_arn(cls, name: str, region: str) -> "ComputeEnvironment":
        """Construct ComputeEnvironment object from name or from ARN

        Args:
            name: name or ARN of the compute environment

        Returns:
            The ComputeEnvironment object
        """
        client = boto3.client("batch", region_name=region)

        response = client.describe_compute_environments(
            computeEnvironments=[
                name,
            ],
        )

        if len(response["computeEnvironments"]) > 1:
            raise ValueError(f"{len(response['computeEnvironments'])=}")
        elif len(response["computeEnvironments"]) == 0:
            raise ValueError(f"{len(response['computeEnvironments'])=}")

        return ComputeEnvironment(response["computeEnvironments"][0], region)

    @classmethod
    def create(
        cls,
        *,
        name: Optional[str] = None,
        region: str,
        resource_type: str = "EC2",  # EC2 or SPOT
        allocation_strategey: str = "BEST_FIT",
        min_instances: int = 0,
        max_instances: int = 3,
        instance_type: str,
        subnets: Optional[List[str]] = None,
        security_group_ids: Optional[List[str]] = None,
        ec2_key_pair: Optional[str] = None,
        instance_role: str = "arn:aws:iam::617504802562:instance-profile/ecsInstanceRole",
        launch_template_name: str = "gitlab-registry-access-template",
    ) -> "ComputeEnvironment":
        """Create a new compute environment

        Args:
            name: Name of the compute environment
            resource_type: The type of the resource. Use 'EC2' for on-demand instances or 'SPOT' for spot instances
            allocation_strategey: Allocation strategy of the resource. If not set, the default value **BEST_FIT** will be used
                Other Valid options are ('BEST_FIT'|'BEST_FIT_PROGRESSIVE'|'SPOT_CAPACITY_OPTIMIZED')
            min_instances: Minimum instances to keep running in compute environment's ecs cluster
            max_instances: Maximum instances to scale up in compute environment's ecs cluster
            instance_type: Instance type to use to execute jobs
            subnets: subnets to use; default values are "subnet-19d65943" for eu-west-1a and "subnet-f296d494" for eu-west-1b
            security_group_ids: Security groups to use; default value is "sg-5d3ee12b" which is default security group of eu-west-1
            ec2_key_pair: EC2 key pair to use in spinned instances
            instance_role: ECS instance profile applied to Amazon EC2 instances in a compute environment
            launch_template_name: Launch template to use to spin up instances
        Returns:
            The newly created ComputeEnvironment object
        """
        client = boto3.client("batch", region_name=region)

        if name is None:
            name = f"compute-environment-{get_random_string()}"

        response = client.describe_compute_environments(
            computeEnvironments=[
                name,
            ],
        )

        if len(response["computeEnvironments"]) > 1:
            raise ValueError(f"{len(response['computeEnvironments'])=}")
        elif len(response["computeEnvironments"]) == 1:
            return ComputeEnvironment(
                response=response["computeEnvironments"][0], region=region
            )

        # ToDo: Remove default values for following and have a separate config dict for g4dn instance, r instance, etc in eu-west-1
        if subnets is None:
            #             subnets = ["subnet-19d65943", "subnet-f296d494"]
            subnets = get_subnets_with_instance_availability(region)
        if security_group_ids is None:
            #             security_group_ids = ["sg-5d3ee12b"]
            security_group_ids = [get_default_security_group_id(region)]

        vcpus, memory, gpu = get_instance_info(
            instance_type=instance_type, region=region
        )
        min_vcpus = min_instances * vcpus
        max_vcpus = max_instances * vcpus

        compute_resources = {
            "type": resource_type,
            "allocationStrategy": allocation_strategey,
            "minvCpus": min_vcpus,
            "maxvCpus": max_vcpus,
            "instanceTypes": [instance_type],
            "subnets": subnets,
            "securityGroupIds": security_group_ids,
            "instanceRole": instance_role,
            "launchTemplate": {
                "launchTemplateName": launch_template_name,
                "version": "$Default",
            },
        }
        if ec2_key_pair is not None:
            compute_resources["ec2KeyPair"] = ec2_key_pair

        response = client.create_compute_environment(
            computeEnvironmentName=name,
            type="MANAGED",
            state="ENABLED",
            computeResources=compute_resources,
        )
        compute_env = ComputeEnvironment(response=response, region=region)
        response = compute_env.wait(status="VALID", state="ENABLED")
        return ComputeEnvironment(response=response, region=region)

    def wait(
        self,
        status: str,
        state: str,
        timeout: int = 0,
        sleep_step: int = 1,
    ) -> Dict[str, Any]:
        """Wait until the compute environment reaches the given status and state

        Args:
            status: Status to wait for ('CREATING'|'UPDATING'|'DELETING'|'DELETED'|'VALID'|'INVALID')
            state: State to wait for ('ENABLED'|'DISABLED')
            timeout: The maximum time allowed in seconds for the command to complete. If greater than 0,
                then the command will be killed after the timeout
            sleep_step: The time interval in seconds to check the completion status of the command

        Returns:
            The response of describe compute environment
        """
        client = boto3.client("batch", region_name=self.region)

        i = 0
        while True:
            if 0 < timeout <= i:
                logger.info(f"wait timedout after {i:,d} seconds for arn: '{self.arn}'")
                break
            response = client.describe_compute_environments(
                computeEnvironments=[
                    self.arn,
                ],
            )
            logger.info(
                f'wait(): {self.arn=}, status={response["computeEnvironments"][0]["status"]}, state={response["computeEnvironments"][0]["state"]}'
            )
            if (
                response["computeEnvironments"][0]["status"] == status
                and response["computeEnvironments"][0]["state"] == state
            ):
                break
            sleep(sleep_step)
            i = i + sleep_step
        return response["computeEnvironments"][0]

    def update(self, *args: Any, **kwargs: Any) -> None:
        """Update compute environment"""
        client = boto3.client("batch", region_name=self.region)
        response = client.update_compute_environment(
            computeEnvironment=self.arn, *args, **kwargs
        )

    def delete(self) -> None:
        """Delete compute environment"""
        client = boto3.client("batch", region_name=self.region)
        response = client.delete_compute_environment(
            computeEnvironment=self.arn,
        )

    def __enter__(self) -> "ComputeEnvironment":
        return self

    def __exit__(self, *exc: Any) -> None:
        client = boto3.client("batch", region_name=self.region)
        self.update(state="DISABLED")
        self.wait(status="VALID", state="DISABLED")
        self.delete()

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 21
class JobQueue(ContextDecorator):
    """A class for creating and managing the job queues"""

    def __init__(self, response: Dict[str, Any], region: str) -> None:
        """Constructs a new Job Queue instance

        Args:
            response: The job queue describe response
        """
        self.response = response
        self.region = region

    @property
    def arn(self) -> str:
        """Get arn of the job queue

        Returns:
            The arn of job queue
        """
        return self.response["jobQueueArn"]

    @property
    def name(self) -> str:
        """Get name of the job queue

        Returns:
            The name of job queue
        """
        return self.response["jobQueueName"]

    @classmethod
    def from_name_or_arn(cls, name: str, region: str) -> "JobQueue":
        """Construct JobQueue object from name or ARN

        Args:
            name: The name or arn of job queue
            region: Region of job queue

        Returns:
            The JobQueue object
        """
        client = boto3.client("batch", region_name=region)

        response = client.describe_job_queues(
            jobQueues=[name],
        )

        if len(response["jobQueues"]) > 1:
            raise ValueError(f"{len(response['jobQueues'])=}")
        elif len(response["jobQueues"]) == 0:
            raise ValueError(f"{len(response['jobQueues'])=}")

        return JobQueue(response["jobQueues"][0], region)

    @classmethod
    def create(
        cls,
        *,
        name: Optional[str] = None,
        compute_environment: ComputeEnvironment,
        priority: int = 100,
    ) -> "JobQueue":
        """Create a new job queue

        Args:
            name: Name of the job queue
            compute_environment: The ComputeEnvironment object
            priority: Priority to assign to the job queue

        Returns:
            The newly constructed JobQueue object
        """
        client = boto3.client("batch", region_name=compute_environment.region)

        if name is None:
            name = f"job-queue-{get_random_string()}"
        response = client.describe_job_queues(
            jobQueues=[
                name,
            ],
        )

        if len(response["jobQueues"]) > 1:
            raise ValueError(f"{len(response['jobQueues'])=}")
        elif len(response["jobQueues"]) == 1:
            return JobQueue(response["jobQueues"][0], region=compute_environment.region)

        response = client.create_job_queue(
            jobQueueName=name,
            state="ENABLED",
            priority=priority,
            computeEnvironmentOrder=[
                {"order": 1, "computeEnvironment": compute_environment.arn},
            ],
        )

        job_queue = JobQueue(response=response, region=compute_environment.region)
        response = job_queue.wait(status="VALID", state="ENABLED")
        return JobQueue(response=response, region=compute_environment.region)

    def wait(
        self,
        status: Optional[str] = None,
        state: Optional[str] = None,
        is_deleted: bool = False,
        timeout: int = 0,
        sleep_step: int = 1,
    ) -> Union[None, Dict[str, Any]]:
        """Wait until the job queue reaches the given status and state or until job queue is deleted

        Args:
            status: Status to wait for('CREATING'|'UPDATING'|'DELETING'|'DELETED'|'VALID'|'INVALID')
            state: State to wait for('ENABLED'|'DISABLED')
            is_deleted: A flag indicating whether to wait for job queue deletion. If not set, then
                the default value **False** will be used.
            timeout: The maximum time allowed in seconds for the command to complete. If greater than 0,
                then the command will be killed after the timeout
            sleep_step: The time interval in seconds to check the completion status of the command

        Returns:
            The response of describe job queue
        """
        ensure(is_deleted != ((status is not None) or (state is not None)))
        ensure((status is None) == (state is None))
        client = boto3.client("batch", region_name=self.region)

        i = 0
        while True:
            if 0 < timeout <= i:
                logger.info(f"wait timedout after {i:,d} seconds for arn: '{self.arn}'")
                break
            response = client.describe_job_queues(
                jobQueues=[
                    self.arn,
                ],
            )
            if is_deleted and not response["jobQueues"]:
                logger.info(f"wait(): {self.arn=} deleted")
                return None
            logger.info(
                f'wait(): {self.arn=}, status={response["jobQueues"][0]["status"]}, state={response["jobQueues"][0]["state"]}'
            )
            if (
                response["jobQueues"][0]["status"] == status
                and response["jobQueues"][0]["state"] == state
            ):
                break
            sleep(sleep_step)
            i = i + sleep_step
        return response["jobQueues"][0]

    def update(self, *args: Any, **kwargs: Any) -> None:
        """Update job queue"""
        client = boto3.client("batch", region_name=self.region)
        response = client.update_job_queue(jobQueue=self.arn, *args, **kwargs)

    def delete(self) -> None:
        """Delete job queue"""
        client = boto3.client("batch", region_name=self.region)
        response = client.delete_job_queue(
            jobQueue=self.arn,
        )

    def __enter__(self) -> "JobQueue":
        return self

    def __exit__(self, *exc: Any) -> None:
        client = boto3.client("batch", region_name=self.region)
        self.update(state="DISABLED")
        self.wait(status="VALID", state="DISABLED")
        self.delete()
        self.wait(is_deleted=True)

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 22
@patch
def create_job_queue(
    self: ComputeEnvironment, *, name: Optional[str] = None, priority: int = 100
) -> JobQueue:
    return JobQueue.create(name=name, compute_environment=self, priority=priority)

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 25
def get_max_vcpus_memory_for_container(
    host_vcpus: int, host_memory: int
) -> Tuple[int, int]:
    """Get maximum VCPUs and memory that can be allocated to the container from available host VCPUs and memory

    Args:
        host_vcpus: available VCPUs on the host
        host_memory: available memory on the host(in MiB)

    Returns:
        The max VCPUs and max memory that can be allocated to the container as a tuple
    """

    if host_vcpus <= 4:
        max_container_vcpus = max(1, host_vcpus - 1)
    else:
        max_container_vcpus = host_vcpus - 2
    max_container_memory = host_memory - (host_vcpus * 256)
    return max_container_vcpus, max_container_memory

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 27
class JobDefinition(ContextDecorator):
    """A class for creating and managing the job definition"""

    def __init__(self, response: Dict[str, Any], region: str) -> None:
        """Constructs a new JobDefinition instance

        Args:
            response: job definition describe response
        """
        self.response = response
        self.region = region

    @property
    def arn(self) -> str:
        """Get ARN of the job definition

        Returns:
            The ARN of job definition
        """
        return self.response["jobDefinitionArn"]

    @property
    def name(self) -> str:
        """Get name of the job definition

        Returns:
            The name of job definition
        """
        return self.response["jobDefinitionName"]

    @classmethod
    def from_name_or_arn(cls, name: str, region: str) -> "JobDefinition":
        """Construct the JobDefinition object from name or from ARN

        Args:
            name: Name or ARN of job defintion
            region: Region of job definition

        Returns:
            The JobDefinition object
        """
        client = boto3.client("batch", region_name=region)

        response = client.describe_job_definitions(
            jobDefinitions=[name],
        )

        if len(response["jobDefinitions"]) > 1:
            raise ValueError(f"{len(response['jobDefinitions'])=}")
        elif len(response["jobDefinitions"]) == 0:
            raise ValueError(f"{len(response['jobDefinitions'])=}")

        return JobDefinition(response["jobDefinitions"][0], region)

    @classmethod
    def create(
        cls,
        *,
        name: Optional[str] = None,
        image: str,
        job_role_arn: str = "arn:aws:iam::617504802562:role/ecsTaskExecutionRole",
        execution_role_arn: str = "arn:aws:iam::617504802562:role/ecsTaskExecutionRole",
        compute_environment: ComputeEnvironment,
        command: Optional[str] = None,
        environment_vars: Optional[Dict[str, str]] = None,
        retries: int = 3,
    ) -> "JobDefinition":
        """
        Create job definition

        Args:
            name: Name of the job definition
            image: Image to start container
            job_role_arn: ARN of the IAM role that the container can assume for AWS permissions
            execution_role_arn: ARN of the execution role that batch job can assume
            compute_environment: ComputeEnvironment object
            command: Command to execute after starting container
            environment_vars: Environment vars to set in the container
            retries: Times to retry if the job fails (includes first execution)

        Returns:
            The JobDefinition object
        """
        client = boto3.client("batch", region_name=compute_environment.region)

        if name is None:
            name = f"job-definition-{get_random_string()}"
        response = client.describe_job_definitions(jobDefinitionName=name)

        for jd in reversed(response["jobDefinitions"]):
            if jd["status"] == "ACTIVE":
                return JobDefinition(jd, region=compute_environment.region)

        vcpus, memory, gpu = get_instance_info(
            instance_type=compute_environment.instance_type,
            region=compute_environment.region,
        )
        container_vcpus, container_memory = get_max_vcpus_memory_for_container(
            host_vcpus=vcpus, host_memory=memory
        )
        container_properties = {
            "image": image,
            "jobRoleArn": job_role_arn,
            "executionRoleArn": execution_role_arn,
            "resourceRequirements": [
                dict(value=str(container_vcpus), type="VCPU"),
                dict(value=str(container_memory), type="MEMORY"),
            ],
            "logConfiguration": {
                "logDriver": "awslogs",
            },
        }
        if gpu > 0:
            container_properties["resourceRequirements"].append(  # type: ignore
                dict(value=str(gpu), type="GPU")
            )
        if command is not None:
            container_properties["command"] = shlex.split(command)
        if environment_vars is not None:
            container_properties["environment"] = [
                dict(name=name, value=value) for name, value in environment_vars.items()
            ]

        response = client.register_job_definition(
            jobDefinitionName=name,
            type="container",
            containerProperties=container_properties,
            retryStrategy={
                "attempts": retries,
            },
            platformCapabilities=[
                "EC2",
            ],
        )

        job_definition = JobDefinition(
            response=response, region=compute_environment.region
        )
        response = job_definition.wait(status="ACTIVE")
        return JobDefinition(response=response, region=compute_environment.region)

    def wait(
        self,
        status: str,
        timeout: int = 0,
        sleep_step: int = 1,
    ) -> Dict[str, Any]:
        """
        Wait until job definition reaches the given status

        Args:
            status: Status to wait for('ACTIVE'|'INACTIVE')
            timeout: The maximum time allowed in seconds for the command to complete. If greater than 0,
                then the command will be killed after the timeout
            sleep_step: The time interval in seconds to check the completion status of the command

        Returns:
            The response of describe job definition
        """
        client = boto3.client("batch", region_name=self.region)

        i = 0
        while True:
            if 0 < timeout <= i:
                logger.info(f"wait timedout after {i:,d} seconds for arn: '{self.arn}'")
                break
            response = client.describe_job_definitions(
                jobDefinitions=[self.arn],
            )
            logger.info(
                f'wait(): {self.arn=}, status={response["jobDefinitions"][0]["status"]}'
            )
            if response["jobDefinitions"][0]["status"] == status:
                break
            sleep(sleep_step)
            i = i + sleep_step
        return response["jobDefinitions"][0]

    def delete(self) -> None:
        """Delete job definition"""
        client = boto3.client("batch", region_name=self.region)
        response = client.deregister_job_definition(
            jobDefinition=self.arn,
        )

    def __enter__(self) -> "JobDefinition":
        return self

    def __exit__(self, *exc: Any) -> None:
        client = boto3.client("batch", region_name=self.region)
        self.delete()
        self.wait(status="INACTIVE")

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 28
@patch
def create_job_definition(
    self: ComputeEnvironment,
    *,
    name: Optional[str] = None,
    image: str,
    job_role_arn: str = "arn:aws:iam::617504802562:role/ecsTaskExecutionRole",
    execution_role_arn: str = "arn:aws:iam::617504802562:role/ecsTaskExecutionRole",
    command: Optional[str] = None,
    environment_vars: Optional[Dict[str, str]] = None,
    retries: int = 3,
) -> JobDefinition:
    return JobDefinition.create(
        name=name,
        compute_environment=self,
        image=image,
        job_role_arn=job_role_arn,
        execution_role_arn=execution_role_arn,
        command=command,
        environment_vars=environment_vars,
        retries=retries,
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 31
class Job(ContextDecorator):
    """A class for creating and managing the jobs"""

    def __init__(self, response: Dict[str, Any], region: str) -> None:
        """Constructs a new Job instance

        Args:
            response: job describe response
            region: region of job
        """
        self.response = response
        self.region = region

    @property
    def arn(self) -> str:
        """Get ARN of the job

        Returns:
            ARN of the job
        """
        return self.response["jobArn"]

    @property
    def name(self) -> str:
        """Get name of the job

        Returns:
            name of the job
        """
        return self.response["jobName"]

    @property
    def job_id(self) -> str:
        """Get job id of the job

        Returns:
            job id of the job
        """
        return self.response["jobId"]

    @classmethod
    def from_job_id(cls, job_id: str, region: str) -> "Job":
        """Construct Job object from job id

        Args:
            job_id: id of the job
            region: region of the job

        Returns:
            The Job object
        """
        client = boto3.client("batch", region_name=region)

        response = client.describe_jobs(
            jobs=[job_id],
        )

        if len(response["jobs"]) > 1:
            raise ValueError(f"{len(response['jobs'])=}")
        elif len(response["jobs"]) == 0:
            raise ValueError(f"{len(response['jobs'])=}")

        return Job(response["jobs"][0], region)

    @classmethod
    def create(
        cls,
        *,
        name: Optional[str] = None,
        job_queue: JobQueue,
        job_definition: JobDefinition,
        vcpus: Optional[int] = None,
        memory: Optional[int] = None,
        gpu: Optional[int] = None,
        command: Optional[str] = None,
        environment_vars: Optional[Dict[str, str]] = None,
        retries: Optional[int] = None,
    ) -> "Job":
        """
        Create job

        Args:
            name: Name of the job
            job_queue: JobQueue object
            job_definition: JobDefinition object
            vcpus: Overwrite VCPUs value in job definition
            memory: Overwrite memory value in job definition
            gpu: Overwrite GPU value in job definition
            command: Command to execute after starting container
            environment_vars: Environment vars to set in the container
            retries: Times to retry if job fails (includes first execution)

        Returns:
            The Job object
        """
        client = boto3.client("batch", region_name=job_definition.region)

        if name is None:
            name = f"job-{get_random_string()}"

        container_overrides: Dict[str, Any] = {"resourceRequirements": []}

        if vcpus is not None:
            container_overrides["resourceRequirements"].append(
                dict(value=str(vcpus), type="VCPU")
            )
        if memory is not None:
            container_overrides["resourceRequirements"].append(
                dict(value=str(memory), type="MEMORY")
            )
        if gpu is not None:
            container_overrides["resourceRequirements"].append(
                dict(value=str(gpu), type="GPU")
            )
        if command is not None:
            container_overrides["command"] = shlex.split(command)
        if environment_vars is not None:
            container_overrides["environment"] = [
                dict(name=name, value=value) for name, value in environment_vars.items()
            ]

        params = dict(
            jobName=name,
            jobQueue=job_queue.arn,
            jobDefinition=job_definition.arn,
            containerOverrides=container_overrides,
        )

        if retries is not None:
            params["retryStrategy"] = dict(attempts=retries)

        response = client.submit_job(**params)

        return Job(response, job_definition.region)

    def wait(
        self,
        status: str,
        timeout: int = 0,
        sleep_step: int = 1,
    ) -> Dict[str, Any]:
        """
        Wait until job reaches the given status or until it fails

        Args:
            status: Status to wait for('SUBMITTED'|'PENDING'|'RUNNABLE'|'STARTING'|'RUNNING'|'SUCCEEDED'|'FAILED')
            timeout: The maximum time allowed in seconds for the command to complete. If greater than 0,
                then the command will be killed after the timeout
            sleep_step: The time interval in seconds to check the completion status of the command

        Returns:
            response of describe job
        """
        client = boto3.client("batch", region_name=self.region)

        i = 0
        while True:
            if 0 < timeout <= i:
                logger.info(f"wait timedout after {i:,d} seconds for arn: '{self.arn}'")
                break
            response = client.describe_jobs(
                jobs=[self.job_id],
            )
            logger.info(
                f'wait(): {self.job_id=}, status={response["jobs"][0]["status"]}'
            )
            if response["jobs"][0]["status"] == status:
                break
            elif response["jobs"][0]["status"] == "FAILED":
                raise ValueError(f'{response["jobs"][0]["status"]=}')
            sleep(sleep_step)
            i = i + sleep_step
        return response["jobs"][0]

    def delete(self) -> None:
        """Delete job"""
        client = boto3.client("batch", region_name=self.region)
        response = client.terminate_job(
            jobId=self.job_id,
        )

    def __enter__(self) -> "Job":
        return self

    def __exit__(self, *exc: Any) -> None:
        client = boto3.client("batch", region_name=self.region)
        self.wait(status="SUCCEEDED")

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 32
@patch
def create_job(
    self: JobQueue,
    *,
    name: Optional[str] = None,
    job_definition: JobDefinition,
    vcpus: Optional[int] = None,
    memory: Optional[int] = None,
    gpu: Optional[int] = None,
    command: Optional[str] = None,
    environment_vars: Optional[Dict[str, str]] = None,
    retries: Optional[int] = None,
) -> Job:
    return Job.create(
        name=name,
        job_queue=self,
        job_definition=job_definition,
        vcpus=vcpus,
        memory=memory,
        gpu=gpu,
        command=command,
        environment_vars=environment_vars,
        retries=retries,
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 33
@patch  # type: ignore
def create_job(
    self: JobDefinition,
    *,
    name: Optional[str] = None,
    job_queue: JobQueue,
    vcpus: Optional[int] = None,
    memory: Optional[int] = None,
    gpu: Optional[int] = None,
    command: Optional[str] = None,
    environment_vars: Optional[Dict[str, str]] = None,
    retries: Optional[int] = None,
) -> Job:
    return Job.create(
        name=name,
        job_queue=job_queue,
        job_definition=self,
        vcpus=vcpus,
        memory=memory,
        gpu=gpu,
        command=command,
        environment_vars=environment_vars,
        retries=retries,
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 38
def aws_batch_create_job(  # type: ignore
    *,
    name: Optional[str] = None,
    job_queue_arn: str,
    job_definition_arn: str,
    region: str,
    vcpus: Optional[int] = None,
    memory: Optional[int] = None,
    gpu: Optional[int] = None,
    command: Optional[str] = None,
    environment_vars: Optional[Dict[str, str]] = None,
    retries: Optional[int] = None,
) -> Job:
    """Create a new CLI job

    Args:
        name: Name of the job
        job_queue_arn: ARN of job queue
        job_definition_arn: ARN of job definition
        region: Region to create job
        vcpus: Overwrite VCPUs value in job definition
        memory: Overwrite memory value in job definition
        gpu: Overwrite GPU value in job definition
        command: Command to execute after starting container
        environment_vars: Environment vars to set in the container
        retries: Times to retry if job fails(includes first execution)

    Returns:
        A new CLI job
    """
    job_queue = JobQueue.from_name_or_arn(job_queue_arn, region)
    job_definition = JobDefinition.from_name_or_arn(job_definition_arn, region)

    job = Job.create(
        name=name,
        job_queue=job_queue,
        job_definition=job_definition,
        vcpus=vcpus,
        memory=memory,
        gpu=gpu,
        command=command,
        environment_vars=environment_vars,
        retries=retries,
    )
    logger.info(f"{job.arn=}")
    return job

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 40
def _create_default_batch_environment_config(
    prefix: str, output_path: Union[str, Path], regions: Optional[List[str]] = None
) -> None:
    """Generate batch environment YAML config to set up the batch job environment

    Args:
        prefix: Prefix to use in names
        output_path: Path of yaml file to store generated config
    """

    def _f(task_name: str, image: str, instance_type: str, prefix: str) -> str:
        return f"""
    {task_name}:
      compute_environment:
          name: {prefix}_{task_name}_compute_environment
          instance_type: {instance_type}
          min_instances: 0
          max_instances: 10
      job_queue:
          name: {prefix}_{task_name}_job_queue
          priority: 100
      job_definition:
          name: {prefix}_{task_name}_job_definition
          image: {image}
        """

    instance_types = dict(
        preprocessing="r5.16xlarge",
        training="g4dn.xlarge",
        predictions="g4dn.xlarge",
        csv_processing="r5.16xlarge",
    )

    image = "ghcr.io/airtai/airt-service:dev"

    yaml_str = {}

    if not regions:
        regions = get_available_aws_regions()

    for region in regions:
        yaml_str[region] = yaml.safe_load(
            "\n".join(
                [
                    _f(task_name, image, instance_type, prefix)
                    for task_name, instance_type in instance_types.items()
                ]
            )
        )

    with open(output_path, "w") as f:
        yaml.dump(yaml_str, f, default_flow_style=False)

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 41
@call_parse
def create_default_batch_environment_config(
    prefix: Param("prefix", str), output_path: Param("output_path", str), regions: Param("regions", List[str]) = None  # type: ignore
) -> None:
    """Generate batch environment YAML config to set up the batch job environment

    Args:
        prefix: Prefix to use in names
        output_path: Path of yaml file to store generated config
    """

    _create_default_batch_environment_config(
        prefix=prefix, output_path=output_path, regions=regions
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 43
def _create_batch_environment(input_yaml_path: str, output_yaml_path: str) -> None:
    """Create a batch environment based on the config specified and store the created environment ARN in the output YAML file

    Args:
        input_yaml_path: YAML config file path for creating the batch environment
        output_yaml_path: YAML file path to store the created environment ARN
    """
    with open(input_yaml_path) as f:
        d = yaml.safe_load(f)

    output: Dict[str, Dict[str, Dict[str, str]]] = dict()

    for region, config in d.items():
        output[region] = {}
        for task_name, value in config.items():
            sanitized_print(f"{task_name=}")

            compute_env = ComputeEnvironment.create(
                region=region, **value["compute_environment"]
            )
            sanitized_print(f"{compute_env.arn=}")

            job_queue = compute_env.create_job_queue(**value["job_queue"])  # type: ignore
            sanitized_print(f"{job_queue.arn=}")

            job_definition = compute_env.create_job_definition(**value["job_definition"])  # type: ignore
            sanitized_print(f"{job_definition.arn=}")

            output[region][task_name] = dict(
                compute_environment_arn=compute_env.arn,
                job_queue_arn=job_queue.arn,
                job_definition_arn=job_definition.arn,
            )
    with open(output_yaml_path, "w") as f:
        yaml.dump(output, f, default_flow_style=False)

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 44
@call_parse
def create_batch_environment(
    input_yaml_path: Param("yaml_path", str), output_yaml_path: Param("yaml_path", str)  # type: ignore
) -> None:
    """Create a batch environment based on the config specified and store the created environment ARN in the output YAML file

    Args:
        input_yaml_path: YAML config file path for creating the batch environment
        output_yaml_path: YAML file path to store the created environment ARN
    """
    _create_batch_environment(
        input_yaml_path=input_yaml_path, output_yaml_path=output_yaml_path
    )

# %% ../../notebooks/AWS_Batch_Job_Utils.ipynb 45
@contextmanager
def create_testing_batch_environment_ctx(
    input_yaml_path: str, output_yaml_path: str
) -> Iterator[None]:
    """Create batch environment and tear it down after yield for testing

    Args:
        input_yaml_path: path of yaml file which has config to create batch environment
        output_yaml_path: path of yaml file to store created environment arn
    """
    _create_batch_environment(
        input_yaml_path=input_yaml_path, output_yaml_path=output_yaml_path
    )
    try:
        yield
    finally:
        with open(output_yaml_path) as f:
            d = yaml.safe_load(f)

        for region, config in d.items():
            for task_name, value in config.items():
                sanitized_print(f"deleting job definition - {task_name}")
                job_definition = JobDefinition.from_name_or_arn(
                    value["job_definition_arn"], region
                )
                job_definition.delete()
                sanitized_print(f"deleting job queue - {task_name}")
                job_queue = JobQueue.from_name_or_arn(value["job_queue_arn"], region)
                job_queue.update(state="DISABLED")
                job_queue.wait(status="VALID", state="DISABLED")
                job_queue.delete()
                job_queue.wait(is_deleted=True)
                sanitized_print(f"deleting compute env - {task_name}")
                compute_env = ComputeEnvironment.from_name_or_arn(
                    value["compute_environment_arn"], region
                )
                compute_env.update(state="DISABLED")
                compute_env.wait(status="VALID", state="DISABLED")
                compute_env.delete()
