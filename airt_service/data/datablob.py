# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/DataBlob_Router.ipynb.

# %% auto 0
__all__ = ['datablob_router', 'S3Request', 'CloudProvider', 'FromS3Request', 'from_s3_route', 'AzureBlobStorageRequest',
           'FromAzureBlobStorageRequest', 'from_azure_blob_storage_route', 'DBRequest', 'FromDBRequest',
           'from_mysql_route', 'ClickHouseRequest', 'FromClickHouseRequest', 'from_clickhouse_route',
           'FromLocalRequest', 'FromLocalResponse', 'from_local_start_route', 'FileType', 'ToDataSourceRequest',
           'to_datasource_route', 'get_details_of_datablob', 'delete_datablob', 'get_all_datablobs', 'tag_datablob']

# %% ../../notebooks/DataBlob_Router.ipynb 3
import json
import shlex
import uuid as uuid_pkg
from enum import Enum
from time import sleep
from typing import *

import airt_service
import airt_service.sanitizer
import boto3
import numpy as np
from airt.executor.subcommand import SimpleCLICommand
from airt.helpers import get_s3_bucket_name_and_folder_from_uri
from airt.logger import get_logger
from airt.patching import patch
from ..airflow.executor import AirflowExecutor
from ..auth import get_current_active_user
from airt_service.aws.utils import (
    create_s3_datablob_path,
    get_s3_bucket_and_path_from_uri,
    verify_aws_region,
)
from ..azure.utils import verify_azure_region
from ..batch_job import create_batch_job
from .clickhouse import create_db_uri_for_clickhouse_datablob
from .datasource import DataSource
from airt_service.data.utils import (
    create_db_uri_for_azure_blob_storage_datablob,
    create_db_uri_for_db_datablob,
    create_db_uri_for_local_datablob,
    create_db_uri_for_s3_datablob,
    delete_data_object_files_in_cloud,
)
from airt_service.db.models import (
    DataBlob,
    DataBlobRead,
    DataSourceRead,
    Tag,
    TagCreate,
    User,
    get_session,
)
from ..errors import ERRORS, HTTPError
from ..helpers import commit_or_rollback
from botocore.client import Config
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, Query, status
from pydantic import BaseModel
from sqlalchemy.exc import NoResultFound
from sqlalchemy.orm.exc import StaleDataError
from sqlmodel import Session, select

# %% ../../notebooks/DataBlob_Router.ipynb 6
logger = get_logger(__name__)

# %% ../../notebooks/DataBlob_Router.ipynb 8
# Default router for all data sources
datablob_router = APIRouter(
    prefix="/datablob",
    tags=["datablob"],
    #     dependencies=[Depends(get_current_active_user)],
    responses={
        404: {"description": "Not found"},
        500: {
            "model": HTTPError,
            "description": ERRORS["INTERNAL_SERVER_ERROR"],
        },
    },
)

# %% ../../notebooks/DataBlob_Router.ipynb 9
@patch
def remove_tag_from_previous_datablobs(self: DataBlob, tag_name: str, session: Session):
    """Remove tag_name associated with other/previous datablobs

    Args:
        tag_name: Tag name to remove from other datablobs
        session: Sqlmodel session
    """
    tag_to_remove = Tag.get_by_name(name=tag_name, session=session)  # type: ignore
    try:
        datablobs = session.exec(
            select(DataBlob).where(
                DataBlob.type == self.type,
                DataBlob.uri == self.uri,
                DataBlob.user == self.user,
            )
        ).all()
    except NoResultFound:
        return

    for datablob in datablobs:
        if tag_to_remove in datablob.tags:
            try:
                datablob.tags.remove(tag_to_remove)
                session.add(datablob)
                session.commit()
            except StaleDataError:
                session.rollback()

# %% ../../notebooks/DataBlob_Router.ipynb 11
create_datablob_responses = {
    400: {"model": HTTPError, "description": "DataBlob error"},
}


@patch(cls_method=True)
def _create(
    cls: DataBlob,
    *,
    type: str,
    uri: str,
    source: str,
    cloud_provider: str,
    region: str,
    total_steps: int,
    user_tag: Optional[str] = None,
    user: User,
    session: Session,
) -> DataBlob:
    """Function to create new datablob based on given params

    Args:
        type: Datablob type
        uri: DB uri of datablob
        source: Datablob uri
        cloud_provider: Cloud provider to store datablob files
        region: Region of cloud provider
        total_steps: Total steps
        user_tag: Tag created by user to add to new datablob
        user: User object
        session: Sqlmodel session

    Returns:
        The created datablob object
    Raises:
        HTTPException if request has bad parameters
    """
    try:
        datablob = DataBlob(
            type=type,
            uri=uri,
            source=source,
            cloud_provider=cloud_provider,
            region=region,
            total_steps=total_steps,
            user=user,
        )

        for tag_name in [user_tag, "latest"] if user_tag is not None else ["latest"]:
            datablob.remove_tag_from_previous_datablobs(  # type: ignore
                tag_name=tag_name, session=session
            )
            datablob.tags.append(Tag.get_by_name(name=tag_name, session=session))  # type: ignore

        session.add(datablob)
        session.commit()
        return datablob
    except Exception as e:
        logger.exception(e)
        error_message = (
            e._message() if callable(getattr(e, "_message", None)) else str(e)  # type: ignore
        )
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=error_message,
        )

# %% ../../notebooks/DataBlob_Router.ipynb 13
class S3Request(BaseModel):
    """Base Request object for from_s3 and to_s3 routes"""

    uri: str
    access_key: str
    secret_key: str

# %% ../../notebooks/DataBlob_Router.ipynb 14
class CloudProvider(str, Enum):
    aws = "aws"
    azure = "azure"

# %% ../../notebooks/DataBlob_Router.ipynb 15
class FromS3Request(S3Request):
    """Request object for the /data/s3 route

    Args:
        uri: S3 uri of the folder where parquet files are stored
        access_key: Access key for the s3 bucket
        secret_key: Secret key for the s3 bucket
        cloud_provider: Cloud provider to save files
        region: Region of the cloud provider
        tag: Tag to add to the datablob
    """

    cloud_provider: CloudProvider = "aws"  # type: ignore
    region: Optional[str] = None
    tag: Optional[str] = None

    class Config:
        use_enum_values = True

# %% ../../notebooks/DataBlob_Router.ipynb 16
@patch(cls_method=True)
def from_s3(
    cls: DataBlob,
    *,
    from_s3_request: FromS3Request,
    user: User,
    session: Session,
    background_tasks: BackgroundTasks,
    no_retries: int = 3,
) -> DataBlob:
    """Create a datablob from an S3 bucket

    Args:
        from_s3_request: The from_s3 request
        user: User object
        session: Sqlmodel session
        background_tasks: BackgroundTasks object
        no_retries: Number of times to retry before raising an exception

    Returns:
        A new datablob created from a S3
    """

    uri = create_db_uri_for_s3_datablob(
        uri=from_s3_request.uri,
        access_key=from_s3_request.access_key,
        secret_key=from_s3_request.secret_key,
    )

    cloud_provider = from_s3_request.cloud_provider
    region = from_s3_request.region
    if region is None:
        s3_client = boto3.client(
            "s3",
            aws_access_key_id=from_s3_request.access_key,
            aws_secret_access_key=from_s3_request.secret_key,
            config=Config(signature_version="s3v4"),
        )
        bucket_name, folder = get_s3_bucket_name_and_folder_from_uri(
            from_s3_request.uri
        )
        region = s3_client.get_bucket_location(Bucket=bucket_name)["LocationConstraint"]

    verify_aws_region(region) if cloud_provider == "aws" else verify_azure_region(
        region
    )
    source = from_s3_request.uri

    for i in range(no_retries):
        e: Optional[Exception] = None
        try:
            datablob = DataBlob._create(  # type: ignore
                type="s3",
                uri=uri,
                source=source,
                cloud_provider=cloud_provider,
                region=region,
                total_steps=1,
                user_tag=from_s3_request.tag,
                user=user,
                session=session,
            )
            break
        except Exception as _e:
            e = _e
            sleep(np.random.uniform(1, 5))

    if e:
        logger.exception(f"DataBlob.from_s3() failed", exc_info=e)
        raise HTTPException(status_code=500, detail=f"Unexpected exception: {e}")

    #     command = f"s3_pull {datablob.id}"

    #     create_batch_job(
    #         command=command, task="csv_processing", region=region, background_tasks=background_tasks
    #     )

    steps = [SimpleCLICommand(command="s3_pull {datablob_id}")]
    executor = AirflowExecutor.create_executor(
        steps, cloud_provider=datablob.cloud_provider, region=datablob.region
    )
    dag_file_path, run_id = executor.execute(
        description="s3 pull",
        tags="s3_pull",
        datablob_id=datablob.id,
    )

    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 17
@datablob_router.post(
    "/from_s3", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore
)
def from_s3_route(
    *,
    from_s3_request: FromS3Request,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from csv/parquet files in s3 bucket"""
    user = session.merge(user)
    return DataBlob.from_s3(  # type: ignore
        from_s3_request=from_s3_request,
        user=user,
        session=session,
        background_tasks=background_tasks,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 20
class AzureBlobStorageRequest(BaseModel):
    """Base Request object for from_azure_blob_storage and to_azure_blob_storage routes"""

    uri: str
    credential: str

# %% ../../notebooks/DataBlob_Router.ipynb 21
class FromAzureBlobStorageRequest(AzureBlobStorageRequest):
    """Request object for the from_azure_blob_storage route

    Args:
        uri: Azure blob storage uri of the folder where parquet files are stored
        credential: Credential for the blob storage container
        cloud_provider: Cloud provider to save files
        region: Region of the cloud provider
        tag: Tag to add to the datablob
    """

    cloud_provider: CloudProvider = "azure"  # type: ignore
    #     region: Optional[str] = None
    region: str
    tag: Optional[str] = None

    class Config:
        use_enum_values = True

# %% ../../notebooks/DataBlob_Router.ipynb 22
@patch(cls_method=True)
def from_azure_blob_storage(
    cls: DataBlob,
    *,
    from_azure_blob_storage_request: FromAzureBlobStorageRequest,
    user: User,
    session: Session,
    background_tasks: BackgroundTasks,
    no_retries: int = 3,
) -> DataBlob:
    """Create a datablob from given azure blob storage

    Args:
        from_azure_blob_storage_request: The from_azure_blob_storage request
        user: User object
        session: Sqlmodel session
        background_tasks: BackgroundTasks object
        no_retries: Number of times to retry before raising an exception

    Returns:
        A new datablob created from azure blob storage
    """

    uri = create_db_uri_for_azure_blob_storage_datablob(
        uri=from_azure_blob_storage_request.uri,
        credential=from_azure_blob_storage_request.credential,
    )

    cloud_provider = from_azure_blob_storage_request.cloud_provider
    region = from_azure_blob_storage_request.region
    # ToDo: get region
    #     if region is None:
    #         s3_client = boto3.client(
    #             "s3",
    #             aws_access_key_id=from_s3_request.access_key,
    #             aws_secret_access_key=from_s3_request.secret_key,
    #         )
    #         bucket_name, folder = get_s3_bucket_name_and_folder_from_uri(from_s3_request.uri)
    #         region = s3_client.get_bucket_location(Bucket=bucket_name)["LocationConstraint"]

    verify_aws_region(region) if cloud_provider == "aws" else verify_azure_region(
        region
    )
    source = from_azure_blob_storage_request.uri

    datablob = DataBlob._create(  # type: ignore
        type="azure_blob_storage",
        uri=uri,
        source=source,
        cloud_provider=cloud_provider,
        region=region,
        total_steps=1,
        user_tag=from_azure_blob_storage_request.tag,
        user=user,
        session=session,
    )

    command = f"azure_blob_storage_pull {datablob.id}"
    create_batch_job(
        command=command,
        task="csv_processing",
        cloud_provider=cloud_provider,
        region=region,
        background_tasks=background_tasks,
    )

    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 23
@datablob_router.post(
    "/from_azure_blob_storage", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore
)
def from_azure_blob_storage_route(
    *,
    from_azure_blob_storage_request: FromAzureBlobStorageRequest,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from csv/parquet files in s3 bucket"""
    user = session.merge(user)
    return DataBlob.from_azure_blob_storage(  # type: ignore
        from_azure_blob_storage_request=from_azure_blob_storage_request,
        user=user,
        session=session,
        background_tasks=background_tasks,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 26
class DBRequest(BaseModel):
    """Base request object for from_db and to_db routes"""

    host: str
    port: int
    username: str
    password: str
    database: str
    table: str

# %% ../../notebooks/DataBlob_Router.ipynb 27
class FromDBRequest(DBRequest):
    """Request object for the /datablob/db route

    Args:
        host: Remote database host name
        port: DB port
        username: Username to access the db
        password: Password to access the db
        database: Database to use
        table: Table to import data from
        cloud_provider: Cloud provider to save files
        region: Region of the cloud provider
        tag: Tag to add to the datablob
    """

    cloud_provider: CloudProvider = "aws"  # type: ignore
    region: str = "eu-west-1"
    tag: Optional[str] = None

    class Config:
        use_enum_values = True

# %% ../../notebooks/DataBlob_Router.ipynb 28
@patch(cls_method=True)
def from_rdbms(
    cls: DataBlob,
    *,
    from_db_request: FromDBRequest,
    database_server: str,
    user: User,
    session: Session,
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from a RDBMS

    Args:
        from_db_request: The from_db request
        database_server: Database engine name
        user: User object
        session: Sqlmodel session
        background_tasks: BackgroundTasks object

    Returns:
        A new datablob created from a RDBMS
    """
    host = from_db_request.host
    port = from_db_request.port
    table = from_db_request.table
    database = from_db_request.database

    uri = create_db_uri_for_db_datablob(
        username=from_db_request.username,
        password=from_db_request.password,
        host=host,
        port=port,
        table=table,
        database=database,
        database_server=database_server,
    )

    source = f"{database_server}://{host}:{port}/{database}/{table}"
    verify_aws_region(
        from_db_request.region
    ) if from_db_request.cloud_provider == "aws" else verify_azure_region(
        from_db_request.region
    )

    with commit_or_rollback(session):
        datablob = DataBlob._create(  # type: ignore
            type="db",
            uri=uri,
            source=source,
            cloud_provider=from_db_request.cloud_provider,
            region=from_db_request.region,
            total_steps=1,
            user_tag=from_db_request.tag,
            user=user,
            session=session,
        )

    if database_server in ["mysql", "postgresql"]:
        command = f"db_pull {datablob.id}"
    else:
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail=f"{ERRORS['PULL_NOT_AVAILABLE']} for database server {database_server}",
        )

    create_batch_job(
        command=command,
        task="csv_processing",
        cloud_provider=from_db_request.cloud_provider,
        region=from_db_request.region,
        background_tasks=background_tasks,
    )
    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 29
@datablob_router.post(
    "/from_mysql", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore
)
def from_mysql_route(
    *,
    from_db_request: FromDBRequest,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from a database"""
    user = session.merge(user)
    return DataBlob.from_rdbms(  # type: ignore
        from_db_request=from_db_request,
        database_server="mysql",
        user=user,
        session=session,
        background_tasks=background_tasks,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 32
class ClickHouseRequest(DBRequest):
    """Base request object for from_clickhouse and to_clickhouse routes"""

    protocol: str

# %% ../../notebooks/DataBlob_Router.ipynb 33
class FromClickHouseRequest(ClickHouseRequest):
    """Request object for the /datablob/from_clickhouse route

    Args:
        host: Hostname where db is hosted
        port: DB port
        username: Username to access the db
        password: Password to access the db
        database: Database to use
        table: Table to import/export data
        protocol: Protocol to use (native/http)
        index_column: Column to use to partition rows and to use as index
        timestamp_column: Timestamp column
        filters: Additional column filters as a dictionary
        cloud_provider: Cloud provider to save files
        region: Region of the cloud provider
        tag: Tag to add to the datablob
    """

    index_column: str
    timestamp_column: str
    filters: Optional[Dict[str, Any]] = None
    cloud_provider: CloudProvider = "aws"  # type: ignore
    region: str = "eu-west-1"
    tag: Optional[str] = None

    class Config:
        use_enum_values = True

# %% ../../notebooks/DataBlob_Router.ipynb 34
@patch(cls_method=True)
def from_clickhouse(
    cls: DataBlob,
    *,
    from_clickhouse_request: FromClickHouseRequest,
    user: User,
    session: Session,
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from a clickhouse database

    Args:
        from_clickhouse_request: The from_clickhouse request
        user: User object
        session: Sqlmodel session
        background_tasks: BackgroundTasks object

    Returns:
        A new datablob created from a clickhouse database
    """

    host = from_clickhouse_request.host
    port = from_clickhouse_request.port
    table = from_clickhouse_request.table
    database = from_clickhouse_request.database
    protocol = from_clickhouse_request.protocol

    uri = create_db_uri_for_clickhouse_datablob(
        username=from_clickhouse_request.username,
        password=from_clickhouse_request.password,
        host=host,
        port=port,
        table=table,
        database=database,
        protocol=protocol,
    )

    source = f"clickhouse+{protocol}://{host}:{port}/{database}/{table}"
    verify_aws_region(
        from_clickhouse_request.region
    ) if from_clickhouse_request.cloud_provider == "aws" else verify_azure_region(
        from_clickhouse_request.region
    )

    with commit_or_rollback(session):
        datablob = DataBlob._create(  # type: ignore
            type="db",
            uri=uri,
            source=source,
            cloud_provider=from_clickhouse_request.cloud_provider,
            region=from_clickhouse_request.region,
            total_steps=1,
            user_tag=from_clickhouse_request.tag,
            user=user,
            session=session,
        )

    command = f"clickhouse_pull {datablob.id} {from_clickhouse_request.index_column} {from_clickhouse_request.timestamp_column}"
    if from_clickhouse_request.filters:
        command = (
            command
            + f" --filters_json {shlex.quote(json.dumps(from_clickhouse_request.filters))}"
        )

    create_batch_job(
        command=command,
        task="csv_processing",
        cloud_provider=from_clickhouse_request.cloud_provider,
        region=from_clickhouse_request.region,
        background_tasks=background_tasks,
    )
    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 35
@datablob_router.post(
    "/from_clickhouse", response_model=DataBlobRead, responses=create_datablob_responses  # type: ignore
)
def from_clickhouse_route(
    *,
    from_clickhouse_request: FromClickHouseRequest,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
    background_tasks: BackgroundTasks,
) -> DataBlob:
    """Create a datablob from a database"""
    user = session.merge(user)
    return DataBlob.from_clickhouse(  # type: ignore
        from_clickhouse_request=from_clickhouse_request,
        user=user,
        session=session,
        background_tasks=background_tasks,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 37
class FromLocalRequest(BaseModel):
    """Request object for from_local route

    Args:
        path: Local path of the datablob
        cloud_provider: Cloud provider to save files
        region: Region of the cloud provider
        tag: Tag to add to the datablob
    """

    path: str
    cloud_provider: CloudProvider = "aws"  # type: ignore
    region: str = "eu-west-1"
    tag: Optional[str] = None

    class Config:
        use_enum_values = True


class FromLocalResponse(BaseModel):
    """Response object for the /datablob/from_local route

    Args:
        uuid: Datablob uuid
        type: Type of the datablob
        presigned: Presigned s3 url(valid for 24 hours) and other params to upload CSV file
    """

    uuid: uuid_pkg.UUID
    type: str
    presigned: Dict[str, Any]

# %% ../../notebooks/DataBlob_Router.ipynb 38
@patch(cls_method=True)
def from_local(
    cls: DataBlob,
    *,
    path: str,
    cloud_provider: str,
    region: str,
    user_tag: Optional[str] = None,
    user: User,
    session: Session,
) -> FromLocalResponse:
    """Create a datablob from local file(s)

    Args:
        path: The relative or absolute path to a local file or to a directory containing the files.
        user_tag: A string to tag the datablob
        user: User object
        session: Sqlmodel session

    Returns:
        A new datablob created from local file(s)
    """
    if cloud_provider == "azure":
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=ERRORS["AZURE_NOT_SUPPORTED"],
        )
    verify_aws_region(region)

    with commit_or_rollback(session):
        datablob = DataBlob._create(  # type: ignore
            type="local",
            uri=None,
            source=path,
            cloud_provider=cloud_provider,
            region=region,
            total_steps=1,
            user_tag=user_tag,
            user=user,
            session=session,
        )

    destination_bucket, s3_path = create_s3_datablob_path(
        user_id=user.id, datablob_id=datablob.id, region=region  # type: ignore
    )
    uri = create_db_uri_for_local_datablob(bucket=destination_bucket, s3_path=s3_path)
    with commit_or_rollback(session):
        datablob.uri = uri
        session.add(datablob)

    presigned = boto3.client(
        "s3", region_name=region, config=Config(signature_version="s3v4")
    ).generate_presigned_post(
        Bucket=destination_bucket.name,
        Key=s3_path + "/" + "${filename}",
        Fields=None,
        Conditions=[["starts-with", "$key", s3_path]],
        ExpiresIn=60 * 60 * 24,
    )
    from_local_response = FromLocalResponse(
        uuid=datablob.uuid, type=datablob.type, presigned=presigned
    )
    logger.info(f"DataBlob.from_local(): {from_local_response.__repr__()}")
    return from_local_response

# %% ../../notebooks/DataBlob_Router.ipynb 39
@datablob_router.post(
    "/from_local/start",
    response_model=FromLocalResponse,
    responses=create_datablob_responses,  # type: ignore
)
def from_local_start_route(
    from_local_request: FromLocalRequest,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
) -> FromLocalResponse:
    """Get presigned s3 url to upload local CSV/Parquet files and create datablob from it"""
    user = session.merge(user)
    return DataBlob.from_local(  # type: ignore
        path=from_local_request.path,
        cloud_provider=from_local_request.cloud_provider,
        region=from_local_request.region,
        user_tag=from_local_request.tag,
        user=user,
        session=session,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 41
get_datablob_responses = {
    400: {"model": HTTPError, "description": ERRORS["INCORRECT_DATABLOB_ID"]},
    422: {"model": HTTPError, "description": "DataBlob error"},
}


@patch(cls_method=True)
def get(cls: DataBlob, uuid: str, user: User, session: Session) -> DataBlob:
    """Get datablob based on uuid

    Args:
        uuid: Datablob uuid
        user: User object
        session: Sqlmodel session

    Returns:
        The datablob object for given datablob uuid

    Raises:
        HTTPException: if datablob id is incorrect or if datablob is deleted
    """
    try:
        datablob = session.exec(
            select(DataBlob).where(DataBlob.uuid == uuid, DataBlob.user == user)
        ).one()
    except NoResultFound:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=ERRORS["INCORRECT_DATABLOB_ID"],
        )

    if datablob.disabled:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=ERRORS["DATABLOB_IS_DELETED"],
        )

    if datablob.error is not None:
        raise HTTPException(
            status_code=status.HTTP_422_UNPROCESSABLE_ENTITY, detail=datablob.error
        )

    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 44
@patch
def is_ready(self: DataBlob):
    """Check if the datablob's completed steps equal to total steps, else raise HTTPException"""
    if self.completed_steps != self.total_steps:
        if self.path:
            bucket, s3_path = get_s3_bucket_and_path_from_uri(self.path)  # type: ignore
        else:
            bucket, s3_path = create_s3_datablob_path(user_id=self.user.id, datablob_id=self.id, region=self.region)  # type: ignore

        if len(list(bucket.objects.filter(Prefix=s3_path + "/"))) == 0:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=ERRORS["DATABLOB_CSV_FILES_NOT_AVAILABLE"],
            )

# %% ../../notebooks/DataBlob_Router.ipynb 45
class FileType(str, Enum):
    csv = "csv"
    parquet = "parquet"


class ToDataSourceRequest(BaseModel):
    """Request object for the /datablob/{datablob_id}/to_datasource route

    Args:

        file_type: type of files in datablob; currently csv and parquet are supported
        deduplicate_data: If set to True (default value False), then duplicate rows are removed while processing
        index_column: Name of the column used to index and partition the data into partitions
        sort_by: Name of the column or list of columns  used to sort data within the same index value
        blocksize: Size of partition
        kwargs: Keyword arguments which are passed to the **dask.dataframe.read_csv()** function,
            typically params for underlining **pd.read_csv()** from Pandas.
    """

    file_type: FileType
    deduplicate_data: bool = False
    index_column: str
    sort_by: Union[str, List[str]]
    blocksize: str = "256MB"
    kwargs: Optional[Dict[str, Any]] = None

    class Config:
        use_enum_values = True

# %% ../../notebooks/DataBlob_Router.ipynb 46
@patch
def to_datasource(
    self: DataBlob,
    to_datasource_request: ToDataSourceRequest,
    user: User,
    session: Session,
    background_tasks: BackgroundTasks,
) -> DataSource:
    """Process the CSV/Parquet datablob files and return a datasource object

    Args:
        to_datasource_request: The to_datasource_request object
        user: User object
        session: Sqlmodel session
        background_tasks: BackgroundTasks object
    """

    self.is_ready()  # type: ignore
    datasource = DataSource._create(datablob=self, user=user, session=session)  # type: ignore

    if to_datasource_request.file_type == "csv":
        process_command = "process_csv"
    elif to_datasource_request.file_type == "parquet":
        process_command = "process_parquet"

    sort_by = (
        [to_datasource_request.sort_by]
        if isinstance(to_datasource_request.sort_by, str)
        else to_datasource_request.sort_by
    )

    command = f"{process_command} {self.id} {datasource.id} {shlex.quote(to_datasource_request.index_column)} {shlex.quote(json.dumps(sort_by))} --blocksize {to_datasource_request.blocksize}"
    if to_datasource_request.kwargs is not None:
        command = (
            command
            + f" --kwargs_json {shlex.quote(json.dumps(to_datasource_request.kwargs))}"
        )
    if to_datasource_request.deduplicate_data:
        command = command + " --deduplicate_data"

    create_batch_job(
        command=command,
        task="csv_processing",
        cloud_provider=self.cloud_provider,
        region=self.region,
        background_tasks=background_tasks,
    )

    return datasource

# %% ../../notebooks/DataBlob_Router.ipynb 47
@datablob_router.post(
    "/{datablob_uuid}/to_datasource",
    status_code=status.HTTP_202_ACCEPTED,
    response_model=DataSourceRead,
    responses=get_datablob_responses,  # type: ignore
)
def to_datasource_route(
    *,
    datablob_uuid: str,
    to_datasource_request: ToDataSourceRequest,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
    background_tasks: BackgroundTasks,
) -> DataSource:
    """Pull uploaded CSV/Parquet datablob, process it and store in s3 client storage bucket as parquet"""
    user = session.merge(user)
    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore

    return datablob.to_datasource(
        to_datasource_request, user, session, background_tasks
    )

# %% ../../notebooks/DataBlob_Router.ipynb 50
@datablob_router.get(
    "/{datablob_uuid}", response_model=DataBlobRead, responses=get_datablob_responses  # type: ignore
)
def get_details_of_datablob(
    datablob_uuid: str,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
) -> DataBlob:
    """Get details of the datablob"""
    user = session.merge(user)
    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore
    return datablob

# %% ../../notebooks/DataBlob_Router.ipynb 53
@patch
def delete(self: DataBlob, user: User, session: Session):
    """Delete a datablob

    Args:
        user: User object
        session: Sqlmodel session
    """
    delete_data_object_files_in_cloud(data_object=self)

    self.disabled = True

    with commit_or_rollback(session):
        session.add(self)

    return self

# %% ../../notebooks/DataBlob_Router.ipynb 54
@datablob_router.delete(
    "/{datablob_uuid}",
    response_model=DataBlobRead,
    responses=get_datablob_responses,  # type: ignore
)
def delete_datablob(
    datablob_uuid: str,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
) -> DataBlob:
    """Delete datablob"""
    user = session.merge(user)
    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore

    return datablob.delete(user, session)

# %% ../../notebooks/DataBlob_Router.ipynb 56
@patch(cls_method=True)
def get_all(
    cls: DataBlob,
    disabled: bool,
    completed: bool,
    offset: int,
    limit: int,
    user: User,
    session: Session,
) -> List[DataBlob]:
    """Get all datablobs created by the user

    Args:
        disabled: Whether to get disabled datablobs
        completed: Whether to include only datablobs which are successfully pulled from its source
        offset: Offset results by given integer
        limit: Limit results by given integer
        user: User object
        session: Sqlmodel session

    Returns:
        A list of datablob objects
    """
    statement = select(DataBlob).where(DataBlob.user == user)
    statement = statement.where(DataBlob.disabled == disabled)
    if completed:
        statement = statement.where(DataBlob.completed_steps == DataBlob.total_steps)
    # get all data sources from db
    return session.exec(statement.offset(offset).limit(limit)).all()

# %% ../../notebooks/DataBlob_Router.ipynb 57
@datablob_router.get("/", response_model=List[DataBlobRead])
def get_all_datablobs(
    disabled: bool = False,
    completed: bool = False,
    offset: int = 0,
    limit: int = Query(default=100, lte=100),
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
) -> List[DataBlob]:
    """
    Get all datablobs created by user
    """
    user = session.merge(user)
    return DataBlob.get_all(  # type: ignore
        disabled=disabled,
        completed=completed,
        offset=offset,
        limit=limit,
        user=user,
        session=session,
    )

# %% ../../notebooks/DataBlob_Router.ipynb 60
@patch
def tag(self: DataBlob, tag_name: str, session: Session):
    """Tag an existing datablob

    Args:
        tag_name: A string to tag the datablob
        session: Sqlmodel session
    """
    user_tag = Tag.get_by_name(name=tag_name, session=session)  # type: ignore

    self.remove_tag_from_previous_datablobs(tag_name=user_tag.name, session=session)  # type: ignore
    self.tags.append(user_tag)

    with commit_or_rollback(session):
        session.add(self)

    return self

# %% ../../notebooks/DataBlob_Router.ipynb 61
@datablob_router.post("/{datablob_uuid}/tag", response_model=DataBlobRead)
def tag_datablob(
    datablob_uuid: str,
    tag_to_create: TagCreate,
    user: User = Depends(get_current_active_user),
    session: Session = Depends(get_session),
) -> DataBlob:
    """Add tag to datablob"""
    user = session.merge(user)
    datablob = DataBlob.get(uuid=datablob_uuid, user=user, session=session)  # type: ignore

    return datablob.tag(tag_name=tag_to_create.name, session=session)
