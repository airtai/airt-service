# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/DataBlob_DB.ipynb.

# %% auto 0
__all__ = ['download_from_db', 'db_pull', 'db_push']

# %% ../../notebooks/DataBlob_DB.ipynb 3
import tempfile
from pathlib import Path
from typing import *

import dask.dataframe as dd
import pandas as pd
from airt.engine.engine import using_cluster
from airt.logger import get_logger
from airt.remote_path import RemotePath
from fastcore.script import Param, call_parse
from fastcore.utils import *
from sqlmodel import select

import airt_service.sanitizer
from ..aws.utils import create_s3_datablob_path
from ..azure.utils import create_azure_blob_storage_datablob_path
from airt_service.data.utils import (
    calculate_data_object_folder_size_and_path,
    calculate_data_object_pulled_on,
    get_db_connection_params_from_db_uri,
)
from airt_service.db.models import (
    DataBlob,
    PredictionPush,
    create_connection_string,
    get_session_with_context,
)
from ..helpers import truncate

# %% ../../notebooks/DataBlob_DB.ipynb 6
logger = get_logger(__name__)

# %% ../../notebooks/DataBlob_DB.ipynb 9
def download_from_db(
    *,
    host: str,
    port: int,
    username: str,
    password: str,
    database: str,
    database_server: str,
    table: str,
    chunksize: Optional[int] = 1_000_000,
    output_path: Path,
) -> None:
    """Download data from database and stores it as parquet files in output path

    Args:
        host: Host of db
        port: Port of db
        username: Username of db
        password: Password of db
        database: Database to use in db
        database_server: Server/engine of db
        table: Table to use in db
        chunksize: Chunksize to download as
        output_path: Path to store parquet files
    """
    conn_str = create_connection_string(
        username=username,
        password=password,
        host=host,
        port=port,
        database=database,
        database_server=database_server,
    )

    with tempfile.TemporaryDirectory() as td:
        d = Path(td)
        for i, df in enumerate(
            pd.read_sql_table(table_name=table, con=conn_str, chunksize=chunksize)
        ):
            fname = d / f"{database_server}_{database}_{table}_data_{i:09d}.parquet"
            logger.info(
                f"Writing data retrieved from the database to temporary file {fname}"
            )
            df.to_parquet(fname)  # type: ignore
        logger.info(
            f"Rewriting temporary parquet files from {d} to output directory {output_path}"
        )
        ddf = dd.read_parquet(
            d,
            blocksize=None,
        )
        ddf.to_parquet(output_path)

# %% ../../notebooks/DataBlob_DB.ipynb 11
@call_parse  # type: ignore
def db_pull(datablob_id: Param("id of datablob in db", int)) -> None:  # type: ignore
    """Pull the datablob and update its progress in internal db

    Args:
        datablob_id: Id of datablob in db

    Example:
        The following code executes a CLI command:
        ```db_pull 1
        ```
    """
    with get_session_with_context() as session:
        datablob = session.exec(
            select(DataBlob).where(DataBlob.id == datablob_id)
        ).one()

        datablob.error = None
        datablob.completed_steps = 0
        datablob.folder_size = None
        datablob.path = None

        (
            username,
            password,
            host,
            port,
            table,
            database,
            database_server,
        ) = get_db_connection_params_from_db_uri(datablob.uri)

        try:
            if datablob.cloud_provider == "aws":
                destination_bucket, s3_path = create_s3_datablob_path(
                    user_id=datablob.user.id,
                    datablob_id=datablob.id,
                    region=datablob.region,
                )
                destination_remote_url = f"s3://{destination_bucket.name}/{s3_path}"
            elif datablob.cloud_provider == "azure":
                (
                    destination_container_client,
                    destination_azure_blob_storage_path,
                ) = create_azure_blob_storage_datablob_path(
                    user_id=datablob.user.id,
                    datablob_id=datablob.id,
                    region=datablob.region,
                )
                destination_remote_url = f"{destination_container_client.url}/{destination_azure_blob_storage_path}"

            with RemotePath.from_url(
                remote_url=destination_remote_url,
                pull_on_enter=False,
                push_on_exit=True,
                exist_ok=True,
                parents=True,
            ) as destionation_s3_path:
                sync_path = destionation_s3_path.as_path()
                download_from_db(
                    host=host,
                    port=port,
                    username=username,
                    password=password,
                    database=database,
                    database_server=database_server,
                    table=table,
                    output_path=sync_path,
                )
                calculate_data_object_pulled_on(datablob)

                if len(list(sync_path.glob("*"))) == 0:
                    raise ValueError(f"no files to download, table is empty")

            # Calculate folder size in S3
            calculate_data_object_folder_size_and_path(datablob)
        except Exception as e:
            datablob.error = truncate(str(e))
        session.add(datablob)
        session.commit()

# %% ../../notebooks/DataBlob_DB.ipynb 13
@call_parse  # type: ignore
def db_push(prediction_push_id: int) -> None:
    """Push prediction data to a rdbms

    Params:
        prediction_push_id: Id of prediction_push

    Example:
        The following code executes a CLI command:
        ```db_push 1
        ```
    """
    with get_session_with_context() as session:
        prediction_push = session.exec(
            select(PredictionPush).where(PredictionPush.id == prediction_push_id)
        ).one()

        prediction_push.error = None
        prediction_push.completed_steps = 0

        (
            username,
            password,
            host,
            port,
            table,
            database,
            database_server,
        ) = get_db_connection_params_from_db_uri(db_uri=prediction_push.uri)

        try:
            with RemotePath.from_url(
                remote_url=prediction_push.prediction.path,
                pull_on_enter=True,
                push_on_exit=False,
                exist_ok=True,
                parents=False,
            ) as s3_path:
                with using_cluster("cpu") as engine:
                    ddf = engine.dd.read_parquet(s3_path.as_path())
                    conn_str = create_connection_string(
                        username=username,
                        password=password,
                        host=host,
                        port=port,
                        database=database,
                        database_server=database_server,
                    )
                    ddf.to_sql(
                        name=table,
                        uri=conn_str,
                        if_exists="append",
                        index=True,
                        method="multi",
                    )
            prediction_push.completed_steps = 1
        except Exception as e:
            prediction_push.error = truncate(str(e))

        session.add(prediction_push)
        session.commit()
