# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb.

# %% auto 0
__all__ = ['copy_between_azure_blob_storage', 'azure_blob_storage_pull', 'azure_blob_storage_push']

# %% ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb 3
import shutil
from datetime import datetime
from typing import *

from airt.logger import get_logger
from airt.remote_path import RemotePath
from azure.identity import DefaultAzureCredential
from fastcore.script import call_parse
from fastcore.utils import *
from sqlmodel import select

import airt_service.sanitizer
from ..aws.utils import create_s3_datablob_path
from ..azure.utils import create_azure_blob_storage_datablob_path
from ..constants import METADATA_FOLDER_PATH
from airt_service.data.utils import (
    calculate_data_object_folder_size_and_path,
    calculate_data_object_pulled_on,
    get_azure_blob_storage_connection_params_from_db_uri,
)
from ..db.models import DataBlob, PredictionPush, get_session_with_context
from ..helpers import truncate

# %% ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb 6
logger = get_logger(__name__)

# %% ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb 7
def copy_between_azure_blob_storage(
    source_remote_url: str,
    destination_remote_url: str,
    source_credential: Optional[Union[str, DefaultAzureCredential]] = None,
    destination_credential: Optional[Union[str, DefaultAzureCredential]] = None,
    datablob: Optional[DataBlob] = None,
    skip_metadata_dir: Optional[bool] = False,
) -> None:
    """Copy files from source azure blob storage path and to destination azure blob storage path

    By default, all files are copied to the destination_remote_url. In case
    the **skip_metadata_dir** flag is set to **True**, then the **.metadata_by_airt**
    folder will not be copied to the destination_remote_url.

    Args:
        source_remote_url: S3 uri where files to copy are located
        destination_remote_url: S3 uri to copy files
        source_credential: Source azure blob storage credential
        destination_credential: Destination azure blob storage credential
        datablob: Optional datablob object to calculate pulled_on field
        skip_metadata_dir: If set to **True** then the **.metadata_by_airt** folder
            will not be copied to the destination_remote_url.
    """
    source_credential = (
        source_credential if source_credential else DefaultAzureCredential()
    )
    destination_credential = (
        destination_credential if destination_credential else DefaultAzureCredential()
    )

    with RemotePath.from_url(
        remote_url=destination_remote_url,
        pull_on_enter=False,
        push_on_exit=True,
        exist_ok=True,
        parents=True,
        credential=destination_credential,
    ) as destination_azure_blob_storage_path:
        sync_path = destination_azure_blob_storage_path.as_path()
        with RemotePath.from_url(
            remote_url=source_remote_url,
            pull_on_enter=True,
            push_on_exit=False,
            exist_ok=True,
            parents=False,
            credential=source_credential,
        ) as source_azure_blob_storage_path:
            if datablob is not None:
                calculate_data_object_pulled_on(datablob)

            source_files = source_azure_blob_storage_path.as_path().iterdir()

            if skip_metadata_dir:
                source_files = [
                    f for f in source_files if METADATA_FOLDER_PATH not in str(f)
                ]

            for f in source_files:
                shutil.move(str(f), sync_path)

        if len(list(sync_path.glob("*"))) == 0:
            raise ValueError(
                f"URI {source_remote_url} is invalid or no files available"
            )

# %% ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb 10
@call_parse
def azure_blob_storage_pull(datablob_id: int) -> None:
    """Pull the data from azure blob storage and updates progress in db

    Args:
        datablob_id: Id of datablob in db

    Example:
        The following code executes a CLI command:
        ```azure_blob_storage_pull 1
        ```
    """
    with get_session_with_context() as session:
        datablob = session.exec(
            select(DataBlob).where(DataBlob.id == datablob_id)
        ).one()

        datablob.error = None
        datablob.completed_steps = 0
        datablob.folder_size = None
        datablob.path = None

        (
            source_remote_url,
            source_credential,
        ) = get_azure_blob_storage_connection_params_from_db_uri(db_uri=datablob.uri)

        try:
            if datablob.cloud_provider == "aws":
                destination_bucket, s3_path = create_s3_datablob_path(
                    user_id=datablob.user.id,
                    datablob_id=datablob.id,
                    region=datablob.region,
                )
                destination_remote_url = f"s3://{destination_bucket.name}/{s3_path}"
            elif datablob.cloud_provider == "azure":
                (
                    destination_container_client,
                    destination_azure_blob_storage_path,
                ) = create_azure_blob_storage_datablob_path(
                    user_id=datablob.user.id,
                    datablob_id=datablob.id,
                    region=datablob.region,
                )
                destination_remote_url = f"{destination_container_client.url}/{destination_azure_blob_storage_path}"

            with RemotePath.from_url(
                remote_url=destination_remote_url,
                pull_on_enter=False,
                push_on_exit=True,
                exist_ok=True,
                parents=True,
            ) as destination_remote_path:
                sync_path = destination_remote_path.as_path()
                with RemotePath.from_url(
                    remote_url=source_remote_url,
                    pull_on_enter=True,
                    push_on_exit=False,
                    exist_ok=True,
                    parents=False,
                    credential=source_credential,
                ) as source_azure_blob_storage_path:
                    calculate_data_object_pulled_on(datablob)

                    source_files = source_azure_blob_storage_path.as_path().iterdir()
                    for f in source_files:
                        shutil.move(str(f), sync_path)

                if len(list(sync_path.glob("*"))) == 0:
                    raise ValueError(
                        f"URI {source_remote_url} is invalid or no files available"
                    )

            # Calculate folder size in S3/Azure blob storage
            calculate_data_object_folder_size_and_path(datablob)
        except Exception as e:
            datablob.error = truncate(str(e))

        session.add(datablob)
        session.commit()

# %% ../../notebooks/DataBlob_Azure_Blob_Storage.ipynb 14
@call_parse
def azure_blob_storage_push(prediction_push_id: int) -> None:
    """Push the data to azure blob storage and update its progress in db

    Args:
        prediction_push_id: Id of prediction_push

    Example:
        The following code executes a CLI command:
        ```azure_blob_storage_push 1
        ```
    """
    with get_session_with_context() as session:
        prediction_push = session.exec(
            select(PredictionPush).where(PredictionPush.id == prediction_push_id)
        ).one()

        prediction_push.error = None
        prediction_push.completed_steps = 0

        (
            destination_uri,
            destination_credential,
        ) = get_azure_blob_storage_connection_params_from_db_uri(
            db_uri=prediction_push.uri
        )

        try:
            with RemotePath.from_url(
                remote_url=destination_uri,
                pull_on_enter=False,
                push_on_exit=True,
                exist_ok=True,
                parents=True,
                credential=destination_credential,
            ) as destination_azure_blob_storage_path:
                sync_path = destination_azure_blob_storage_path.as_path()
                with RemotePath.from_url(
                    remote_url=prediction_push.prediction.path,
                    pull_on_enter=True,
                    push_on_exit=False,
                    exist_ok=True,
                    parents=False,
                ) as source_remote_path:
                    source_files = source_remote_path.as_path().iterdir()
                    for f in source_files:
                        shutil.move(str(f), sync_path)
            prediction_push.completed_steps = 1
        except Exception as e:
            prediction_push.error = truncate(str(e))

        session.add(prediction_push)
        session.commit()
